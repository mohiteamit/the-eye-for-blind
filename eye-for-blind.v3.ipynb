{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye for Blind – Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "Eye for Blind: An Assistive Image Captioning System with Visual Attention\n",
    "\n",
    "This project implements a deep learning model that generates natural language descriptions of images, particularly aimed at visually impaired users. The model leverages an attention mechanism to selectively focus on image regions when generating each word, mimicking human vision.\n",
    "\n",
    "Inspired by \"Show, Attend and Tell\" (Xu et al., 2015), this implementation:\n",
    "1. Uses a CNN encoder (InceptionV3) to extract image features.\n",
    "2. Applies additive (Bahdanau) attention during decoding.\n",
    "3. Employs a decoder LSTM to generate captions.\n",
    "4. Converts generated captions to speech using gTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Data\n",
    "    'subset_ratio': 1.0,\n",
    "    'image_dir': '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file': '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    \n",
    "    # Training\n",
    "    'seed': 42,\n",
    "    'epochs': 20,\n",
    "    'patience': 8,\n",
    "    'learning_rate': 1e-4,  # Reduced from 3e-4\n",
    "    'grad_clip_value': 1.0,  # Reduced from 5.0\n",
    "    'scheduled_sampling_max_prob': 0.25,\n",
    "    'attention_reg_lambda': 1.0,\n",
    "    'dropout_rate': 0.5,  # Increased from 0.3\n",
    "    \n",
    "    # Model Architecture\n",
    "    'embedding_dim': 512,\n",
    "    'units': 512,\n",
    "    'max_length': 30,\n",
    "    'vocab_min_count': 3,\n",
    "    \n",
    "    # Data Pipeline\n",
    "    'batch_size': 128,\n",
    "    'buffer_size': 10000,\n",
    "    'image_size': 299,\n",
    "    'resize_size': 342,\n",
    "    \n",
    "    # Inference\n",
    "    'beam_size': 5,\n",
    "    'length_penalty': 0.7,\n",
    "    'focus_threshold': 0.5,  # For attention diagnostics\n",
    "    \n",
    "    # System\n",
    "    'mixed_precision': False,\n",
    "    'enable_checkpointing': True,  # New flag\n",
    "    'checkpoint_dir': './checkpoints/',\n",
    "    'restore_checkpoint': False,  # New flag\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "\n",
    "# Mixed precision\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# GPU setup\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config.copy()  # Ensure local copy\n",
    "        self.captions_dict = dict()\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        self.test_data = []\n",
    "\n",
    "    def load_captions(self):\n",
    "        df = pd.read_csv(self.config['caption_file'], sep='|', header=None, \n",
    "                         names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "        self.captions_dict = {img: group['comment'].tolist() for img, group in df.groupby('image_name')}\n",
    "        return self.captions_dict\n",
    "\n",
    "    def preprocess_caption(self, caption):\n",
    "        if not isinstance(caption, str):\n",
    "            return None\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z0-9.,? ]\", \"\", caption)\n",
    "        return f\"<start> {caption.strip()} <end>\"\n",
    "\n",
    "    def prepare_captions(self):\n",
    "        all_captions = []\n",
    "        for caps in self.captions_dict.values():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p: all_captions.append(p)\n",
    "\n",
    "        word_counts = Counter(w for cap in all_captions for w in cap.split())\n",
    "        valid_words = {w for w, cnt in word_counts.items() if cnt >= self.config['vocab_min_count']}\n",
    "        filtered = [c for c in all_captions if all(w in valid_words or w in ('<start>', '<end>') for w in c.split())]\n",
    "\n",
    "        tokenizer = Tokenizer(oov_token=\"<unk>\", filters='', lower=True)\n",
    "        tokenizer.fit_on_texts(filtered)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        pairs = []\n",
    "        for img, caps in self.captions_dict.items():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p and all(w in valid_words or w in ('<start>', '<end>') for w in p.split()):\n",
    "                    pairs.append((img, p))\n",
    "\n",
    "        if self.config['subset_ratio'] < 1.0:\n",
    "            pairs = pairs[:int(len(pairs) * self.config['subset_ratio'])]\n",
    "\n",
    "        random.shuffle(pairs)\n",
    "        n = len(pairs)\n",
    "        self.train_data, self.val_data, self.test_data = (\n",
    "            pairs[:int(0.8*n)], pairs[int(0.8*n):int(0.9*n)], pairs[int(0.9*n):])\n",
    "        return filtered\n",
    "\n",
    "    def encode_caption(self, caption):\n",
    "        seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=self.config['max_length'], padding='post')[0]\n",
    "        return padded_seq, len(seq)\n",
    "\n",
    "    @tf.function\n",
    "    def _base_decode(self, path):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        return img\n",
    "\n",
    "    @tf.function\n",
    "    def load_image_train(self, path):\n",
    "        img = self._base_decode(path)\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = self.config['resize_size'] / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.random_crop(img, size=[self.config['image_size'], self.config['image_size'], 3])\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [self.config['image_size'], self.config['image_size'], 3])\n",
    "\n",
    "    @tf.function\n",
    "    def load_image_eval(self, path):\n",
    "        img = self._base_decode(path)\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = self.config['resize_size'] / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.resize_with_crop_or_pad(img, self.config['image_size'], self.config['image_size'])\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [self.config['image_size'], self.config['image_size'], 3])\n",
    "\n",
    "    def build_dataset(self, data, shuffle=True):\n",
    "        def generator():\n",
    "            for img, cap in data:\n",
    "                img_path = os.path.join(self.config['image_dir'], img)\n",
    "                img_tensor = self.load_image_train(tf.convert_to_tensor(img_path))\n",
    "                token_ids, cap_len = self.encode_caption(cap)\n",
    "                yield img_tensor, token_ids, cap_len\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec((self.config['image_size'], self.config['image_size'], 3), tf.float32),\n",
    "                tf.TensorSpec((self.config['max_length'],), tf.int32),\n",
    "                tf.TensorSpec((), tf.int32)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(self.config['buffer_size'])\n",
    "        ds = ds.batch(self.config['batch_size'])\n",
    "        return ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(name=\"encoder\")\n",
    "        self.config = config.copy()\n",
    "        base = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, weights='imagenet',\n",
    "            input_shape=(self.config['image_size'], self.config['image_size'], 3))\n",
    "        base.trainable = False\n",
    "        self.cnn = Model(inputs=base.input, outputs=base.get_layer('mixed10').output)\n",
    "        self.reshape = layers.Reshape((-1, 2048))\n",
    "\n",
    "    def unfreeze_top_layers(self, n=2):\n",
    "        for layer in self.cnn.layers[-n:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return self.reshape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, config, vocab_size):\n",
    "        super().__init__(name=\"decoder\")\n",
    "        self.config = config.copy()\n",
    "        self.units = self.config['units']\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, self.config['embedding_dim'])\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "        self.f_beta = layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.lstm = layers.LSTM(self.units, return_sequences=True, return_state=True)\n",
    "        self.dropout = layers.Dropout(self.config['dropout_rate'])\n",
    "        self.deep_proj = layers.Dense(self.units * 2)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context, alpha = self.attention(features, hidden)\n",
    "        context = self.f_beta(hidden) * context\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        lstm_input = tf.concat([tf.expand_dims(context, 1), x], -1)\n",
    "\n",
    "        hidden = tf.cast(hidden, lstm_input.dtype)\n",
    "        cell = tf.cast(cell, lstm_input.dtype)\n",
    "\n",
    "        lstm_out, h_t, c_t = self.lstm(lstm_input, initial_state=[hidden, cell])\n",
    "        lstm_out = tf.squeeze(lstm_out, 1)\n",
    "\n",
    "        proj = self.deep_proj(tf.concat([lstm_out, context], -1))\n",
    "        proj = tf.reshape(proj, (-1, self.units, 2))\n",
    "        maxout = tf.reduce_max(proj, axis=-1)\n",
    "        maxout = self.dropout(maxout)\n",
    "\n",
    "        logits = self.fc(maxout)\n",
    "        return tf.expand_dims(logits, 1), h_t, c_t, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    def __init__(self, config, processor):\n",
    "        self.config = config.copy()\n",
    "        self.processor = processor\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.optimizer = None\n",
    "        self.loss_fn = None\n",
    "        self.ckpt_manager = None\n",
    "        self.best_bleu = 0.0\n",
    "        self.train_loss_log = []\n",
    "        self.train_bleu_log = []\n",
    "        self.val_bleu_log = []\n",
    "        self.bleu_subset_idx = None\n",
    "        self.smoothie = SmoothingFunction().method4\n",
    "        self.ss_prob = 0.0\n",
    "\n",
    "    def build_model(self):\n",
    "        self.encoder = Encoder(self.config)\n",
    "        self.decoder = Decoder(self.config, self.processor.vocab_size)\n",
    "\n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=self.config['learning_rate'],\n",
    "            decay_steps=10000\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        if self.config['enable_checkpointing']:\n",
    "            ckpt_dir = self.config['checkpoint_dir']\n",
    "            os.makedirs(ckpt_dir, exist_ok=True)\n",
    "            ckpt = tf.train.Checkpoint(\n",
    "                encoder=self.encoder,\n",
    "                decoder=self.decoder,\n",
    "                optimizer=self.optimizer\n",
    "            )\n",
    "            self.ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=3)\n",
    "\n",
    "            if self.config['restore_checkpoint'] and self.ckpt_manager.latest_checkpoint:\n",
    "                ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "                print(f\"Restored from {self.ckpt_manager.latest_checkpoint}\")\n",
    "\n",
    "    def train_step(self, img_tensor, target, cap_len):\n",
    "        batch_size = tf.shape(img_tensor)[0]\n",
    "        hidden = tf.zeros((batch_size, self.config['units']))\n",
    "        cell = tf.zeros_like(hidden)\n",
    "        start_tok = self.processor.tokenizer.word_index['<start>']\n",
    "        dec_input = tf.expand_dims(tf.repeat(start_tok, batch_size), 1)\n",
    "\n",
    "        attention_accum = None\n",
    "        total_ce_loss = 0.0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)\n",
    "\n",
    "            for t in tf.range(1, self.config['max_length']):\n",
    "                logits, hidden, cell, alpha = self.decoder(dec_input, features, hidden, cell)\n",
    "                attention_accum = alpha if attention_accum is None else attention_accum + alpha\n",
    "\n",
    "                ce_t = self.loss_fn(target[:, t], tf.squeeze(logits, 1))\n",
    "                mask = tf.cast(target[:, t] > 0, tf.float32)\n",
    "                total_ce_loss += tf.reduce_sum(ce_t * mask)\n",
    "\n",
    "                pred_ids = tf.argmax(logits, -1, output_type=tf.int32)\n",
    "                pred_ids = tf.squeeze(pred_ids, -1)\n",
    "                ss_mask = tf.random.uniform((batch_size,)) < self.ss_prob\n",
    "                next_ids = tf.where(ss_mask, pred_ids, target[:, t])\n",
    "                dec_input = tf.expand_dims(next_ids, 1)\n",
    "\n",
    "            total_tokens = tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "            ce_loss = total_ce_loss / total_tokens\n",
    "            reg_loss = tf.reduce_mean(tf.square(1.0 - attention_accum))\n",
    "            loss = ce_loss + self.config['attention_reg_lambda'] * reg_loss\n",
    "\n",
    "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(grads, variables))\n",
    "        return loss\n",
    "\n",
    "    def plot_attention(self, image_path, caption, alphas):\n",
    "        img = np.array(Image.open(image_path))\n",
    "        orig_h, orig_w = img.shape[:2]\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        # Attention grid is 8x8 for InceptionV3 mixed10\n",
    "        grid_size = int(np.sqrt(alphas[0].shape[0]))\n",
    "        \n",
    "        for t in range(len(caption)):\n",
    "            ax = fig.add_subplot(3, int(np.ceil(len(caption)/3)), t+1)\n",
    "            ax.set_title(caption[t])\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            alpha = alphas[t].reshape(grid_size, grid_size)\n",
    "            alpha_resized = np.array(Image.fromarray(alpha).resize(\n",
    "                (orig_w, orig_h), Image.BILINEAR))\n",
    "            ax.imshow(alpha_resized, cmap='viridis', alpha=0.6, extent=(0, orig_w, orig_h, 0))\n",
    "            ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def analyze_generations(self, data, max_samples=100):\n",
    "        word_counts = Counter()\n",
    "        unk_count = 0\n",
    "        data_subset = random.sample(data, min(max_samples, len(data)))\n",
    "        \n",
    "        for img, _ in tqdm.tqdm(data_subset):\n",
    "            caption = self.greedy_decode(os.path.join(self.config['image_dir'], img))\n",
    "            word_counts.update(caption)\n",
    "            unk_count += sum(1 for w in caption if w == '<unk>')\n",
    "        \n",
    "        print(\"\\nTop 20 generated words:\")\n",
    "        for word, count in word_counts.most_common(20):\n",
    "            print(f\"{word}: {count}\")\n",
    "        \n",
    "        print(f\"\\nUNK tokens: {unk_count} ({unk_count/sum(word_counts.values()):.1%})\")\n",
    "        return word_counts\n",
    "\n",
    "    def train(self, train_ds, val_data, epochs=None):\n",
    "        if epochs is None:\n",
    "            epochs = self.config['epochs']\n",
    "\n",
    "        if self.bleu_subset_idx is None:\n",
    "            total_train = len(self.processor.train_data)\n",
    "            subset_size = min(200, total_train)\n",
    "            self.bleu_subset_idx = random.sample(range(total_train), subset_size)\n",
    "\n",
    "        def _subset(data, idx):\n",
    "            return [data[i] for i in idx]\n",
    "\n",
    "        patience = self.config['patience']\n",
    "        wait = 0\n",
    "        self.ss_max_prob = self.config['scheduled_sampling_max_prob']\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.ss_prob = self.ss_max_prob * epoch / max(1, epochs - 1)\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs} (ε={self.ss_prob:.3f})\")\n",
    "\n",
    "            start = time.time()\n",
    "            total_loss, step = 0.0, 0\n",
    "            progbar = tf.keras.utils.Progbar(None)\n",
    "\n",
    "            for batch, (img_tensor, target, cap_len) in enumerate(train_ds):\n",
    "                batch_loss = self.train_step(img_tensor, target, cap_len)\n",
    "                total_loss += batch_loss\n",
    "                progbar.update(batch + 1, [('loss', batch_loss)])\n",
    "                step += 1\n",
    "\n",
    "            avg_loss = total_loss / step\n",
    "            self.train_loss_log.append(float(avg_loss))\n",
    "\n",
    "            train_subset = _subset(self.processor.train_data, self.bleu_subset_idx)\n",
    "            train_bleu = self.evaluate_bleu(train_subset)['bleu-4']\n",
    "            self.train_bleu_log.append(train_bleu)\n",
    "\n",
    "            val_bleu = self.evaluate_bleu(val_data)['bleu-4']\n",
    "            self.val_bleu_log.append(val_bleu)\n",
    "\n",
    "            if self.config['enable_checkpointing']:\n",
    "                self.ckpt_manager.save()\n",
    "\n",
    "            if val_bleu > self.best_bleu:\n",
    "                self.best_bleu = val_bleu\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: loss={avg_loss:.4f} \"\n",
    "                  f\"train-BLEU={train_bleu:.4f} val-BLEU={val_bleu:.4f} \"\n",
    "                  f\"time={time.time()-start:.1f}s\", flush=True)\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"Print model summaries for Encoder, Attention, and Decoder.\"\"\"\n",
    "        print(\"Building model summaries...\")\n",
    "\n",
    "        # Dummy inputs\n",
    "        dummy_image = tf.random.uniform((1, 299, 299, 3))\n",
    "        dummy_features = tf.random.uniform((1, 64, 2048))\n",
    "        dummy_hidden = tf.zeros((1, self.config['units']))\n",
    "        dummy_cell = tf.zeros((1, self.config['units']))\n",
    "        dummy_token = tf.zeros((1, 1), dtype=tf.int32)\n",
    "\n",
    "        # --- Encoder Summary ---\n",
    "        print(\"\\nEncoder Summary:\")\n",
    "        self.encoder(dummy_image)\n",
    "        self.encoder.summary()\n",
    "\n",
    "        # --- Bahdanau Attention Summary ---\n",
    "        print(\"\\nBahdanau Attention Summary:\")\n",
    "        attention_layer = BahdanauAttention(self.config['units'])\n",
    "        features_input = tf.keras.Input(shape=(64, 2048), name=\"features\")\n",
    "        hidden_input = tf.keras.Input(shape=(self.config['units'],), name=\"hidden\")\n",
    "        context_vector, attn_weights = attention_layer(features_input, hidden_input)\n",
    "        attention_model = tf.keras.Model(inputs=[features_input, hidden_input], outputs=[context_vector, attn_weights])\n",
    "        attention_model.summary()\n",
    "\n",
    "        # --- Decoder Summary ---\n",
    "        print(\"\\nDecoder Summary:\")\n",
    "        self.decoder(dummy_token, dummy_features, dummy_hidden, dummy_cell)\n",
    "        self.decoder.summary()\n",
    "\n",
    "    def beam_search_decode(self,\n",
    "                           image_path: str,\n",
    "                           beam_size: int = 5,\n",
    "                           length_penalty: float = 0.7,\n",
    "                           return_attention: bool = False):\n",
    "        \"\"\"Beam-search with deterministic crop.\"\"\"\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0\n",
    "        )\n",
    "        base_features = self.encoder(img_tensor)       # (1,L,2048)\n",
    "\n",
    "        start_id = self.processor.tokenizer.word_index['<start>']\n",
    "        end_id   = self.processor.tokenizer.word_index['<end>']\n",
    "\n",
    "        beams = [{'seq':[start_id],\n",
    "                  'score':0.0,\n",
    "                  'hidden':tf.zeros((1,self.config['units'])),\n",
    "                  'cell':tf.zeros((1,self.config['units'])),\n",
    "                  'alphas':[]}]\n",
    "\n",
    "        completed = []\n",
    "        for _ in range(self.config['max_length']):\n",
    "            candidates = []\n",
    "            for b in beams:\n",
    "                last_id = b['seq'][-1]\n",
    "                if last_id == end_id:\n",
    "                    completed.append(b); continue\n",
    "                dec_in = tf.expand_dims([last_id], 0)\n",
    "                logits, h, c, alpha = self.decoder(dec_in, base_features,\n",
    "                                                   b['hidden'], b['cell'])\n",
    "                log_probs = tf.nn.log_softmax(logits[0,0])\n",
    "                top_ids = tf.math.top_k(log_probs, k=beam_size).indices.numpy()\n",
    "                for tok in top_ids:\n",
    "                    tok = int(tok)\n",
    "                    candidates.append({\n",
    "                        'seq':   b['seq']+[tok],\n",
    "                        'score': b['score']+float(log_probs[tok]),\n",
    "                        'hidden':h,\n",
    "                        'cell':  c,\n",
    "                        'alphas':b['alphas']+[alpha[0].numpy()]})\n",
    "            if not candidates: break\n",
    "            def lp(b): return b['score']/(len(b['seq'])**length_penalty)\n",
    "            candidates.sort(key=lp, reverse=True)\n",
    "            beams = candidates[:beam_size]\n",
    "            if len(completed) >= beam_size: break\n",
    "\n",
    "        best = max(completed+beams,\n",
    "                   key=lambda b: b['score']/(len(b['seq'])**length_penalty))\n",
    "        words = [self.processor.tokenizer.index_word.get(i,'')\n",
    "                 for i in best['seq']\n",
    "                 if self.processor.tokenizer.index_word.get(i,'') not in\n",
    "                 ('<start>','<end>','<unk>')]\n",
    "        return (words, best['alphas']) if return_attention else words\n",
    "\n",
    "    def greedy_decode(self, image_path: str, return_attention=False):\n",
    "        \"\"\"Generate caption via greedy decoding (deterministic crop).\"\"\"\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0\n",
    "        )\n",
    "\n",
    "        features = self.encoder(img_tensor)\n",
    "        hidden = tf.zeros((1, self.config['units']))\n",
    "        cell   = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims(\n",
    "            [self.processor.tokenizer.word_index['<start>']], 0\n",
    "        )\n",
    "\n",
    "        result, alphas = [], []\n",
    "        for _ in range(self.config['max_length']):\n",
    "            logits, hidden, cell, alpha = self.decoder(\n",
    "                dec_input, features, hidden, cell\n",
    "            )\n",
    "            pred_id = tf.argmax(logits[0, 0]).numpy()\n",
    "            word = self.processor.tokenizer.index_word.get(pred_id, '')\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            if word not in ('<start>', '<unk>'):\n",
    "                result.append(word)\n",
    "            alphas.append(alpha[0].numpy())\n",
    "            dec_input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "        return (result, alphas) if return_attention else result\n",
    "\n",
    "    def evaluate_bleu(self, test_data, max_samples=None):\n",
    "        \"\"\"Calculate BLEU scores on test data.\"\"\"\n",
    "        refs, hyps = [], []\n",
    "        data_to_eval = test_data[:max_samples] if max_samples else test_data\n",
    "        \n",
    "        for img_name, _ in tqdm.tqdm(data_to_eval):\n",
    "            image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            hyp = self.greedy_decode(image_path)\n",
    "            \n",
    "            # Process ground truth captions\n",
    "            gt = [self.processor.preprocess_caption(c).split() for c in self.processor.captions_dict[img_name][:5]]\n",
    "            gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "            \n",
    "            refs.append(gt)\n",
    "            hyps.append(hyp)\n",
    "        \n",
    "        # Calculate BLEU scores for different n-grams\n",
    "        bleu_scores = {}\n",
    "        for i in range(1, 5):\n",
    "            weights = tuple([1.0/i]*i + [0.0]*(4-i))\n",
    "            score = corpus_bleu(refs, hyps, weights=weights, smoothing_function=self.smoothie)\n",
    "            bleu_scores[f'bleu-{i}'] = score\n",
    "            print(f\"BLEU-{i}: {score:.4f}\")\n",
    "        \n",
    "        return bleu_scores\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot loss curve **and** both train/val BLEU-4 curves.\"\"\"\n",
    "        plt.figure(figsize=(14, 5))\n",
    "\n",
    "        # --- left: training loss ---\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_log, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cross-Entropy Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # --- right: BLEU-4 ---\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if self.train_bleu_log:\n",
    "            plt.plot(self.train_bleu_log, label='Train BLEU-4')\n",
    "        plt.plot(self.val_bleu_log,   label='Val BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4')\n",
    "        plt.title('BLEU-4 Scores')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Generate speech audio from caption text.\"\"\"\n",
    "        if not caption:\n",
    "            print(\"Empty caption, nothing to speak\")\n",
    "            return\n",
    "            \n",
    "        tts = gTTS(text=caption, lang='en')\n",
    "        tts.save(filename)\n",
    "        display(Audio(filename))\n",
    "        print(f\"Audio saved to {filename}\")\n",
    "    \n",
    "    def demo(self,\n",
    "             image_path: str,\n",
    "             filename: str = \"caption_audio.mp3\",\n",
    "             beam_size: int = 5,\n",
    "             length_penalty: float = 0.7):\n",
    "        \"\"\"\n",
    "        End-to-end demo (beam-search inference) in the following order:\n",
    "          1. Original image\n",
    "          2. Ground-truth captions\n",
    "          3. Generated caption\n",
    "          4. Audio playback\n",
    "          5. Attention heat-maps\n",
    "        \"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return\n",
    "\n",
    "        # ---------- 1. original image ----------\n",
    "        img = Image.open(image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # ---------- 2. ground-truth captions ----------\n",
    "        img_name = os.path.basename(image_path)\n",
    "        gt_caps = self.processor.captions_dict.get(img_name, [])\n",
    "        if gt_caps:\n",
    "            print(\"Ground-truth captions:\")\n",
    "            for cap in gt_caps:\n",
    "                print(f\"- {cap}\")\n",
    "        else:\n",
    "            print(\"No ground-truth captions found.\")\n",
    "\n",
    "        # ---------- 3. caption generation ----------\n",
    "        words, attention = self.beam_search_decode(\n",
    "            image_path,\n",
    "            beam_size=beam_size,\n",
    "            length_penalty=length_penalty,\n",
    "            return_attention=True\n",
    "        )\n",
    "        caption = \" \".join(words)\n",
    "        print(\"\\nGenerated caption:\")\n",
    "        print(caption)\n",
    "\n",
    "        # ---------- 4. audio ----------\n",
    "        self.speak_caption(caption, filename=filename)\n",
    "\n",
    "        # ---------- 5. attention plot ----------\n",
    "        self.plot_attention(image_path, words, attention)\n",
    "\n",
    "    def prime_dataset(self, ds, steps: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Pre-fill a tf.data shuffle buffer so the first training epoch\n",
    "        starts without the usual “Filling up shuffle buffer …” pause.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        ds    : the *un-iterated* tf.data.Dataset you’ll pass to train()\n",
    "        steps : number of iterator steps to advance; default uses\n",
    "                buffer_size // batch_size + 1 from config.\n",
    "        \"\"\"\n",
    "        if steps is None:\n",
    "            steps = self.config['buffer_size'] // self.config['batch_size'] + 1\n",
    "\n",
    "        it = iter(ds)\n",
    "        for _ in range(steps):\n",
    "            try:\n",
    "                next(it)\n",
    "            except StopIteration:  # dataset shorter than requested priming\n",
    "                break\n",
    "\n",
    "    def fine_tune_cnn(self,\n",
    "                      train_ds,\n",
    "                      val_data,\n",
    "                      layers_to_unfreeze: int = 2,\n",
    "                      lr: float = 1e-5,\n",
    "                      epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Phase-2 fine-tuning of the top Inception blocks.\n",
    "        Call after initial caption training for an extra accuracy bump.\n",
    "        \"\"\"\n",
    "        print(f\"\\nUnfreezing top {layers_to_unfreeze} Inception blocks …\")\n",
    "        self.encoder.unfreeze_top_layers(layers_to_unfreeze)\n",
    "\n",
    "        # New, low learning-rate optimiser\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        print(f\"Fine-tuning CNN for {epochs} epoch(s) at lr={lr} …\")\n",
    "        self.train(train_ds, val_data, epochs=epochs)\n",
    "\n",
    "        print(\"CNN fine-tune finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "processor = DataProcessor(CONFIG)\n",
    "processor.load_captions()\n",
    "processor.prepare_captions()\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds = processor.build_dataset(processor.train_data)\n",
    "val_ds = processor.build_dataset(processor.val_data, shuffle=False)\n",
    "\n",
    "# Build and train model\n",
    "model = ImageCaptioningModel(CONFIG, processor)\n",
    "model.build_model()\n",
    "\n",
    "# Optional: Analyze before training\n",
    "model.analyze_generations(processor.train_data)\n",
    "\n",
    "# Train the model\n",
    "model.train(train_ds, processor.val_data)\n",
    "\n",
    "# Optional: Fine-tune CNN\n",
    "model.fine_tune_cnn(train_ds, processor.val_data, \n",
    "                   layers_to_unfreeze=8, lr=1e-5, epochs=5)\n",
    "\n",
    "# Evaluate\n",
    "model.evaluate_bleu(processor.test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
