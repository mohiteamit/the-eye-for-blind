{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye for Blind – Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "Eye for Blind: An Assistive Image Captioning System with Visual Attention\n",
    "\n",
    "This project implements a deep learning model that generates natural language descriptions of images, particularly aimed at visually impaired users. The model leverages an attention mechanism to selectively focus on image regions when generating each word, mimicking human vision.\n",
    "\n",
    "Inspired by \"Show, Attend and Tell\" (Xu et al., 2015), this implementation:\n",
    "1. Uses a CNN encoder (InceptionV3) to extract image features.\n",
    "2. Applies additive (Bahdanau) attention during decoding.\n",
    "3. Employs a decoder LSTM to generate captions.\n",
    "4. Converts generated captions to speech using gTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# 0 (default): All messages (INFO, WARNING, ERROR) are logged.\n",
    "# 1: INFO messages are not printed.\n",
    "# 2: INFO and WARNING messages are not printed.\n",
    "# 3: INFO, WARNING, and ERROR messages are not printed.\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf #type: ignore\n",
    "from tensorflow.keras import layers, Model #type: ignore\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay #type: ignore\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy #type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #type: ignore\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction #type: ignore\n",
    "from gtts import gTTS #type: ignore\n",
    "from IPython.display import Audio, display\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Data\n",
    "    'image_dir': '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file': '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    'subset_ratio': 1.0,\n",
    "\n",
    "    # Vocabulary\n",
    "    'vocab_min_count': 5,\n",
    "    'max_length': 30,\n",
    "\n",
    "    # Model\n",
    "    'embedding_dim': 512,\n",
    "    'units': 1024,\n",
    "    'decoder_dropout': 0.3,\n",
    "\n",
    "    # Optimiser / schedule\n",
    "    'learning_rate': 5e-4,\n",
    "    'grad_clip_value': 10.0,\n",
    "    'scheduled_sampling_max_prob': 0.15,\n",
    "    'mixed_precision': True,            # RTX 6000 Ada\n",
    "\n",
    "    # Training loop\n",
    "    'epochs': 30,\n",
    "    'batch_size': 128,\n",
    "    'buffer_size': 10000,\n",
    "    'early_stop': True,\n",
    "    'patience': 15,\n",
    "\n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': './checkpoints/full_run',\n",
    "    'save_checkpoints': True,\n",
    "    'delete_old_checkpoints': True,\n",
    "\n",
    "    # Regulariser\n",
    "    'attention_reg_lambda': 1.0,\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "\n",
    "# Mixed precision policy - RTX 6000 Ada has excellent mixed precision support\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision enabled for RTX 6000 Ada\")\n",
    "\n",
    "# Single GPU setup\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Enable memory growth for RTX 6000 Ada\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    # Use default strategy for single GPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Using single GPU: {physical_devices[0].name}, batch size={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Constants\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.captions_dict = dict()\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        self.test_data = []\n",
    "    \n",
    "    def load_captions(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load and convert pipe-delimited Flickr-style caption file to a dict.\"\"\"\n",
    "        print(f\"Loading captions from {self.config['caption_file']}\")\n",
    "        df = pd.read_csv(self.config['caption_file'], sep='|', header=None, \n",
    "                         names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "        \n",
    "        caption_map = {}\n",
    "        for img, group in df.groupby('image_name'):\n",
    "            caption_map[img] = group['comment'].tolist()\n",
    "        \n",
    "        self.captions_dict = caption_map\n",
    "        print(f\"Loaded {len(caption_map)} images with captions\")\n",
    "        return caption_map\n",
    "    \n",
    "    def display_samples(self, num_samples: int = 3):\n",
    "        \"\"\"Display random images with all their associated captions.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        sample_keys = random.sample(list(self.captions_dict.keys()), min(num_samples, len(self.captions_dict)))\n",
    "\n",
    "        for key in sample_keys:\n",
    "            img_path = os.path.join(self.config['image_dir'], key)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(key)\n",
    "                plt.show()\n",
    "\n",
    "                for cap in self.captions_dict[key]:\n",
    "                    print(f\"- {cap}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {key}: {e}\")\n",
    "\n",
    "    def preprocess_caption(self, caption: str) -> Optional[str]:\n",
    "        \"\"\"Clean and format caption text.\"\"\"\n",
    "        if caption is None or not isinstance(caption, str):\n",
    "            return None\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z0-9.,? ]\", \"\", caption)\n",
    "        return f\"<start> {caption.strip()} <end>\"\n",
    "\n",
    "    def prepare_captions(self, subset_ratio=1.0):\n",
    "        \"\"\"Process captions, build tokenizer & train/val/test splits.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        # --- 1. clean & tag ----------------------------------------------------\n",
    "        all_captions = []\n",
    "        for caps in self.captions_dict.values():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p:\n",
    "                    all_captions.append(p)\n",
    "\n",
    "        word_counts = Counter(w for cap in all_captions for w in cap.split())\n",
    "        valid_words = {w for w, cnt in word_counts.items()\n",
    "                    if cnt >= self.config['vocab_min_count']}\n",
    "\n",
    "        def keep(c):\n",
    "            return all(w in valid_words or w in ('<start>', '<end>') for w in c.split())\n",
    "\n",
    "        filtered = [c for c in all_captions if keep(c)]\n",
    "\n",
    "        # --- 2. determine max length ------------------------------------------\n",
    "        lengths = [len(c.split()) for c in filtered]\n",
    "        self.config['max_length'] = int(np.percentile(lengths, 95))\n",
    "        print(f\"max_length set to {self.config['max_length']}\")\n",
    "\n",
    "        # --- 3. build tokenizer (NO filters so < and > stay) -------------------\n",
    "        tokenizer = Tokenizer(oov_token=\"<unk>\", filters='', lower=True)\n",
    "        tokenizer.fit_on_texts(filtered)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(f\"vocab size = {self.vocab_size}\")\n",
    "\n",
    "        # --- 4. Build (image, caption) pairs ------------------------------------\n",
    "        pairs = []\n",
    "        for img, caps in self.captions_dict.items():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p and keep(p):\n",
    "                    pairs.append((img, p))\n",
    "\n",
    "        # --- 5. Force fixed number of images if requested -----------------------\n",
    "        if 'force_subset_images' in self.config and self.config['force_subset_images']:\n",
    "            requested = self.config['force_subset_images']\n",
    "            print(f\"Forcing subset of exactly {requested} images...\")\n",
    "            all_imgs = list({img for img, _ in pairs})\n",
    "            if requested > len(all_imgs):\n",
    "                raise ValueError(f\"Requested {requested} images, but only {len(all_imgs)} available.\")\n",
    "            selected_imgs = set(random.sample(all_imgs, requested))\n",
    "            pairs = [(img, cap) for img, cap in pairs if img in selected_imgs]\n",
    "            print(f\"Subset contains {len(pairs)} (image, caption) pairs from {requested} images.\")\n",
    "        \n",
    "        # --- 6. Subset by ratio if no forced image count ------------------------\n",
    "        elif subset_ratio < 1.0:\n",
    "            orig_len = len(pairs)\n",
    "            pairs = pairs[:int(len(pairs) * subset_ratio)]\n",
    "            print(f\"Subset by ratio: {len(pairs)} pairs from {orig_len}\")\n",
    "\n",
    "        # --- 7. Split into train/val/test ---------------------------------------\n",
    "        random.shuffle(pairs)\n",
    "        n = len(pairs)\n",
    "        self.train_data, self.val_data, self.test_data = (\n",
    "            pairs[:int(0.8 * n)],\n",
    "            pairs[int(0.8 * n):int(0.9 * n)],\n",
    "            pairs[int(0.9 * n):]\n",
    "        )\n",
    "        print(f\"split → train {len(self.train_data)} | val {len(self.val_data)} | test {len(self.test_data)}\")\n",
    "\n",
    "        return filtered\n",
    "\n",
    "        \n",
    "    def encode_caption(self, caption: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Convert caption text to sequence of token ids.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call prepare_captions first.\")\n",
    "        \n",
    "        seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=self.config['max_length'], padding='post')[0]\n",
    "        return padded_seq, len(seq)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def _base_decode(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)          # [0,1]\n",
    "        return img                                                   # (h,w,3)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image_train(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Augment + preprocess (training only).\"\"\"\n",
    "        img = self._base_decode(path)\n",
    "        img = tf.image.random_flip_left_right(img)                   # aug ①\n",
    "\n",
    "        # resize shorter side→342 then *random* crop 299×299\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = 342. / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.random_crop(img, size=[299, 299, 3])          # aug ②\n",
    "\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [299, 299, 3])\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image_eval(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Deterministic centre-crop (validation / inference).\"\"\"\n",
    "        img = self._base_decode(path)\n",
    "\n",
    "        # resize shorter side→342 then *central* crop 299×299\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = 342. / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.resize_with_crop_or_pad(img, 299, 299)\n",
    "\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [299, 299, 3])\n",
    "\n",
    "    def data_generator(self, data):\n",
    "        \"\"\"\n",
    "        Yield (image_tensor, token_ids, caption_len, filename)\n",
    "        so the filename is always available for debug/analysis.\n",
    "        \"\"\"\n",
    "        for img, cap in data:\n",
    "            img_path  = os.path.join(self.config['image_dir'], img)\n",
    "            img_tensor = self.load_image_train(tf.convert_to_tensor(img_path))\n",
    "            token_ids, cap_len = self.encode_caption(cap)\n",
    "            yield img_tensor, token_ids, cap_len, img \n",
    "    \n",
    "    def build_dataset(self, data,\n",
    "                      shuffle=True, cache=True, training: bool = True):\n",
    "\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((299, 299, 3), tf.float32),               # image\n",
    "            tf.TensorSpec((self.config['max_length'],), tf.int32),  # token ids\n",
    "            tf.TensorSpec((), tf.int32),                            # caption len\n",
    "            tf.TensorSpec((), tf.string)                            # filename\n",
    "        )\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: self.data_generator(data),\n",
    "            output_signature=output_signature)\n",
    "\n",
    "        if cache:\n",
    "            ds = ds.cache()\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(self.config['buffer_size'])\n",
    "\n",
    "        ds = ds.batch(self.config['batch_size'])\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"Prepare all datasets for training/validation/testing.\"\"\"\n",
    "        if not self.train_data:\n",
    "            self.prepare_captions()\n",
    "\n",
    "        print(\"Building datasets...\")\n",
    "        train_ds = self.build_dataset(self.train_data)\n",
    "        val_ds = self.build_dataset(self.val_data)\n",
    "        test_ds = self.build_dataset(self.test_data, shuffle=False)\n",
    "        \n",
    "        return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    \"\"\"\n",
    "    Inception-V3 feature extractor with an optional\n",
    "    `unfreeze_top_layers()` helper for later fine-tuning.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"encoder\")\n",
    "        base = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, weights='imagenet',\n",
    "            input_shape=(299, 299, 3))\n",
    "        base.trainable = False                                      # phase-1: frozen\n",
    "        self.cnn = Model(inputs=base.input, outputs=base.get_layer('mixed10').output)\n",
    "        self.reshape = layers.Reshape((-1, 2048))                  # L=64 for 8×8 grid\n",
    "\n",
    "    def unfreeze_top_layers(self, n: int = 2):\n",
    "        \"\"\"\n",
    "        Fine-tune: unfreeze the last *n* Inception blocks (default: mixed9 & mixed10).\n",
    "        Call **after** initial caption training for best accuracy.\n",
    "        \"\"\"\n",
    "        for layer in self.cnn.layers[-n:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    def call(self, x):                                             # (B,299,299,3)\n",
    "        x = self.cnn(x)                                            # (B,8,8,2048)\n",
    "        return self.reshape(x)                                     # (B,64,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "    \n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    \"\"\"\n",
    "    Attention decoder with configurable dropout via config.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int, units: int, vocab_size: int, dropout: float = 0.5):\n",
    "        super().__init__(name=\"decoder\")\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.f_beta    = layers.Dense(1, activation=\"sigmoid\")\n",
    "        self.lstm      = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.dropout   = layers.Dropout(dropout)\n",
    "\n",
    "        self.deep_proj = layers.Dense(units * 2)\n",
    "        self.fc        = layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context, alpha = self.attention(features, hidden)\n",
    "        context = self.f_beta(hidden) * context\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        lstm_input = tf.concat([tf.expand_dims(context, 1), x], -1)\n",
    "\n",
    "        hidden = tf.cast(hidden, lstm_input.dtype)\n",
    "        cell   = tf.cast(cell, lstm_input.dtype)\n",
    "\n",
    "        lstm_out, h_t, c_t = self.lstm(lstm_input, initial_state=[hidden, cell])\n",
    "        lstm_out = tf.squeeze(lstm_out, 1)\n",
    "\n",
    "        proj = self.deep_proj(tf.concat([lstm_out, context], -1))\n",
    "        proj = tf.reshape(proj, (-1, self.units, 2))\n",
    "        maxout = tf.reduce_max(proj, axis=-1)\n",
    "        maxout = self.dropout(maxout)\n",
    "\n",
    "        logits = self.fc(maxout)\n",
    "        return tf.expand_dims(logits, 1), h_t, c_t, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    def __init__(self, config, processor):\n",
    "        self.config          = config\n",
    "        self.processor       = processor\n",
    "        self.encoder         = None\n",
    "        self.decoder         = None\n",
    "        self.optimizer       = None\n",
    "        self.loss_fn         = None\n",
    "        self.ckpt_manager    = None\n",
    "\n",
    "        self.best_bleu       = 0.0\n",
    "        self.train_loss_log  = []\n",
    "        self.train_bleu_log  = []\n",
    "        self.val_bleu_log    = []\n",
    "        self.bleu_subset_idx = None\n",
    "        self.grad_norm_log = []  \n",
    "\n",
    "        self.smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Single-GPU build with slower cosine decay.\"\"\"\n",
    "        print(\"Building model …\")\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim=self.config['embedding_dim'],\n",
    "            units=self.config['units'],\n",
    "            vocab_size=self.processor.vocab_size,\n",
    "            dropout=self.config.get('decoder_dropout', 0.5)\n",
    "        )\n",
    "\n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=self.config['learning_rate'],\n",
    "            decay_steps=50_000)          # stretched cosine\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.loss_fn   = SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                    reduction='none')\n",
    "\n",
    "        if not self.config.get('save_checkpoints', True):\n",
    "            self.ckpt_manager = None\n",
    "            print(\"Checkpointing disabled.\")\n",
    "            return\n",
    "\n",
    "        ckpt = tf.train.Checkpoint(encoder=self.encoder,\n",
    "                                decoder=self.decoder,\n",
    "                                optimizer=self.optimizer)\n",
    "        ckpt_dir  = self.config['checkpoint_dir']\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            ckpt, ckpt_dir,\n",
    "            max_to_keep = 1 if self.config.get('delete_old_checkpoints', True) else 5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(\"Restored:\", self.ckpt_manager.latest_checkpoint)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print model summaries for Encoder, Attention, and Decoder.\"\"\"\n",
    "        print(\"Building model summaries...\")\n",
    "\n",
    "        # Dummy inputs\n",
    "        dummy_image = tf.random.uniform((1, 299, 299, 3))\n",
    "        dummy_features = tf.random.uniform((1, 64, 2048))\n",
    "        dummy_hidden = tf.zeros((1, self.config['units']))\n",
    "        dummy_cell = tf.zeros((1, self.config['units']))\n",
    "        dummy_token = tf.zeros((1, 1), dtype=tf.int32)\n",
    "\n",
    "        # --- Encoder Summary ---\n",
    "        print(\"\\nEncoder Summary:\")\n",
    "        self.encoder(dummy_image)\n",
    "        self.encoder.summary()\n",
    "\n",
    "        # --- Bahdanau Attention Summary ---\n",
    "        print(\"\\nBahdanau Attention Summary:\")\n",
    "        attention_layer = BahdanauAttention(self.config['units'])\n",
    "        features_input = tf.keras.Input(shape=(64, 2048), name=\"features\")\n",
    "        hidden_input = tf.keras.Input(shape=(self.config['units'],), name=\"hidden\")\n",
    "        context_vector, attn_weights = attention_layer(features_input, hidden_input)\n",
    "        attention_model = tf.keras.Model(inputs=[features_input, hidden_input], outputs=[context_vector, attn_weights])\n",
    "        attention_model.summary()\n",
    "\n",
    "        # --- Decoder Summary ---\n",
    "        print(\"\\nDecoder Summary:\")\n",
    "        self.decoder(dummy_token, dummy_features, dummy_hidden, dummy_cell)\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, img_tensor, target, cap_len):\n",
    "        \"\"\"\n",
    "        Returns: (loss, grad_norm)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(img_tensor)[0]\n",
    "        hidden     = tf.zeros((batch_size, self.config['units']))\n",
    "        cell       = tf.zeros_like(hidden)\n",
    "\n",
    "        start_tok  = self.processor.tokenizer.word_index['<start>']\n",
    "        dec_input  = tf.expand_dims(tf.repeat(start_tok, batch_size), 1)\n",
    "\n",
    "        attention_accum = None\n",
    "        total_ce_loss   = 0.0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)\n",
    "\n",
    "            for t in tf.range(1, self.config['max_length']):\n",
    "                logits, hidden, cell, alpha = self.decoder(\n",
    "                    dec_input, features, hidden, cell)\n",
    "\n",
    "                attention_accum = (alpha if attention_accum is None\n",
    "                                else attention_accum + alpha)\n",
    "\n",
    "                ce_t  = self.loss_fn(target[:, t], tf.squeeze(logits, 1))\n",
    "                mask  = tf.cast(target[:, t] > 0, tf.float32)\n",
    "                total_ce_loss += tf.reduce_sum(ce_t * mask)\n",
    "\n",
    "                pred_ids = tf.argmax(logits, -1, output_type=tf.int32)\n",
    "                pred_ids = tf.squeeze(pred_ids, -1)\n",
    "\n",
    "                ss_mask = tf.random.uniform((batch_size,)) < self.ss_prob\n",
    "                next_ids = tf.where(ss_mask, pred_ids, target[:, t])\n",
    "                dec_input = tf.expand_dims(next_ids, 1)\n",
    "\n",
    "            total_tokens = tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "            ce_loss      = total_ce_loss / total_tokens\n",
    "            reg_loss     = tf.reduce_mean(tf.square(1.0 - attention_accum))\n",
    "            loss         = ce_loss + self.config.get('attention_reg_lambda', 1.0) * reg_loss\n",
    "\n",
    "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads     = tape.gradient(loss, variables)\n",
    "        grads, _  = tf.clip_by_global_norm(grads, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "        grad_norm = tf.linalg.global_norm(grads)   # scalar tensor\n",
    "        return loss, grad_norm\n",
    "\n",
    "    def beam_search_decode(self,\n",
    "                           image_path: str,\n",
    "                           beam_size: int = 5,\n",
    "                           length_penalty: float = 0.7,\n",
    "                           return_attention: bool = False):\n",
    "        \"\"\"Beam-search with deterministic crop.\"\"\"\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0\n",
    "        )\n",
    "        base_features = self.encoder(img_tensor)       # (1,L,2048)\n",
    "\n",
    "        start_id = self.processor.tokenizer.word_index['<start>']\n",
    "        end_id   = self.processor.tokenizer.word_index['<end>']\n",
    "\n",
    "        beams = [{'seq':[start_id],\n",
    "                  'score':0.0,\n",
    "                  'hidden':tf.zeros((1,self.config['units'])),\n",
    "                  'cell':tf.zeros((1,self.config['units'])),\n",
    "                  'alphas':[]}]\n",
    "\n",
    "        completed = []\n",
    "        for _ in range(self.config['max_length']):\n",
    "            candidates = []\n",
    "            for b in beams:\n",
    "                last_id = b['seq'][-1]\n",
    "                if last_id == end_id:\n",
    "                    completed.append(b); continue\n",
    "                dec_in = tf.expand_dims([last_id], 0)\n",
    "                logits, h, c, alpha = self.decoder(dec_in, base_features,\n",
    "                                                   b['hidden'], b['cell'])\n",
    "                log_probs = tf.nn.log_softmax(logits[0,0])\n",
    "                top_ids = tf.math.top_k(log_probs, k=beam_size).indices.numpy()\n",
    "                for tok in top_ids:\n",
    "                    tok = int(tok)\n",
    "                    candidates.append({\n",
    "                        'seq':   b['seq']+[tok],\n",
    "                        'score': b['score']+float(log_probs[tok]),\n",
    "                        'hidden':h,\n",
    "                        'cell':  c,\n",
    "                        'alphas':b['alphas']+[alpha[0].numpy()]})\n",
    "            if not candidates: break\n",
    "            def lp(b): return b['score']/(len(b['seq'])**length_penalty)\n",
    "            candidates.sort(key=lp, reverse=True)\n",
    "            beams = candidates[:beam_size]\n",
    "            if len(completed) >= beam_size: break\n",
    "\n",
    "        best = max(completed+beams,\n",
    "                   key=lambda b: b['score']/(len(b['seq'])**length_penalty))\n",
    "        words = [self.processor.tokenizer.index_word.get(i,'')\n",
    "                 for i in best['seq']\n",
    "                 if self.processor.tokenizer.index_word.get(i,'') not in\n",
    "                 ('<start>','<end>','<unk>')]\n",
    "        return (words, best['alphas']) if return_attention else words\n",
    "\n",
    "    def greedy_decode(self, image_path: str, return_attention=False):\n",
    "        \"\"\"Generate caption via greedy decoding (deterministic crop).\"\"\"\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0\n",
    "        )\n",
    "\n",
    "        features = self.encoder(img_tensor)\n",
    "        hidden = tf.zeros((1, self.config['units']))\n",
    "        cell   = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims(\n",
    "            [self.processor.tokenizer.word_index['<start>']], 0\n",
    "        )\n",
    "\n",
    "        result, alphas = [], []\n",
    "        for _ in range(self.config['max_length']):\n",
    "            logits, hidden, cell, alpha = self.decoder(\n",
    "                dec_input, features, hidden, cell\n",
    "            )\n",
    "            pred_id = tf.argmax(logits[0, 0]).numpy()\n",
    "            word = self.processor.tokenizer.index_word.get(pred_id, '')\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            if word not in ('<start>', '<unk>'):\n",
    "                result.append(word)\n",
    "            alphas.append(alpha[0].numpy())\n",
    "            dec_input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "        return (result, alphas) if return_attention else result\n",
    "\n",
    "    def evaluate_bleu(self, test_data, max_samples=None):\n",
    "        \"\"\"Calculate BLEU scores on test data.\"\"\"\n",
    "        refs, hyps = [], []\n",
    "        data_to_eval = test_data[:max_samples] if max_samples else test_data\n",
    "        \n",
    "        for img_name, _ in tqdm.tqdm(data_to_eval):\n",
    "            image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            hyp = self.greedy_decode(image_path)\n",
    "            \n",
    "            # Process ground truth captions\n",
    "            gt = [self.processor.preprocess_caption(c).split() for c in self.processor.captions_dict[img_name][:5]]\n",
    "            gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "            \n",
    "            refs.append(gt)\n",
    "            hyps.append(hyp)\n",
    "        \n",
    "        # Calculate BLEU scores for different n-grams\n",
    "        bleu_scores = {}\n",
    "        for i in range(1, 5):\n",
    "            weights = tuple([1.0/i]*i + [0.0]*(4-i))\n",
    "            score = corpus_bleu(refs, hyps, weights=weights, smoothing_function=self.smoothie)\n",
    "            bleu_scores[f'bleu-{i}'] = score\n",
    "            print(f\"BLEU-{i}: {score:.4f}\")\n",
    "        \n",
    "        return bleu_scores\n",
    "    \n",
    "    def train(self, train_ds, val_data, epochs=None, subset_size: int = 200, bleu_check = False):\n",
    "        \"\"\"\n",
    "        Train the captioning model, now logging gradient norms every batch.\n",
    "        \"\"\"\n",
    "\n",
    "        # ── 0. setup ─────────────────────────────────────────────────────────\n",
    "        if epochs is None:\n",
    "            epochs = self.config['epochs']\n",
    "\n",
    "        if self.bleu_subset_idx is None:\n",
    "            total_train = len(self.processor.train_data)\n",
    "            subset_size = min(subset_size, total_train)\n",
    "            self.bleu_subset_idx = random.sample(range(total_train), subset_size)\n",
    "\n",
    "        def _subset(data, idx):\n",
    "            return [data[i] for i in idx]\n",
    "\n",
    "        patience          = self.config.get('patience', 8)\n",
    "        wait              = 0\n",
    "        apply_early_stop  = self.config.get('early_stop', True)\n",
    "        self.ss_max_prob  = self.config.get('scheduled_sampling_max_prob', 0.0)\n",
    "\n",
    "        # clear grad-norm history for a fresh plot\n",
    "        self.grad_norm_log = []\n",
    "\n",
    "        # ── 1. epoch loop ────────────────────────────────────────────────────\n",
    "        for epoch in range(epochs):\n",
    "            self.ss_prob = self.ss_max_prob * (epoch / max(1, epochs - 1))\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}  (ε={self.ss_prob:.3f})\")\n",
    "\n",
    "            start_time      = time.time()\n",
    "            total_loss      = 0.0\n",
    "            latest_grad_norm = 0.0\n",
    "\n",
    "            prog = tf.keras.utils.Progbar(None, stateful_metrics=['loss'])\n",
    "\n",
    "            # ── 1.1 batch loop ───────────────────────────────────────────────\n",
    "            for batch, (img, tgt, cap_len, _) in enumerate(train_ds):\n",
    "                if batch == 0 and prog.target is None:\n",
    "                    # rough number of batches for nice progress bar scaling\n",
    "                    prog.target = len(self.processor.train_data) // self.config['batch_size'] + 1\n",
    "                \n",
    "                batch_loss, batch_norm = self.train_step(img, tgt, cap_len)\n",
    "\n",
    "                # Convert EagerTensors to Python floats for safe math / logging\n",
    "                total_loss       += batch_loss.numpy()\n",
    "                latest_grad_norm  = batch_norm.numpy()\n",
    "                self.grad_norm_log.append(latest_grad_norm)\n",
    "\n",
    "                prog.update(batch + 1, values=[('loss', batch_loss)])\n",
    "\n",
    "            avg_loss = total_loss / (batch + 1)\n",
    "            self.train_loss_log.append(avg_loss)\n",
    "\n",
    "            print(f\"Grad-norm (last step): {latest_grad_norm:.2f}\")\n",
    "\n",
    "            # ── 1.2 quick BLEU checks ───────────────────────────────────────\n",
    "            train_subset = _subset(self.processor.train_data, self.bleu_subset_idx)\n",
    "            train_bleu   = self.evaluate_bleu(train_subset)['bleu-4']\n",
    "            val_bleu     = self.evaluate_bleu(val_data)['bleu-4']\n",
    "            self.train_bleu_log.append(train_bleu)\n",
    "            self.val_bleu_log.append(val_bleu)\n",
    "\n",
    "            # ── 1.3 checkpoint ──────────────────────────────────────────────\n",
    "            if self.ckpt_manager:\n",
    "                self.ckpt_manager.save()\n",
    "\n",
    "            # ── 1.4 early-stopping ──────────────────────────────────────────\n",
    "            if val_bleu > self.best_bleu:\n",
    "                self.best_bleu = val_bleu\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if apply_early_stop and wait >= patience:\n",
    "                    print(f\"Early stopping (no BLEU gain for {wait} epochs)\")\n",
    "                    break\n",
    "\n",
    "            # current learning-rate\n",
    "            lr_now = float(self.optimizer.learning_rate(self.optimizer.iterations).numpy())\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                f\"loss={avg_loss:.4f}  \"\n",
    "                f\"trainBLEU={train_bleu:.4f}  valBLEU={val_bleu:.4f}  \"\n",
    "                f\"lr={lr_now:.2e}  \"\n",
    "                f\"time={time.time() - start_time:.1f}s\")\n",
    "\n",
    "        return self.train_loss_log, self.val_bleu_log\n",
    "    \n",
    "    def plot_attention(self, image_path: str, caption: list, alphas: list):\n",
    "        \"\"\"Improved visualization of attention with better contrast and 299x299 alignment.\"\"\"\n",
    "        img = np.array(Image.open(image_path).resize((299, 299)))\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "        for t in range(len(caption)):\n",
    "            ax = fig.add_subplot(3, int(np.ceil(len(caption) / 3)), t + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_shape = int(np.sqrt(alpha.size))\n",
    "            alpha = alpha.reshape((attention_shape, attention_shape))\n",
    "\n",
    "            # Normalize and boost contrast\n",
    "            alpha -= alpha.min()\n",
    "            if alpha.max() > 0:\n",
    "                alpha /= alpha.max()\n",
    "\n",
    "            # Resize attention map to 299×299\n",
    "            alpha_resized = Image.fromarray(np.uint8(255 * alpha)).resize((299, 299), resample=Image.BICUBIC)\n",
    "            alpha_resized = np.array(alpha_resized) / 255.0  # back to [0,1] float\n",
    "\n",
    "            ax.imshow(alpha_resized, cmap='jet', alpha=0.5, extent=(0, 299, 299, 0))\n",
    "            ax.set_title(f\"{t+1}: '{caption[t]}'\", fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot loss curve **and** both train/val BLEU-4 curves.\"\"\"\n",
    "        plt.figure(figsize=(14, 5))\n",
    "\n",
    "        # --- left: training loss ---\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_log, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cross-Entropy Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # --- right: BLEU-4 ---\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if self.train_bleu_log:\n",
    "            plt.plot(self.train_bleu_log, label='Train BLEU-4')\n",
    "        plt.plot(self.val_bleu_log,   label='Val BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4')\n",
    "        plt.title('BLEU-4 Scores')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Generate speech audio from caption text.\"\"\"\n",
    "        if not caption:\n",
    "            print(\"Empty caption, nothing to speak\")\n",
    "            return\n",
    "            \n",
    "        tts = gTTS(text=caption, lang='en')\n",
    "        tts.save(filename)\n",
    "        display(Audio(filename))\n",
    "        print(f\"Audio saved to {filename}\")\n",
    "    \n",
    "    def demo(\n",
    "            self,\n",
    "            image_path: str,\n",
    "            filename: str = \"caption_audio.mp3\",\n",
    "            beam_size: int = 5,\n",
    "            length_penalty: float = 0.7):\n",
    "        \"\"\"\n",
    "        End-to-end demo (beam-search inference):\n",
    "\n",
    "        1. Original image  – now titled with the **filename**\n",
    "        2. Ground-truth captions\n",
    "        3. Generated caption\n",
    "        4. Audio playback\n",
    "        5. Attention heat-maps\n",
    "        \"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return\n",
    "\n",
    "        # ---------------- 1. original image ----------------\n",
    "        img_name = os.path.basename(image_path)              # <- filename for the title\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(img_name, fontsize=14, pad=10)             # <- show filename here\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # ---------------- 2. ground-truth captions ----------\n",
    "        gt_caps = self.processor.captions_dict.get(img_name, [])\n",
    "        if gt_caps:\n",
    "            print(\"Ground-truth captions:\")\n",
    "            for cap in gt_caps:\n",
    "                print(\" -\", cap)\n",
    "        else:\n",
    "            print(\"No ground-truth captions found.\")\n",
    "\n",
    "        # ---------------- 3. caption generation -------------\n",
    "        words, attention = self.beam_search_decode(\n",
    "            image_path,\n",
    "            beam_size=beam_size,\n",
    "            length_penalty=length_penalty,\n",
    "            return_attention=True\n",
    "        )\n",
    "        caption = \" \".join(words)\n",
    "        print(\"\\nGenerated caption:\")\n",
    "        print(caption)\n",
    "\n",
    "        # ---------------- 4. audio --------------------------\n",
    "        self.speak_caption(caption, filename=filename)\n",
    "\n",
    "        # ---------------- 5. attention plot ----------------\n",
    "        self.plot_attention(image_path, words, attention)\n",
    "\n",
    "\n",
    "    def prime_dataset(self, ds, steps: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Pre-fill a tf.data shuffle buffer so the first training epoch\n",
    "        starts without the usual “Filling up shuffle buffer …” pause.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        ds    : the *un-iterated* tf.data.Dataset you’ll pass to train()\n",
    "        steps : number of iterator steps to advance; default uses\n",
    "                buffer_size // batch_size + 1 from config.\n",
    "        \"\"\"\n",
    "        if steps is None:\n",
    "            steps = self.config['buffer_size'] // self.config['batch_size'] + 1\n",
    "\n",
    "        it = iter(ds)\n",
    "        for _ in range(steps):\n",
    "            try:\n",
    "                next(it)\n",
    "            except StopIteration:  # dataset shorter than requested priming\n",
    "                break\n",
    "\n",
    "    def fine_tune_cnn(self,\n",
    "                      train_ds,\n",
    "                      val_data,\n",
    "                      layers_to_unfreeze: int = 2,\n",
    "                      lr: float = 1e-5,\n",
    "                      epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Phase-2 fine-tuning of the top Inception blocks.\n",
    "        Call after initial caption training for an extra accuracy bump.\n",
    "        \"\"\"\n",
    "        print(f\"\\nUnfreezing top {layers_to_unfreeze} Inception blocks …\")\n",
    "        self.encoder.unfreeze_top_layers(layers_to_unfreeze)\n",
    "\n",
    "        # New, low learning-rate optimiser\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        print(f\"Fine-tuning CNN for {epochs} epoch(s) at lr={lr} …\")\n",
    "        self.train(train_ds, val_data, epochs=epochs)\n",
    "\n",
    "        print(\"CNN fine-tune finished.\")\n",
    "\n",
    "\n",
    "    def plot_grad_norms(self):\n",
    "        \"\"\"Simple line plot of gradient norms per batch.\"\"\"\n",
    "        if not self.grad_norm_log:\n",
    "            print(\"No grad-norm data logged yet.\")\n",
    "            return\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(self.grad_norm_log)\n",
    "        plt.xlabel('Update step')\n",
    "        plt.ylabel('Global grad norm')\n",
    "        plt.title('Gradient-norm trajectory')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = processor.load_captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.display_samples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.prepare_captions(subset_ratio=CONFIG['subset_ratio'])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds, val_ds, _ = processor.prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel(CONFIG, processor)\n",
    "model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prime_dataset(train_ds, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prime_dataset(ds=train_ds, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_ds, processor.val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced_val = random.sample(processor.val_data, min(len(processor.val_data), 10))\n",
    "# model.train(train_ds=train_ds, val_data=reduced_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_grad_norms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fine_tune_cnn(train_ds, processor.val_data, layers_to_unfreeze=8, lr=1e-5, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_bleu(processor.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = processor.train_data[:5]  # known to be from training set\n",
    "for img_name, _ in samples:\n",
    "    img_path = os.path.join(CONFIG['image_dir'], img_name)\n",
    "    pred = model.greedy_decode(img_path)\n",
    "    print(\"Predicted:\", ' '.join(pred))\n",
    "    print(\"Ground Truth:\", processor.captions_dict[img_name][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pair = random.choice(processor.train_data)\n",
    "sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "model.demo(sample_img, filename='caption_audio01.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    img_tensor, tok_ids, _, filenames = batch\n",
    "    fname = filenames[0].numpy().decode()\n",
    "\n",
    "    decoded = ' '.join(\n",
    "        processor.tokenizer.index_word.get(int(i), '')\n",
    "        for i in tok_ids[0].numpy() if i)\n",
    "\n",
    "    print(\"Filename :\", fname)\n",
    "    print(\"Caption  :\", decoded)\n",
    "    print(\"GT       :\", processor.captions_dict[fname][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
