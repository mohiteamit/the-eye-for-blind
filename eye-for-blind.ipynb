{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye for Blind â€“ Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "Eye for Blind: An Assistive Image Captioning System with Visual Attention\n",
    "\n",
    "This project implements a deep learning model that generates natural language descriptions of images, particularly aimed at visually impaired users. The model leverages an attention mechanism to selectively focus on image regions when generating each word, mimicking human vision.\n",
    "\n",
    "Inspired by \"Show, Attend and Tell\" (Xu et al., 2015), this implementation:\n",
    "1. Uses a CNN encoder (InceptionV3) to extract image features.\n",
    "2. Applies additive (Bahdanau) attention during decoding.\n",
    "3. Employs a decoder LSTM to generate captions.\n",
    "4. Converts generated captions to speech using gTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import tensorflow as tf #type: ignore\n",
    "from tensorflow.keras import layers, Model #type: ignore\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay #type: ignore\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy #type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #type: ignore\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction #type: ignore\n",
    "from gtts import gTTS #type: ignore\n",
    "from IPython.display import Audio, display\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'image_dir': 'data/flickr30k_images',\n",
    "    'caption_file': 'data/flickr30k_images/results.csv',\n",
    "    'batch_size': 128,  # Increased for better GPU utilization\n",
    "    'buffer_size': 8192,  # Larger shuffle buffer\n",
    "    'max_length': 30,\n",
    "    'embedding_dim': 512,\n",
    "    'units': 512,\n",
    "    'seed': 42,\n",
    "    'epochs': 15,\n",
    "    'patience': 4,\n",
    "    'learning_rate': 1e-3,\n",
    "    'grad_clip_value': 5.0,\n",
    "    'vocab_min_count': 5,\n",
    "    'checkpoint_dir': './checkpoints',\n",
    "    'mixed_precision': True,  # Enable mixed precision for faster training\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision enabled\")\n",
    "\n",
    "# Configure GPUs for optimal performance\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    print(f\"Found {len(physical_devices)} GPUs\")\n",
    "    for gpu in physical_devices:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"Error setting memory growth: {e}\")\n",
    "    \n",
    "    # Enable multi-GPU training if available\n",
    "    if len(physical_devices) > 1:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print(f\"Using strategy: {strategy}\")\n",
    "        CONFIG['batch_size'] *= len(physical_devices)  # Scale batch size with number of GPUs\n",
    "        print(f\"Adjusted batch size to {CONFIG['batch_size']}\")\n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Constants\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.captions_dict = dict()\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        self.test_data = []\n",
    "    \n",
    "    def load_captions(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load and convert pipe-delimited Flickr-style caption file to a dict.\"\"\"\n",
    "        print(f\"Loading captions from {self.config['caption_file']}\")\n",
    "        df = pd.read_csv(self.config['caption_file'], sep='|', header=None, \n",
    "                         names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "        \n",
    "        caption_map = {}\n",
    "        for img, group in df.groupby('image_name'):\n",
    "            caption_map[img] = group['comment'].tolist()\n",
    "        \n",
    "        self.captions_dict = caption_map\n",
    "        print(f\"Loaded {len(caption_map)} images with captions\")\n",
    "        return caption_map\n",
    "    \n",
    "    def display_samples(self, num_samples: int = 3):\n",
    "        \"\"\"Display sample images with their captions.\"\"\"\n",
    "        if self.captions_dict is None:\n",
    "            self.load_captions()\n",
    "        \n",
    "        sample_keys = random.sample(list(self.captions_dict.keys()), num_samples)\n",
    "        for key in sample_keys:\n",
    "            img_path = os.path.join(self.config['image_dir'], key)\n",
    "            img = Image.open(img_path)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(key)\n",
    "            plt.show()\n",
    "            for cap in self.captions_dict[key][:5]: #type: ignore\n",
    "                print(f\"- {cap}\")\n",
    "            print()\n",
    "    \n",
    "    def preprocess_caption(self, caption: str) -> Optional[str]:\n",
    "        \"\"\"Clean and format caption text.\"\"\"\n",
    "        if caption is None or not isinstance(caption, str):\n",
    "            return None\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z0-9.,? ]\", \"\", caption)\n",
    "        return f\"<start> {caption.strip()} <end>\"\n",
    "    \n",
    "    def prepare_captions(self):\n",
    "        \"\"\"Process all captions and create vocabulary.\"\"\"\n",
    "        if self.captions_dict is None:\n",
    "            self.load_captions()\n",
    "        \n",
    "        print(\"Preprocessing captions...\")\n",
    "        # Prepare captions and vocabulary\n",
    "        all_captions = []\n",
    "        for caps in self.captions_dict.values():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p:\n",
    "                    all_captions.append(p)\n",
    "        \n",
    "        print(f\"Total captions: {len(all_captions)}\")\n",
    "        word_counts = Counter(word for cap in all_captions for word in cap.split())\n",
    "        valid_words = {word for word, count in word_counts.items() \n",
    "                      if count >= self.config['vocab_min_count']}\n",
    "        \n",
    "        # Filter captions by valid words\n",
    "        def keep_caption(caption: str) -> bool:\n",
    "            words = caption.split()\n",
    "            return all(w in valid_words or w in ('<start>', '<end>') for w in words)\n",
    "        \n",
    "        filtered_captions = [c for c in all_captions if keep_caption(c)]\n",
    "        print(f\"Filtered captions: {len(filtered_captions)}\")\n",
    "        \n",
    "        # Create tokenizer\n",
    "        print(\"Building tokenizer...\")\n",
    "        tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "        tokenizer.fit_on_texts(filtered_captions)\n",
    "        \n",
    "        # Ensure special tokens are in the vocabulary\n",
    "        special_tokens = ['<start>', '<end>']\n",
    "        for token in special_tokens:\n",
    "            if token not in tokenizer.word_index:\n",
    "                new_index = len(tokenizer.word_index) + 1\n",
    "                tokenizer.word_index[token] = new_index\n",
    "                tokenizer.index_word[new_index] = token\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(f\"Vocabulary size: {self.vocab_size}\")\n",
    "        \n",
    "        # Create image-caption pairs\n",
    "        print(\"Creating image-caption pairs...\")\n",
    "        image_caption_pairs = []\n",
    "        for img, caps in self.captions_dict.items():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p and keep_caption(p):\n",
    "                    image_caption_pairs.append((img, p))\n",
    "        \n",
    "        # Create train/val/test splits\n",
    "        random.shuffle(image_caption_pairs)\n",
    "        num_total = len(image_caption_pairs)\n",
    "        train_split = int(0.8 * num_total)\n",
    "        val_split = int(0.9 * num_total)\n",
    "        self.train_data = image_caption_pairs[:train_split]\n",
    "        self.val_data = image_caption_pairs[train_split:val_split]\n",
    "        self.test_data = image_caption_pairs[val_split:]\n",
    "        \n",
    "        print(f\"Dataset split: Train={len(self.train_data)}, Val={len(self.val_data)}, Test={len(self.test_data)}\")\n",
    "        return filtered_captions\n",
    "    \n",
    "    def encode_caption(self, caption: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Convert caption text to sequence of token ids.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call prepare_captions first.\")\n",
    "        \n",
    "        seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=self.config['max_length'], padding='post')[0]\n",
    "        return padded_seq, len(seq)\n",
    "    \n",
    "    @tf.function\n",
    "    def load_image(self, path: str) -> tf.Tensor:\n",
    "        \"\"\"Load and preprocess an image with caching for efficiency.\"\"\"\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "        img = tf.image.resize_with_pad(img, 299, 299)\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img\n",
    "    \n",
    "    def data_generator(self, data):\n",
    "        \"\"\"Generator function for dataset creation.\"\"\"\n",
    "        for img, cap in data:\n",
    "            img_path = os.path.join(self.config['image_dir'], img)\n",
    "            img_tensor = self.load_image(img_path)\n",
    "            token_ids, cap_len = self.encode_caption(cap)\n",
    "            yield img_tensor, token_ids, cap_len\n",
    "    \n",
    "    def build_dataset(self, data, shuffle=True, cache=True):\n",
    "        \"\"\"Build efficient TensorFlow dataset with caching and prefetching.\"\"\"\n",
    "        output_signature = (\n",
    "            tf.TensorSpec(shape=(299, 299, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(self.config['max_length'],), dtype=tf.int32),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "        \n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: self.data_generator(data), \n",
    "            output_signature=output_signature\n",
    "        )\n",
    "        \n",
    "        if cache:\n",
    "            ds = ds.cache()\n",
    "        \n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(self.config['buffer_size'])\n",
    "        \n",
    "        ds = ds.padded_batch(self.config['batch_size'])\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "        \n",
    "        return ds\n",
    "    \n",
    "    def prepare_datasets(self, small_subset=False):\n",
    "        \"\"\"Prepare all datasets for training/validation/testing.\"\"\"\n",
    "        if self.train_data is None:\n",
    "            self.prepare_captions()\n",
    "        \n",
    "        if small_subset:\n",
    "            # Use 10% of each dataset\n",
    "            train_subset = self.train_data[:int(len(self.train_data) * 0.1)]\n",
    "            val_subset = self.val_data[:int(len(self.val_data) * 0.1)]\n",
    "            test_subset = self.test_data[:int(len(self.test_data) * 0.1)]\n",
    "            print(f\"Using small subset: Train={len(train_subset)}, Val={len(val_subset)}, Test={len(test_subset)}\")\n",
    "        else:\n",
    "            train_subset = self.train_data\n",
    "            val_subset = self.val_data\n",
    "            test_subset = self.test_data\n",
    "        \n",
    "        print(\"Building datasets...\")\n",
    "        train_ds = self.build_dataset(train_subset)\n",
    "        val_ds = self.build_dataset(val_subset)\n",
    "        test_ds = self.build_dataset(test_subset, shuffle=False)\n",
    "        \n",
    "        return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"encoder\")\n",
    "        # Use efficient model loading with feature extraction only\n",
    "        base = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, \n",
    "            weights='imagenet',\n",
    "            input_shape=(299, 299, 3)\n",
    "        )\n",
    "        base.trainable = False\n",
    "        # Use specific layer for feature extraction\n",
    "        output_layer = base.get_layer('mixed10').output\n",
    "        self.cnn = Model(inputs=base.input, outputs=output_layer)\n",
    "        self.reshape = layers.Reshape((-1, 2048))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return self.reshape(x)\n",
    "\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "    \n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super().__init__(name=\"decoder\")\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.dropout = layers.Dropout(0.3)  # Add dropout for regularization\n",
    "    \n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context, attn = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context, 1), x], axis=-1)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=[hidden, cell])\n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state_h, state_c, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    \"\"\"\n",
    "    ImageCaptioningModel: A class for generating captions from images using an encoder-decoder architecture.\n",
    "    This model uses a CNN encoder to extract image features and an RNN decoder with attention\n",
    "    to generate textual descriptions. It includes functionality for training, evaluation,\n",
    "    inference with visualization of attention weights, and text-to-speech conversion.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, processor):\n",
    "        \"\"\"\n",
    "        Initialize the image captioning model.\n",
    "        Args:\n",
    "            config (dict): Configuration parameters for the model including learning rates,\n",
    "                          dimensions, checkpoint directories, etc.\n",
    "            processor (object): Data processor that handles image loading, tokenization,\n",
    "                               and vocabulary management.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.optimizer = None\n",
    "        self.loss_fn = None\n",
    "        self.ckpt_manager = None\n",
    "        self.best_bleu = 0\n",
    "        self.train_loss_log = []\n",
    "        self.val_bleu_log = []\n",
    "        self.smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Initialize model components including encoder, decoder, optimizer, and checkpoint system.\n",
    "        Sets up the CNN encoder, RNN decoder with attention, Adam optimizer with cosine decay,\n",
    "        and checkpoint management for model persistence.\n",
    "        \"\"\"\n",
    "        print(\"Building model...\")\n",
    "        with strategy.scope():\n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder(\n",
    "                embedding_dim=self.config['embedding_dim'], \n",
    "                units=self.config['units'], \n",
    "                vocab_size=self.processor.vocab_size\n",
    "            )\n",
    "            \n",
    "            lr_schedule = CosineDecay(\n",
    "                initial_learning_rate=self.config['learning_rate'],\n",
    "                decay_steps=10000\n",
    "            )\n",
    "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "            self.loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        \n",
    "        # Set up checkpointing\n",
    "        ckpt_dir = self.config['checkpoint_dir']\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            encoder=self.encoder, \n",
    "            decoder=self.decoder, \n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=3)\n",
    "        \n",
    "        # Try to restore the latest checkpoint\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Restored from checkpoint: {self.ckpt_manager.latest_checkpoint}\")\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, img_tensor, target, cap_len):\n",
    "        \"\"\"\n",
    "        Execute a single training step with gradient calculation and optimization.\n",
    "        Args:\n",
    "            img_tensor (Tensor): Batch of processed images\n",
    "            target (Tensor): Target caption sequences\n",
    "            cap_len (Tensor): Length of each caption in the batch\n",
    "        Returns:\n",
    "            float: Average loss for this training step\n",
    "        \"\"\"\n",
    "        loss = 0.0\n",
    "        batch_size = tf.shape(img_tensor)[0]\n",
    "        hidden = tf.zeros((batch_size, self.config['units']))\n",
    "        cell = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims(\n",
    "            tf.repeat(self.processor.tokenizer.word_index['<start>'], batch_size), 1\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)\n",
    "            for t in range(1, self.config['max_length']):\n",
    "                logits, hidden, cell, _ = self.decoder(dec_input, features, hidden, cell)\n",
    "                loss_ = self.loss_fn(target[:, t], tf.squeeze(logits, 1))\n",
    "                mask = tf.cast(target[:, t] > 0, tf.float32)\n",
    "                loss += tf.reduce_sum(loss_ * mask)\n",
    "                dec_input = tf.expand_dims(target[:, t], 1)\n",
    "            \n",
    "            # Average loss per token\n",
    "            total_loss = loss / tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "        \n",
    "        # Get trainable variables and apply gradients\n",
    "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        gradients = tape.gradient(total_loss, variables)\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def greedy_decode(self, image_path: str, return_attention=False):\n",
    "        \"\"\"\n",
    "        Generate a caption for an image using greedy decoding strategy.\n",
    "        Args:\n",
    "            image_path (str): Path to the image file\n",
    "            return_attention (bool): Whether to return attention weights\n",
    "        Returns:\n",
    "            list or tuple: List of words if return_attention is False,\n",
    "                          tuple of (words, attention_weights) if True\n",
    "        \"\"\"\n",
    "        img_tensor = tf.expand_dims(self.processor.load_image(image_path), 0)\n",
    "        features = self.encoder(img_tensor)\n",
    "        hidden = tf.zeros((1, self.config['units']))\n",
    "        cell = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims([self.processor.tokenizer.word_index['<start>']], 0)\n",
    "        \n",
    "        result, alphas = [], []\n",
    "        \n",
    "        for _ in range(self.config['max_length']):\n",
    "            logits, hidden, cell, alpha = self.decoder(dec_input, features, hidden, cell)\n",
    "            predicted_id = tf.argmax(logits[0, 0]).numpy()\n",
    "            \n",
    "            word = self.processor.tokenizer.index_word.get(predicted_id, '')\n",
    "            if word == '<end>': \n",
    "                break\n",
    "                \n",
    "            if word not in ('<start>', '<unk>'):\n",
    "                result.append(word)\n",
    "                \n",
    "            alphas.append(alpha[0].numpy())\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        return (result, alphas) if return_attention else result\n",
    "    \n",
    "    def evaluate_bleu(self, test_data, max_samples=None):\n",
    "        \"\"\"\n",
    "        Calculate BLEU scores on test data to evaluate model performance.\n",
    "        Args:\n",
    "            test_data (list): List of (image_name, captions) pairs for evaluation\n",
    "            max_samples (int, optional): Maximum number of samples to evaluate\n",
    "        Returns:\n",
    "            dict: Dictionary containing BLEU scores for different n-gram sizes\n",
    "        \"\"\"\n",
    "        refs, hyps = [], []\n",
    "        data_to_eval = test_data[:max_samples] if max_samples else test_data\n",
    "        \n",
    "        for img_name, caps in tqdm.tqdm(data_to_eval):\n",
    "            image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            hyp = self.greedy_decode(image_path)\n",
    "            \n",
    "            # Process ground truth captions\n",
    "            gt = [self.processor.preprocess_caption(c).split() for c in self.processor.captions_dict[img_name][:5]]\n",
    "            gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "            \n",
    "            refs.append(gt)\n",
    "            hyps.append(hyp)\n",
    "        \n",
    "        # Calculate BLEU scores for different n-grams\n",
    "        bleu_scores = {}\n",
    "        for i in range(1, 5):\n",
    "            weights = tuple([1.0/i]*i + [0.0]*(4-i))\n",
    "            score = corpus_bleu(refs, hyps, weights=weights, smoothing_function=self.smoothie)\n",
    "            bleu_scores[f'bleu-{i}'] = score\n",
    "            print(f\"BLEU-{i}: {score:.4f}\")\n",
    "        \n",
    "        return bleu_scores\n",
    "    \n",
    "    def train(self, train_ds, val_data, epochs=None):\n",
    "        \"\"\"\n",
    "        Train the model with early stopping based on validation BLEU score.\n",
    "        Args:\n",
    "            train_ds (tf.data.Dataset): Training dataset\n",
    "            val_data (list): Validation data for BLEU score calculation\n",
    "            epochs (int, optional): Number of epochs to train for, defaults to config value\n",
    "        Returns:\n",
    "            tuple: Training loss history and validation BLEU score history\n",
    "        \"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config['epochs']\n",
    "        \n",
    "        patience = self.config['patience']\n",
    "        wait = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            total_loss = 0.0\n",
    "            step = 0\n",
    "            \n",
    "            # Training loop\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                target=None,  # Will be updated after first batch\n",
    "                stateful_metrics=['loss']\n",
    "            )\n",
    "            \n",
    "            for batch, (img_tensor, target, cap_len) in enumerate(train_ds):\n",
    "                if batch == 0 and progbar.target is None:\n",
    "                    # Estimate total steps\n",
    "                    progbar.target = len(self.processor.train_data) // self.config['batch_size'] + 1\n",
    "                \n",
    "                batch_loss = self.train_step(img_tensor, target, cap_len)\n",
    "                total_loss += batch_loss\n",
    "                progbar.update(batch + 1, values=[('loss', batch_loss)])\n",
    "                step += 1\n",
    "            \n",
    "            # Average loss for the epoch\n",
    "            avg_loss = total_loss / step\n",
    "            self.train_loss_log.append(float(avg_loss))\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.ckpt_manager.save()\n",
    "            \n",
    "            # Validation on a subset for speed\n",
    "            print(\"Evaluating on validation subset...\")\n",
    "            validation_subset = val_data[:100]  # Evaluate on subset for speed\n",
    "            bleu_scores = self.evaluate_bleu(validation_subset)\n",
    "            bleu4 = bleu_scores['bleu-4']\n",
    "            self.val_bleu_log.append(bleu4)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if bleu4 > self.best_bleu:\n",
    "                self.best_bleu = bleu4\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, BLEU-4 = {bleu4:.4f}, Time = {time.time()-start:.2f}s\")\n",
    "        \n",
    "        return self.train_loss_log, self.val_bleu_log\n",
    "    \n",
    "    def plot_attention(self, image_path: str, caption: list, alphas: list):\n",
    "        \"\"\"\n",
    "        Visualize attention weights overlaid on the source image.\n",
    "        Args:\n",
    "            image_path (str): Path to the image file\n",
    "            caption (list): List of words in the generated caption\n",
    "            alphas (list): List of attention weight matrices\n",
    "        \"\"\"\n",
    "        img = np.array(Image.open(image_path).resize((224, 224)))\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        for t in range(len(caption)):\n",
    "            ax = fig.add_subplot(3, int(np.ceil(len(caption)/3)), t+1)\n",
    "            ax.set_title(caption[t])\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_shape = int(np.sqrt(alpha.size))\n",
    "            alpha = alpha.reshape(attention_shape, attention_shape)\n",
    "            ax.imshow(alpha, cmap='viridis', alpha=0.6, extent=(0, 224, 224, 0))\n",
    "            ax.axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"\n",
    "        Plot training history including loss and BLEU scores over epochs.\n",
    "        Generates a figure with two subplots showing the training loss curve\n",
    "        and validation BLEU-4 score progression.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_log, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_bleu_log, label='Val BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4')\n",
    "        plt.title('Validation BLEU-4')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def speak_caption(self, caption: str, filename=\"tts_output.mp3\"):\n",
    "        \"\"\"\n",
    "        Generate speech audio from the caption text using gTTS.\n",
    "        Args:\n",
    "            caption (str): Text to convert to speech\n",
    "            filename (str): Output audio file path\n",
    "        \"\"\"\n",
    "        if not caption:\n",
    "            print(\"Empty caption, nothing to speak\")\n",
    "            return\n",
    "            \n",
    "        tts = gTTS(text=caption, lang='en')\n",
    "        tts.save(filename)\n",
    "        display(Audio(filename))\n",
    "        print(f\"Audio saved to {filename}\")\n",
    "    \n",
    "    def demo(self, image_path):\n",
    "        \"\"\"\n",
    "        Run a full demonstration of the model's capabilities.\n",
    "        Displays the image, generates a caption with attention visualization,\n",
    "        and converts the caption to speech.\n",
    "        Args:\n",
    "            image_path (str): Path to the image file\n",
    "        Returns:\n",
    "            str: The generated caption\n",
    "        \"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Generating caption for: {image_path}\")\n",
    "        \n",
    "        # Display the image\n",
    "        img = Image.open(image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate caption with attention\n",
    "        words, attention = self.greedy_decode(image_path, return_attention=True)\n",
    "        caption = \" \".join(words)\n",
    "        print(f\"Generated caption: {caption}\")\n",
    "        \n",
    "        # Plot attention\n",
    "        self.plot_attention(image_path, words, attention)\n",
    "        \n",
    "        # Generate speech\n",
    "        self.speak_caption(caption)\n",
    "        \n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    # Initialize processor and load data\n",
    "    processor = DataProcessor(CONFIG)\n",
    "    processor.load_captions()\n",
    "    processor.display_samples(2)\n",
    "    processor.prepare_captions()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_ds, val_ds, _ = processor.prepare_datasets()\n",
    "    \n",
    "    # Build and train model\n",
    "    model = ImageCaptioningModel(CONFIG, processor)\n",
    "    model.build_model()\n",
    "    \n",
    "    # Uncomment to train the model\n",
    "    # model.train(train_ds, processor.val_data)\n",
    "    # model.plot_history()\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"Evaluating on test set:\")\n",
    "    model.evaluate_bleu(processor.test_data[:20])\n",
    "    \n",
    "    # Demo with a sample image\n",
    "    sample_img = os.path.join(CONFIG['image_dir'], processor.test_data[0][0])\n",
    "    model.demo(sample_img)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
