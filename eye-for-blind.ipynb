{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye for Blind – Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "Eye for Blind: An Assistive Image Captioning System with Visual Attention\n",
    "\n",
    "This project implements a deep learning model that generates natural language descriptions of images, particularly aimed at visually impaired users. The model leverages an attention mechanism to selectively focus on image regions when generating each word, mimicking human vision.\n",
    "\n",
    "Inspired by \"Show, Attend and Tell\" (Xu et al., 2015), this implementation:\n",
    "1. Uses a CNN encoder (InceptionV3) to extract image features.\n",
    "2. Applies additive (Bahdanau) attention during decoding.\n",
    "3. Employs a decoder LSTM to generate captions.\n",
    "4. Converts generated captions to speech using gTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf #type: ignore\n",
    "from tensorflow.keras import layers, Model #type: ignore\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay #type: ignore\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy #type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #type: ignore\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction #type: ignore\n",
    "from gtts import gTTS #type: ignore\n",
    "from IPython.display import Audio, display\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'subset_ratio' : 1.0,\n",
    "    'image_dir': '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file': '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    \n",
    "    # GPU Utilization\n",
    "    'batch_size': 128,           # Fully utilize 48GB VRAM; reduce if OOM\n",
    "    'buffer_size': 10000,        # Larger shuffle buffer helps training stability\n",
    "    \n",
    "    # Model Capacity\n",
    "    'max_length': 30,            # Reasonable for captions\n",
    "    'embedding_dim': 512,        # Good for attention + LSTM\n",
    "    'units': 512,                # LSTM/Attention size\n",
    "    \n",
    "    # Training Behavior\n",
    "    'seed': 42,\n",
    "    'epochs': 20,                # Slightly more for small dataset\n",
    "    'patience': 4,               # Early stopping tolerance\n",
    "    'learning_rate': 3e-4,       # Lower for small datasets to reduce overfitting\n",
    "    'grad_clip_value': 5.0,      # Prevent exploding gradients\n",
    "    \n",
    "    # Vocabulary\n",
    "    'vocab_min_count': 3,        # Include more words for small run\n",
    "    \n",
    "    # Output & Precision\n",
    "    'checkpoint_dir': './checkpoints/10pct',\n",
    "    'mixed_precision': False,     # RTX 6000 Ada has 4th-gen Tensor Cores—use them\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "\n",
    "# Mixed precision policy - RTX 6000 Ada has excellent mixed precision support\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision enabled for RTX 6000 Ada\")\n",
    "\n",
    "# Single GPU setup\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Enable memory growth for RTX 6000 Ada\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    # Use default strategy for single GPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Using single GPU: {physical_devices[0].name}, batch size={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Constants\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.captions_dict = dict()\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        self.test_data = []\n",
    "    \n",
    "    def load_captions(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load and convert pipe-delimited Flickr-style caption file to a dict.\"\"\"\n",
    "        print(f\"Loading captions from {self.config['caption_file']}\")\n",
    "        df = pd.read_csv(self.config['caption_file'], sep='|', header=None, \n",
    "                         names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "        \n",
    "        caption_map = {}\n",
    "        for img, group in df.groupby('image_name'):\n",
    "            caption_map[img] = group['comment'].tolist()\n",
    "        \n",
    "        self.captions_dict = caption_map\n",
    "        print(f\"Loaded {len(caption_map)} images with captions\")\n",
    "        return caption_map\n",
    "    \n",
    "    def display_samples(self, num_samples: int = 3):\n",
    "        \"\"\"Display random images with all their associated captions.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        sample_keys = random.sample(list(self.captions_dict.keys()), min(num_samples, len(self.captions_dict)))\n",
    "\n",
    "        for key in sample_keys:\n",
    "            img_path = os.path.join(self.config['image_dir'], key)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(key)\n",
    "                plt.show()\n",
    "\n",
    "                for cap in self.captions_dict[key]:\n",
    "                    print(f\"- {cap}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {key}: {e}\")\n",
    "\n",
    "    def preprocess_caption(self, caption: str) -> Optional[str]:\n",
    "        \"\"\"Clean and format caption text.\"\"\"\n",
    "        if caption is None or not isinstance(caption, str):\n",
    "            return None\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z0-9.,? ]\", \"\", caption)\n",
    "        return f\"<start> {caption.strip()} <end>\"\n",
    "    \n",
    "    def prepare_captions(self, subset_ratio=1.0):\n",
    "        \"\"\"Process captions, build tokenizer & train/val/test splits.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        # --- 1. clean & tag ----------------------------------------------------\n",
    "        all_captions = []\n",
    "        for caps in self.captions_dict.values():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p:\n",
    "                    all_captions.append(p)\n",
    "\n",
    "        word_counts = Counter(w for cap in all_captions for w in cap.split())\n",
    "        valid_words = {w for w, cnt in word_counts.items()\n",
    "                    if cnt >= self.config['vocab_min_count']}\n",
    "\n",
    "        def keep(c):\n",
    "            return all(w in valid_words or w in ('<start>', '<end>') for w in c.split())\n",
    "\n",
    "        filtered = [c for c in all_captions if keep(c)]\n",
    "\n",
    "        # --- 2. determine max length ------------------------------------------\n",
    "        lengths = [len(c.split()) for c in filtered]\n",
    "        self.config['max_length'] = int(np.percentile(lengths, 95))\n",
    "        print(f\"max_length set to {self.config['max_length']}\")\n",
    "\n",
    "        # --- 3. build tokenizer (NO filters so < and > stay) -------------------\n",
    "        tokenizer = Tokenizer(oov_token=\"<unk>\", filters='', lower=True)\n",
    "        tokenizer.fit_on_texts(filtered)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(f\"vocab size = {self.vocab_size}\")\n",
    "\n",
    "        # --- 4. build (image, caption) list ------------------------------------\n",
    "        pairs = []\n",
    "        for img, caps in self.captions_dict.items():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p and keep(p):\n",
    "                    pairs.append((img, p))\n",
    "\n",
    "        if subset_ratio < 1.0:\n",
    "            pairs = pairs[:int(len(pairs) * subset_ratio)]\n",
    "            print(f\"subset: {len(pairs)} pairs\")\n",
    "\n",
    "        random.shuffle(pairs)\n",
    "        n = len(pairs)\n",
    "        self.train_data, self.val_data, self.test_data = (\n",
    "            pairs[:int(0.8*n)],\n",
    "            pairs[int(0.8*n):int(0.9*n)],\n",
    "            pairs[int(0.9*n):],\n",
    "        )\n",
    "        print(f\"split  →  train {len(self.train_data)} | val {len(self.val_data)} | test {len(self.test_data)}\")\n",
    "\n",
    "        return filtered\n",
    "    \n",
    "    def encode_caption(self, caption: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Convert caption text to sequence of token ids.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call prepare_captions first.\")\n",
    "        \n",
    "        seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=self.config['max_length'], padding='post')[0]\n",
    "        return padded_seq, len(seq)\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image(self, path: str) -> tf.Tensor:\n",
    "        \"\"\"Load and preprocess an image efficiently in graph mode.\"\"\"\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "        img = tf.image.resize(img, [299, 299])\n",
    "        img = tf.ensure_shape(img, [299, 299, 3])\n",
    "        return tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "    def data_generator(self, data):\n",
    "        \"\"\"Generator function for dataset creation.\"\"\"\n",
    "        for img, cap in data:\n",
    "            img_path = os.path.join(self.config['image_dir'], img)\n",
    "            img_tensor = self.load_image(tf.convert_to_tensor(img_path))\n",
    "            token_ids, cap_len = self.encode_caption(cap)\n",
    "            yield img_tensor, token_ids, cap_len\n",
    "    \n",
    "    def build_dataset(self, data, shuffle=True, cache=True):\n",
    "        \"\"\"Create a tf.data.Dataset optimized for single GPU.\"\"\"\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((299, 299, 3), tf.float32),\n",
    "            tf.TensorSpec((self.config['max_length'],), tf.int32),\n",
    "            tf.TensorSpec((), tf.int32)\n",
    "        )\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: self.data_generator(data),\n",
    "            output_signature=output_signature\n",
    "        )\n",
    "\n",
    "        if cache:\n",
    "            ds = ds.cache()\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(self.config['buffer_size'])\n",
    "\n",
    "        ds = ds.batch(self.config['batch_size'])\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"Prepare all datasets for training/validation/testing.\"\"\"\n",
    "        if not self.train_data:\n",
    "            self.prepare_captions()\n",
    "\n",
    "        print(\"Building datasets...\")\n",
    "        train_ds = self.build_dataset(self.train_data)\n",
    "        val_ds = self.build_dataset(self.val_data)\n",
    "        test_ds = self.build_dataset(self.test_data, shuffle=False)\n",
    "        \n",
    "        return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"encoder\")\n",
    "        # Use efficient model loading with feature extraction only\n",
    "        base = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, \n",
    "            weights='imagenet',\n",
    "            input_shape=(299, 299, 3)\n",
    "        )\n",
    "        base.trainable = False\n",
    "        # Use specific layer for feature extraction\n",
    "        output_layer = base.get_layer('mixed10').output\n",
    "        self.cnn = Model(inputs=base.input, outputs=output_layer)\n",
    "        self.reshape = layers.Reshape((-1, 2048))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return self.reshape(x)\n",
    "\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "    \n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super().__init__(name=\"decoder\")\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.dropout = layers.Dropout(0.3)\n",
    "    \n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context, attn = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context, 1), x], axis=-1)\n",
    "        \n",
    "        hidden = tf.cast(hidden, x.dtype)\n",
    "        cell = tf.cast(cell, x.dtype)\n",
    "\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=[hidden, cell])\n",
    "        output = self.dropout(output)\n",
    "        logits = self.fc(output)\n",
    "        return logits, state_h, state_c, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    def __init__(self, config, processor):\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.optimizer = None\n",
    "        self.loss_fn = None\n",
    "        self.ckpt_manager = None\n",
    "        self.best_bleu = 0\n",
    "        self.train_loss_log = []\n",
    "        self.val_bleu_log = []\n",
    "        self.smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build model for single GPU - no distribution strategy needed.\"\"\"\n",
    "        print(\"Building model for single GPU...\")\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim=self.config['embedding_dim'], \n",
    "            units=self.config['units'], \n",
    "            vocab_size=self.processor.vocab_size\n",
    "        )\n",
    "        \n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=self.config['learning_rate'],\n",
    "            decay_steps=10000\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        \n",
    "        # Set up checkpointing\n",
    "        ckpt_dir = self.config['checkpoint_dir']\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            encoder=self.encoder, \n",
    "            decoder=self.decoder, \n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=3)\n",
    "        \n",
    "        # Try to restore the latest checkpoint\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Restored from checkpoint: {self.ckpt_manager.latest_checkpoint}\")\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print model summaries for Encoder, Attention, and Decoder.\"\"\"\n",
    "        print(\"Building model summaries...\")\n",
    "\n",
    "        # Dummy inputs\n",
    "        dummy_image = tf.random.uniform((1, 299, 299, 3))\n",
    "        dummy_features = tf.random.uniform((1, 64, 2048))\n",
    "        dummy_hidden = tf.zeros((1, self.config['units']))\n",
    "        dummy_cell = tf.zeros((1, self.config['units']))\n",
    "        dummy_token = tf.zeros((1, 1), dtype=tf.int32)\n",
    "\n",
    "        # --- Encoder Summary ---\n",
    "        print(\"\\nEncoder Summary:\")\n",
    "        self.encoder(dummy_image)\n",
    "        self.encoder.summary()\n",
    "\n",
    "        # --- Bahdanau Attention Summary ---\n",
    "        print(\"\\nBahdanau Attention Summary:\")\n",
    "        attention_layer = BahdanauAttention(self.config['units'])\n",
    "        features_input = tf.keras.Input(shape=(64, 2048), name=\"features\")\n",
    "        hidden_input = tf.keras.Input(shape=(self.config['units'],), name=\"hidden\")\n",
    "        context_vector, attn_weights = attention_layer(features_input, hidden_input)\n",
    "        attention_model = tf.keras.Model(inputs=[features_input, hidden_input], outputs=[context_vector, attn_weights])\n",
    "        attention_model.summary()\n",
    "\n",
    "        # --- Decoder Summary ---\n",
    "        print(\"\\nDecoder Summary:\")\n",
    "        self.decoder(dummy_token, dummy_features, dummy_hidden, dummy_cell)\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, img_tensor, target, cap_len):\n",
    "        \"\"\"Execute a single training step.\"\"\"\n",
    "        # Ensure models are built\n",
    "        if self.encoder is None or self.decoder is None:\n",
    "            raise ValueError(\"Models not built. Call build_model() first.\")\n",
    "        \n",
    "        loss = 0.0\n",
    "        batch_size = tf.shape(img_tensor)[0]\n",
    "        hidden = tf.zeros((batch_size, self.config['units']))\n",
    "        cell = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims(\n",
    "            tf.repeat(self.processor.tokenizer.word_index['<start>'], batch_size), 1\n",
    "        )\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)\n",
    "            for t in range(1, self.config['max_length']):\n",
    "                logits, hidden, cell, _ = self.decoder(dec_input, features, hidden, cell)\n",
    "                loss_ = self.loss_fn(target[:, t], tf.squeeze(logits, 1))\n",
    "                mask = tf.cast(target[:, t] > 0, tf.float32)\n",
    "                loss += tf.reduce_sum(tf.cast(loss_, tf.float32) * mask)\n",
    "                dec_input = tf.expand_dims(target[:, t], 1)\n",
    "            \n",
    "            # Average loss per token\n",
    "            total_loss = loss / tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "            \n",
    "            # Handle mixed precision - scale loss for gradient computation\n",
    "            if self.config['mixed_precision']:\n",
    "                total_loss = tf.cast(total_loss, tf.float32)\n",
    "        \n",
    "        # Get trainable variables and apply gradients\n",
    "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        gradients = tape.gradient(total_loss, variables)\n",
    "        \n",
    "        # Handle mixed precision - cast gradients to float32 if needed\n",
    "        if self.config['mixed_precision']:\n",
    "            gradients = [tf.cast(g, tf.float32) if g is not None else None for g in gradients]\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        clipped_gradients, _ = tf.clip_by_global_norm(gradients, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def greedy_decode(self, image_path: str, return_attention=False):\n",
    "        \"\"\"Generate a caption for an image using greedy decoding.\"\"\"\n",
    "        img_tensor = tf.expand_dims(self.processor.load_image(image_path), 0)\n",
    "        features = self.encoder(img_tensor)\n",
    "        hidden = tf.zeros((1, self.config['units']))\n",
    "        cell = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims([self.processor.tokenizer.word_index['<start>']], 0)\n",
    "        \n",
    "        result, alphas = [], []\n",
    "        \n",
    "        for _ in range(self.config['max_length']):\n",
    "            logits, hidden, cell, alpha = self.decoder(dec_input, features, hidden, cell)\n",
    "            predicted_id = tf.argmax(logits[0, 0]).numpy()\n",
    "            \n",
    "            word = self.processor.tokenizer.index_word.get(predicted_id, '')\n",
    "            if word == '<end>': \n",
    "                break\n",
    "                \n",
    "            if word not in ('<start>', '<unk>'):\n",
    "                result.append(word)\n",
    "                \n",
    "            alphas.append(alpha[0].numpy())\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        return (result, alphas) if return_attention else result\n",
    "    \n",
    "    def evaluate_bleu(self, test_data, max_samples=None):\n",
    "        \"\"\"Calculate BLEU scores on test data.\"\"\"\n",
    "        refs, hyps = [], []\n",
    "        data_to_eval = test_data[:max_samples] if max_samples else test_data\n",
    "        \n",
    "        for img_name, _ in tqdm.tqdm(data_to_eval):\n",
    "            image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            hyp = self.greedy_decode(image_path)\n",
    "            \n",
    "            # Process ground truth captions\n",
    "            gt = [self.processor.preprocess_caption(c).split() for c in self.processor.captions_dict[img_name][:5]]\n",
    "            gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "            \n",
    "            refs.append(gt)\n",
    "            hyps.append(hyp)\n",
    "        \n",
    "        # Calculate BLEU scores for different n-grams\n",
    "        bleu_scores = {}\n",
    "        for i in range(1, 5):\n",
    "            weights = tuple([1.0/i]*i + [0.0]*(4-i))\n",
    "            score = corpus_bleu(refs, hyps, weights=weights, smoothing_function=self.smoothie)\n",
    "            bleu_scores[f'bleu-{i}'] = score\n",
    "            print(f\"BLEU-{i}: {score:.4f}\")\n",
    "        \n",
    "        return bleu_scores\n",
    "    \n",
    "    def train(self, train_ds, val_data, epochs=None):\n",
    "        \"\"\"Train the model with early stopping.\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config['epochs']\n",
    "        \n",
    "        patience = self.config['patience']\n",
    "        wait = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            total_loss = 0.0\n",
    "            step = 0\n",
    "            \n",
    "            # Training loop\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                target=None,\n",
    "                stateful_metrics=['loss']\n",
    "            )\n",
    "            \n",
    "            for batch, (img_tensor, target, cap_len) in enumerate(train_ds):\n",
    "                if batch == 0 and progbar.target is None:\n",
    "                    progbar.target = len(self.processor.train_data) // self.config['batch_size'] + 1\n",
    "                \n",
    "                batch_loss = self.train_step(img_tensor, target, cap_len)\n",
    "                total_loss += batch_loss\n",
    "                progbar.update(batch + 1, values=[('loss', batch_loss)])\n",
    "                step += 1\n",
    "            \n",
    "            # Average loss for the epoch\n",
    "            avg_loss = total_loss / step\n",
    "            self.train_loss_log.append(float(avg_loss))\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.ckpt_manager.save()\n",
    "            \n",
    "            # Validation on a subset for speed\n",
    "            print(\"Evaluating on validation subset...\")\n",
    "            validation_subset = val_data[:100]\n",
    "            bleu_scores = self.evaluate_bleu(validation_subset)\n",
    "            bleu4 = bleu_scores['bleu-4']\n",
    "            self.val_bleu_log.append(bleu4)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if bleu4 > self.best_bleu:\n",
    "                self.best_bleu = bleu4\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, BLEU-4 = {bleu4:.4f}, Time = {time.time()-start:.2f}s\", flush=True)\n",
    "        \n",
    "        return self.train_loss_log, self.val_bleu_log\n",
    "    \n",
    "    def plot_attention(self, image_path: str, caption: list, alphas: list):\n",
    "        \"\"\"Visualize attention weights overlaid on the source image.\"\"\"\n",
    "        img = np.array(Image.open(image_path).resize((224, 224)))\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        for t in range(len(caption)):\n",
    "            ax = fig.add_subplot(3, int(np.ceil(len(caption)/3)), t+1)\n",
    "            ax.set_title(caption[t])\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_shape = int(np.sqrt(alpha.size))\n",
    "            alpha = alpha.reshape(attention_shape, attention_shape)\n",
    "            ax.imshow(alpha, cmap='viridis', alpha=0.6, extent=(0, 224, 224, 0))\n",
    "            ax.axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_log, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_bleu_log, label='Val BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4')\n",
    "        plt.title('Validation BLEU-4')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Generate speech audio from caption text.\"\"\"\n",
    "        if not caption:\n",
    "            print(\"Empty caption, nothing to speak\")\n",
    "            return\n",
    "            \n",
    "        tts = gTTS(text=caption, lang='en')\n",
    "        tts.save(filename)\n",
    "        display(Audio(filename))\n",
    "        print(f\"Audio saved to {filename}\")\n",
    "    \n",
    "    def demo(self, image_path, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Run a full demonstration of the model.\"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Generating caption for: {image_path}\")\n",
    "        \n",
    "        # Display the image\n",
    "        img = Image.open(image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate caption with attention\n",
    "        words, attention = self.greedy_decode(image_path, return_attention=True)\n",
    "        caption = \" \".join(words)\n",
    "        print(f\"Generated caption: {caption}\")\n",
    "        \n",
    "        # Plot attention\n",
    "        self.plot_attention(image_path, words, attention)\n",
    "        \n",
    "        # Generate speech\n",
    "        self.speak_caption(caption, filename=filename)\n",
    "        \n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor and load data\n",
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.load_captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.display_samples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.prepare_captions(subset_ratio=CONFIG['subset_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds, val_ds, _ = processor.prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train model\n",
    "model = ImageCaptioningModel(CONFIG, processor)\n",
    "model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train the model\n",
    "model.train(train_ds, processor.val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set:\")\n",
    "model.evaluate_bleu(processor.test_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with a sample image\n",
    "sample_img = os.path.join(CONFIG['image_dir'], processor.test_data[0][0])\n",
    "model.demo(sample_img, filename='caption_audio01.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pair = random.choice(processor.test_data)\n",
    "sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "model.demo(sample_img, filename='caption_audio02.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.display_samples(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Sanity-check: over-fit on a tiny slice to confirm training pipeline works ---\n",
    "\n",
    "# # 1. Use just 32 (image, caption) pairs from the existing training split\n",
    "# mini_pairs = processor.train_data[:32]\n",
    "\n",
    "# # 2. Build a dataset with shuffling and caching\n",
    "# mini_ds = processor.build_dataset(mini_pairs, shuffle=True)\n",
    "\n",
    "# # 3. Fresh model instance (reuse CONFIG but train longer)\n",
    "# OVERFIT_CONFIG = CONFIG.copy()\n",
    "# OVERFIT_CONFIG['epochs'] = 50        # long enough to over-fit\n",
    "# OVERFIT_CONFIG['batch_size'] = 32    # one image per caption for clarity\n",
    "\n",
    "# mini_model = ImageCaptioningModel(OVERFIT_CONFIG, processor)\n",
    "# mini_model.build_model()\n",
    "\n",
    "# # 4. Train and monitor a single sample caption every 5 epochs\n",
    "# sample_path = os.path.join(OVERFIT_CONFIG['image_dir'], mini_pairs[0][0])\n",
    "\n",
    "# for epoch in range(OVERFIT_CONFIG['epochs']):\n",
    "#     # one pass over mini_ds\n",
    "#     for img_tensor, target, cap_len in mini_ds:\n",
    "#         _ = mini_model.train_step(img_tensor, target, cap_len)\n",
    "\n",
    "#     # quick progress print\n",
    "#     if (epoch + 1) % 5 == 0:\n",
    "#         generated = mini_model.greedy_decode(sample_path)\n",
    "#         print(f\"Epoch {epoch+1}: {' '.join(generated)}\")\n",
    "\n",
    "# print(\"\\nFinal caption after over-fitting:\")\n",
    "# print(\" \".join(mini_model.greedy_decode(sample_path)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
