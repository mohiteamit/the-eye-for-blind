{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EYE FOR BLIND: An Image Captioning Project\n",
    "\n",
    "- This notebook is the capstone project for the EPGP program by Upgrad and IIITB. \n",
    "\n",
    "- The project, titled \"Eye for Blind,\" focuses on generating descriptive captions for images.\n",
    "\n",
    "- The implementation is roughly based on the paper: **\"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Tech Review\n",
    "\n",
    "#### 1. Architecture\n",
    "\n",
    "* **Encoder**: Inception V3 (top removed) -> extract *mixed7* grid features -> Dense layer projects to common embedding space.\n",
    "* **Decoder**: LSTM + **Bahdanau Attention** -> attends to image regions each step to predict next word.\n",
    "* **Attention math**: alignment scores -> weighted context vector -> fed to LSTM with word embed.\n",
    "\n",
    "#### 2. Training Tricks\n",
    "\n",
    "* **Feature caching**: save pre-extracted `.npy` files to skip repeat CNN passes.\n",
    "* **Mixed-precision (AMP)**: `mixed_float16` for speed & lower VRAM.\n",
    "* **Custom `train_step` (@tf.function)**: fast graph execution, easy add-ons.\n",
    "* **Gradient clipping**: `tf.clip_by_global_norm` to tame exploding grads.\n",
    "* **Attention regularizer**: penalty if weights <> 1.0.\n",
    "\n",
    "#### 3. Data-to-Text Bridging\n",
    "\n",
    "* **Scheduled sampling**: linearly swap teacher forcing -> self-feeding to cut exposure bias.\n",
    "* **Beam search (inference)**: keeps top-*k* partial captions + length penalty for balanced outputs.\n",
    "\n",
    "#### 4. Training Management\n",
    "\n",
    "* **Early stopping**: halt when val BLEU-4 stalls (patience configurable).\n",
    "\n",
    "#### 5. Evaluation & Explainability\n",
    "\n",
    "* **Metrics**: BLEU-1/2/3/4 + timing stats.\n",
    "* **Attention heat-maps**: visualize focus per generated word.\n",
    "* **EDA helpers**: caption length histos, word freq, sample previews.\n",
    "* **Optional TTS**: gTTS turns captions into audio.\n",
    "\n",
    "---\n",
    "\n",
    "Result: a lean, cache-efficient, attention-guided captioner thatâ€™s fast to train, interpretable, and yields high-quality descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, random, collections, tqdm, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, object] = {\n",
    "    # Directory containing image files\n",
    "    'image_dir': '/home/flickr8k/Images',\n",
    "    \n",
    "    # Path to the captions file\n",
    "    'caption_file': '/home/flickr8k/captions.txt',\n",
    "    \n",
    "    # Directory to cache extracted image features\n",
    "    'feature_cache_dir': '/home/flickr8k/cache',\n",
    "    \n",
    "    # Number of examples to use (None means use all)\n",
    "    'num_examples': None,\n",
    "    \n",
    "    # Maximum length of captions (in tokens)\n",
    "    'max_caption_length': 50,\n",
    "    \n",
    "    # Minimum word frequency for vocabulary pruning\n",
    "    'min_word_frequency': 5,\n",
    "\n",
    "    # Embedding dimension for the model\n",
    "    'embedding_dim': 256,\n",
    "    \n",
    "    # Number of units in LSTM\n",
    "    'units': 512,\n",
    "    \n",
    "    # Dropout rate for the decoder\n",
    "    'decoder_dropout': 0.5,\n",
    "\n",
    "    # Learning rate for the optimizer\n",
    "    'learning_rate': 5e-5,\n",
    "    \n",
    "    # Number of training epochs\n",
    "    'epochs': 30,\n",
    "    \n",
    "    # Batch size for training\n",
    "    'batch_size': 64,\n",
    "\n",
    "    # Buffer size for shuffling the dataset\n",
    "    'buffer_size': 10000,\n",
    "    \n",
    "    # Early stopping patience\n",
    "    'patience': 8,\n",
    "    \n",
    "    # Path to save model checkpoints\n",
    "    'checkpoint_path': './checkpoints/flickr8k_fresh_run',\n",
    "    \n",
    "    # Enable mixed precision training for faster computation\n",
    "    'mixed_precision': True,\n",
    "\n",
    "    # Regularization weight for attention alignment\n",
    "    'attention_reg_lambda': 0.5,\n",
    "    \n",
    "    # Gradient clipping value to prevent exploding gradients\n",
    "    'grad_clip_value': 5.0,\n",
    "    \n",
    "    # Maximum probability for scheduled sampling\n",
    "    'scheduled_sampling_max_prob': 0.2,\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ENV SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "\n",
    "# Enable mixed precision training if configured\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"[AMP] mixed_float16 policy active\")\n",
    "else:\n",
    "    print(\"[AMP] disabled - using float32 throughout\")\n",
    "\n",
    "# Configure GPU memory growth to prevent memory allocation issues\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"Using GPU: {physical_devices[0].name} | batch={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"GPU not found - fallback to CPU\")\n",
    "\n",
    "# Set AUTOTUNE for optimizing data pipeline performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Handles loading, preprocessing, and splitting of image-caption data for image captioning.\n",
    "    - Loads captions and images, checks for missing files.\n",
    "    - Preprocesses captions (lowercase, remove non-alpha, add tokens).\n",
    "    - Builds tokenizer and prunes vocab by min_word_frequency.\n",
    "    - Splits data into train/val/test and pairs images with padded caption sequences.\n",
    "    - Supports mapping to cached image features.\n",
    "    - Can display random samples with captions for sanity check.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        # Store config and initialize all data structures\n",
    "        self.config = config\n",
    "        self.tokenizer: Optional[Tokenizer] = None\n",
    "        self.img_to_cap_map: Dict[str, List[str]] = collections.defaultdict(list)\n",
    "        self.image_paths: List[str] = []\n",
    "        self.all_captions: List[str] = []\n",
    "        self.train_data: List[Tuple[str, List[int]]] = []\n",
    "        self.val_data: List[Tuple[str, List[int]]] = []\n",
    "        self.test_data: List[Tuple[str, List[int]]] = []\n",
    "        self.max_caption_length = 0\n",
    "        self.vocab_size = 0\n",
    "        self.num_steps_per_epoch = 0\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Loads captions from CSV, checks image existence, preprocesses captions,\n",
    "        builds tokenizer, prunes vocab, and computes max caption length.\n",
    "        \"\"\"\n",
    "        print(\"Loading and preprocessing captions...\")\n",
    "        df = pd.read_csv(self.config['caption_file'], engine='python')\n",
    "        df['image'] = df['image'].str.strip()\n",
    "        df['caption'] = df['caption'].str.strip()\n",
    "\n",
    "        temp_img_to_cap_map = collections.defaultdict(list)\n",
    "        all_unique_img_names_from_csv = df['image'].unique()\n",
    "\n",
    "        print(f\"Checking {len(all_unique_img_names_from_csv)} unique image files from CSV...\")\n",
    "        found_images_count = 0\n",
    "\n",
    "        existing_image_files = set(os.listdir(self.config['image_dir']))\n",
    "\n",
    "        for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"Validating images & processing captions\"):\n",
    "            img_name = row['image']\n",
    "            caption = row['caption']\n",
    "\n",
    "            if img_name in existing_image_files:\n",
    "                temp_img_to_cap_map[img_name].append(self.preprocess_text(caption))\n",
    "                if img_name not in self.img_to_cap_map:\n",
    "                    found_images_count += 1\n",
    "                self.img_to_cap_map[img_name] = temp_img_to_cap_map[img_name]\n",
    "\n",
    "        if found_images_count < len(all_unique_img_names_from_csv):\n",
    "            print(f\"Warning: {len(all_unique_img_names_from_csv) - found_images_count} images mentioned in CSV were not found in {self.config['image_dir']}. They have been discarded.\")\n",
    "\n",
    "        self.image_paths = sorted(list(self.img_to_cap_map.keys()))\n",
    "\n",
    "        if self.config['num_examples']:\n",
    "            if len(self.image_paths) > self.config['num_examples']:\n",
    "                self.image_paths = random.sample(self.image_paths, self.config['num_examples'])\n",
    "                self.img_to_cap_map = {img: self.img_to_cap_map[img] for img in self.image_paths}\n",
    "                print(f\"Using a subset of {len(self.image_paths)} images due to 'num_examples' config.\")\n",
    "\n",
    "        self.all_captions = []\n",
    "        for img_name in self.image_paths:\n",
    "            self.all_captions.extend(self.img_to_cap_map[img_name])\n",
    "\n",
    "        print(f\"Total valid images (with captions): {len(self.image_paths)}\")\n",
    "        print(f\"Total valid captions: {len(self.all_captions)}\")\n",
    "\n",
    "        # Build tokenizer and prune vocab\n",
    "        self.tokenizer = Tokenizer(num_words=None, oov_token=\"<unk>\",\n",
    "                                   filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ',\n",
    "                                   lower=True)\n",
    "        self.tokenizer.fit_on_texts(self.all_captions)\n",
    "\n",
    "        word_counts = collections.Counter(word for caption in self.all_captions for word in caption.split())\n",
    "        filtered_word_index = {\n",
    "            word: index for word, index in self.tokenizer.word_index.items()\n",
    "            if word_counts[word] >= self.config['min_word_frequency'] or word in ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        }\n",
    "        self.tokenizer.word_index = filtered_word_index\n",
    "        self.tokenizer.index_word = {v: k for k, v in filtered_word_index.items()}\n",
    "\n",
    "        # Ensure special tokens are correctly in word_index and index_word with proper mapping\n",
    "        special_tokens = ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        next_index = len(self.tokenizer.word_index) + 1\n",
    "        for token in special_tokens:\n",
    "            if token not in self.tokenizer.word_index:\n",
    "                self.tokenizer.word_index[token] = next_index\n",
    "                self.tokenizer.index_word[next_index] = token\n",
    "                next_index += 1\n",
    "\n",
    "        # This is where vocab_size is correctly determined based on the processed data.\n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1 # +1 for 0-indexing\n",
    "        print(f\"Vocabulary size after pruning (min_word_frequency={self.config['min_word_frequency']}): {self.vocab_size}\")\n",
    "\n",
    "        all_seqs = self.tokenizer.texts_to_sequences(self.all_captions)\n",
    "        self.max_caption_length = max(len(s) for s in all_seqs)\n",
    "        # Update CONFIG with the dynamically determined max_caption_length\n",
    "        self.config['max_caption_length'] = self.max_caption_length\n",
    "        print(f\"Max caption length: {self.max_caption_length}\")\n",
    "        \n",
    "    def preprocess_text(self, caption: str) -> str:\n",
    "        \"\"\"\n",
    "        Lowercases, removes non-alpha chars, trims, and adds <start>/<end> tokens.\n",
    "        \"\"\"\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z ]\", \"\", caption)\n",
    "        caption = re.sub(r'\\s+', ' ', caption).strip()\n",
    "        caption = '<start> ' + caption + ' <end>'\n",
    "        return caption\n",
    "\n",
    "    def create_dataset_splits(self, train_ratio=0.8, val_ratio=0.1):\n",
    "        \"\"\"\n",
    "        Shuffles and splits image paths into train/val/test, then pairs with captions.\n",
    "        \"\"\"\n",
    "        random.shuffle(self.image_paths)\n",
    "        num_images = len(self.image_paths)\n",
    "        num_train = int(train_ratio * num_images)\n",
    "        num_val = int(val_ratio * num_images)\n",
    "\n",
    "        train_image_paths = self.image_paths[:num_train]\n",
    "        val_image_paths = self.image_paths[num_train:num_train + num_val]\n",
    "        test_image_paths = self.image_paths[num_train + num_val:]\n",
    "\n",
    "        print(f\"Train images: {len(train_image_paths)}, Val images: {len(val_image_paths)}, Test images: {len(test_image_paths)}\")\n",
    "\n",
    "        self.train_data = self._create_pairs(train_image_paths)\n",
    "        self.val_data = self._create_pairs(val_image_paths)\n",
    "        self.test_data = self._create_pairs(test_image_paths)\n",
    "\n",
    "        print(f\"Train pairs: {len(self.train_data)}, Val pairs: {len(self.val_data)}, Test pairs: {len(self.test_data)}\")\n",
    "\n",
    "        self.num_steps_per_epoch = len(self.train_data) // self.config['batch_size']\n",
    "\n",
    "    def _create_pairs(self, image_names: List[str]) -> List[Tuple[str, List[int]]]:\n",
    "        \"\"\"\n",
    "        For each image, pairs it with all its captions (as padded int sequences).\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        for img_name in image_names:\n",
    "            full_img_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            for caption in self.img_to_cap_map[img_name]:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                padded_seq = pad_sequences([seq], maxlen=self.max_caption_length, padding='post')[0]\n",
    "                pairs.append((full_img_path, list(padded_seq)))\n",
    "        return pairs\n",
    "\n",
    "    def get_data_with_cached_features(self, image_name_to_cached_path_map: Dict[str, str]) -> Tuple[List, List, List]:\n",
    "        \"\"\"\n",
    "        Replaces image paths with cached feature paths if available, for all splits.\n",
    "        Only keeps pairs where cached features exist.\n",
    "        \"\"\"\n",
    "        def _reconstruct_list(data_list):\n",
    "            reconstructed = []\n",
    "            for original_img_path, caption_ids in data_list:\n",
    "                basename = os.path.basename(original_img_path)\n",
    "                cached_path = image_name_to_cached_path_map.get(basename)\n",
    "                if cached_path and os.path.exists(cached_path): # Added check if cached file actually exists\n",
    "                    reconstructed.append((original_img_path, cached_path, caption_ids))\n",
    "            return reconstructed\n",
    "\n",
    "        final_train = _reconstruct_list(self.train_data)\n",
    "        final_val = _reconstruct_list(self.val_data)\n",
    "        final_test = _reconstruct_list(self.test_data)\n",
    "\n",
    "        print(f\"Adjusted train pairs (after feature caching check): {len(final_train)}\")\n",
    "        print(f\"Adjusted val pairs (after feature caching check): {len(final_val)}\")\n",
    "        print(f\"Adjusted test pairs (after feature caching check): {len(final_test)}\")\n",
    "        \n",
    "        return final_train, final_val, final_test\n",
    "\n",
    "    def display_samples(self, num_samples: int = 5):\n",
    "        \"\"\"\n",
    "        Randomly picks a few images and displays them with their ground truth captions.\n",
    "        Useful for sanity check of data loading and preprocessing.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Displaying {num_samples} Random Dataset Samples ---\")\n",
    "\n",
    "        # Get a list of all image paths that have associated captions\n",
    "        available_image_paths = sorted(list(self.img_to_cap_map.keys()))\n",
    "\n",
    "        if not available_image_paths:\n",
    "            print(\"No image paths with captions available to display.\")\n",
    "            return\n",
    "\n",
    "        # Randomly select a few image names\n",
    "        samples_to_display = random.sample(available_image_paths, min(num_samples, len(available_image_paths)))\n",
    "\n",
    "        for i, img_name in enumerate(samples_to_display):\n",
    "            full_image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "\n",
    "            if not os.path.exists(full_image_path):\n",
    "                print(f\"Warning: Image file not found for {img_name} at {full_image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n--- Sample {i+1}/{num_samples} ---\")\n",
    "            print(f\"Image Name: {img_name}\")\n",
    "\n",
    "            try:\n",
    "                img = Image.open(full_image_path)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Image: {img_name}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or displaying image {img_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            gt_captions = self.img_to_cap_map.get(img_name, [])\n",
    "            print(\"Ground Truth Captions:\")\n",
    "            if gt_captions:\n",
    "                for j, cap in enumerate(gt_captions):\n",
    "                    # Clean up <start> and <end> tokens for display\n",
    "                    clean_cap = cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                    print(f\"  {j+1}. {clean_cap}\")\n",
    "            else:\n",
    "                print(\"  No ground truth captions available for this image.\")\n",
    "\n",
    "    def plot_caption_length_histogram(self):\n",
    "        caption_lengths = [len(s.split()) for s in self.all_captions]\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.hist(caption_lengths, bins=range(min(caption_lengths), max(caption_lengths) + 2), edgecolor='black')\n",
    "        plt.title('Histogram of Caption Lengths')\n",
    "        plt.xlabel('Caption Length (words)')\n",
    "        plt.ylabel('Number of Captions')\n",
    "        plt.grid(axis='y', alpha=0.75)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_top_words(self, top_n: int = 20):\n",
    "        words = [word for caption in self.all_captions for word in caption.split() \n",
    "                if word not in ['<start>', '<end>', '<pad>', '<unk>']]\n",
    "        word_counts = collections.Counter(words)\n",
    "        top_words = word_counts.most_common(top_n)\n",
    "        words_df = pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        sns.barplot(x='Frequency', y='Word', hue='Word', data=words_df, palette='viridis', legend=False)\n",
    "        plt.title(f'Top {top_n} Most Frequent Words (excluding special tokens)')\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Word')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_sample_images_with_caption(self, num_images: int = 6):\n",
    "        selected_images = random.sample(self.image_paths, min(num_images, len(self.image_paths)))\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 2, figsize=(10, 15)) # 3 rows, 2 columns for 6 images\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, img_name in enumerate(selected_images):\n",
    "            full_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            if not os.path.exists(full_path):\n",
    "                print(f\"Warning: Image {img_name} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                img = Image.open(full_path)\n",
    "                ax = axes[i]\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(f\"{img_name}\\nCap: {self.img_to_cap_map[img_name][0].replace('<start>','').replace('<end>','').strip()}\", fontsize=10)\n",
    "                ax.axis('off')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {img_name}: {e}\")\n",
    "                axes[i].set_title(\"Image Load Error\")\n",
    "                axes[i].axis('off')\n",
    "                continue\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        for j in range(i + 1, len(axes)):\n",
    "            fig.delaxes(axes[j])\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMAGE FEATURE EXTRACTION (CACHE FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor(Model):\n",
    "    \"\"\"\n",
    "    Extracts image features using a pre-trained InceptionV3 model.\n",
    "    - Resizes images to target size and preprocesses them for InceptionV3.\n",
    "    - Extracts features from the 'mixed7' layer of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_size=(299, 299)):\n",
    "        \"\"\"\n",
    "        Initializes the feature extractor with InceptionV3.\n",
    "        - Freezes the model to prevent training.\n",
    "        - Extracts features from the 'mixed7' layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.inception_v3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        self.inception_v3.trainable = False  # Freeze the model\n",
    "        self.feature_extractor = Model(inputs=self.inception_v3.input,\n",
    "                                       outputs=self.inception_v3.get_layer('mixed7').output)\n",
    "\n",
    "    @tf.function\n",
    "    def load_and_preprocess_image(self, image_path: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Loads and preprocesses an image for feature extraction.\n",
    "        - Reads the image from the given path.\n",
    "        - Decodes and resizes it to the target size.\n",
    "        - Applies InceptionV3 preprocessing.\n",
    "        \"\"\"\n",
    "        img = tf.io.read_file(image_path)  # Read image file\n",
    "        img = tf.image.decode_jpeg(img, channels=3)  # Decode JPEG image\n",
    "        img = tf.image.resize(img, self.target_size)  # Resize to target size\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)  # Preprocess for InceptionV3\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureCacheManager:\n",
    "    \"\"\"\n",
    "    Manages extraction and caching of image features using a pre-trained CNN.\n",
    "    \n",
    "    Extracts features from the 'mixed7' layer of InceptionV3 and saves them as .npy files\n",
    "    for faster training. Features are reshaped to (289, 768) representing 17x17 spatial\n",
    "    locations with 768-dimensional feature vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, feature_extractor: ImageFeatureExtractor):\n",
    "        self.config = config\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.cache_dir = config['feature_cache_dir']\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def manage_feature_cache(self, image_names: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Checks for existing cached features and extracts new ones if needed.\n",
    "        \n",
    "        Processes images in batches to efficiently extract CNN features using GPU.\n",
    "        Returns mapping from image names to their corresponding cache file paths.\n",
    "        Skips images that fail feature extraction to avoid training errors.\n",
    "        \"\"\"\n",
    "        print(f\"\\nManaging image feature cache in: {self.cache_dir}\")\n",
    "        print(f\"Checking/extracting features for {len(image_names)} unique images.\")\n",
    "\n",
    "        image_name_to_cached_path = {}\n",
    "        images_to_extract = []\n",
    "\n",
    "        # Check which images need feature extraction\n",
    "        for img_name in image_names:\n",
    "            cache_file = os.path.join(self.cache_dir, img_name + '.npy')\n",
    "            if not os.path.exists(cache_file):\n",
    "                images_to_extract.append(img_name)\n",
    "            image_name_to_cached_path[img_name] = cache_file\n",
    "\n",
    "        if images_to_extract:\n",
    "            print(f\"Found {len(images_to_extract)} images whose features need extraction...\")\n",
    "            \n",
    "            full_paths_for_extraction = [os.path.join(self.config['image_dir'], img_name) for img_name in images_to_extract]\n",
    "\n",
    "            batch_size_extraction = 16 # Adjust based on your GPU memory\n",
    "            for i in tqdm.tqdm(range(0, len(full_paths_for_extraction), batch_size_extraction), desc=\"Extracting & Caching Features\"):\n",
    "                batch_paths = full_paths_for_extraction[i:i+batch_size_extraction]\n",
    "                \n",
    "                try:\n",
    "                    # Preprocess images and extract features in batch\n",
    "                    img_tensors_processed = tf.stack([self.feature_extractor.load_and_preprocess_image(tf.constant(p)) for p in batch_paths])\n",
    "                    \n",
    "                    features_batch = self.feature_extractor.feature_extractor(img_tensors_processed)\n",
    "                    \n",
    "                    # Reshape to (batch_size, 17*17, 768) for attention mechanism\n",
    "                    features_flat_batch = tf.reshape(features_batch, (tf.shape(features_batch)[0], -1, tf.shape(features_batch)[3]))\n",
    "                    \n",
    "                    # Save features for each image separately\n",
    "                    for j, img_path in enumerate(batch_paths):\n",
    "                        img_name = os.path.basename(img_path)\n",
    "                        cache_path = os.path.join(self.cache_dir, img_name + '.npy')\n",
    "                        # Save each image's features separately as (289, 768)\n",
    "                        np.save(cache_path, features_flat_batch[j].numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing a batch during feature extraction: {e}. Skipping images in this batch.\")\n",
    "                    # Remove failed images from the mapping to prevent training issues\n",
    "                    for img_path_in_batch in batch_paths:\n",
    "                        img_name_in_batch = os.path.basename(img_path_in_batch)\n",
    "                        # Remove from map if feature extraction failed, so it's not used\n",
    "                        if img_name_in_batch in image_name_to_cached_path:\n",
    "                            del image_name_to_cached_path[img_name_in_batch]\n",
    "                    continue\n",
    "        else:\n",
    "            print(\"All image features already cached. Skipping extraction.\")\n",
    "            \n",
    "        print(\"Image feature cache management complete.\")\n",
    "        return image_name_to_cached_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    \"\"\"\n",
    "    Encoder transforms CNN features to embedding space for the decoder.\n",
    "    Uses a fully connected layer to project image features to embedding dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Project image features to embedding dimension\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Bahdanau attention mechanism for focusing on relevant image regions.\n",
    "    Computes attention weights using additive attention with tanh activation.\n",
    "    Returns context vector and attention weights for visualization.\n",
    "    \"\"\"\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        # Dense layers for computing attention scores\n",
    "        self.W1 = layers.Dense(units)  # Transform features\n",
    "        self.W2 = layers.Dense(units)  # Transform hidden state\n",
    "        self.V = layers.Dense(1)       # Final attention score\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        # Add time dimension to hidden state for broadcasting\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # Compute attention scores using additive attention\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        \n",
    "        # Convert scores to probabilities\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        \n",
    "        # Compute weighted sum of features\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, tf.squeeze(attention_weights, -1)\n",
    "\n",
    "class Decoder(Model):\n",
    "    \"\"\"\n",
    "    LSTM-based decoder with attention for generating captions word by word.\n",
    "    Takes embedded words and attended image features to predict next word.\n",
    "    Handles mixed precision training by casting states to appropriate dtype.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, units, vocab_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "        # Word embedding layer\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM with dropout for regularization\n",
    "        self.lstm = layers.LSTM(self.units,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 recurrent_initializer='glorot_uniform',\n",
    "                                 dropout=dropout)\n",
    "\n",
    "        # Dense layers for output projection\n",
    "        self.fc1 = layers.Dense(self.units)\n",
    "        self.fc2 = layers.Dense(vocab_size, dtype='float32')  # Always float32 for stable logits\n",
    "\n",
    "        # Attention mechanism\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden_state, cell_state):\n",
    "        # Get context vector from attention\n",
    "        context_vector, attention_weights = self.attention(features, hidden_state)\n",
    "\n",
    "        # Embed input word\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Concatenate context and embedded word\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Handle mixed precision by casting to float16 if needed\n",
    "        if tf.keras.mixed_precision.global_policy().compute_dtype == 'float16':\n",
    "            hidden_state = tf.cast(hidden_state, tf.float16)\n",
    "            cell_state = tf.cast(cell_state, tf.float16)\n",
    "            x = tf.cast(x, tf.float16)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        output, new_hidden_state, new_cell_state = self.lstm(x, initial_state=[hidden_state, cell_state])\n",
    "\n",
    "        # Flatten for dense layer\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # Project to vocabulary size\n",
    "        x = self.fc1(output)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return logits, new_hidden_state, new_cell_state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TEXT-TO-SPEECH UTILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSpeech:\n",
    "    \"\"\"\n",
    "    Simple wrapper around gTTS (Google Text-to-Speech) for audio playback in Jupyter.\n",
    "    Handles import errors gracefully and provides audio generation for generated captions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.gTTS = gTTS\n",
    "            self.Audio = Audio\n",
    "            self.display = display\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"WARNING: gTTS or IPython.display not found. Speech functionality will be disabled.\")\n",
    "            self.available = False\n",
    "\n",
    "    def speak(self, text: str, filename: str = \"caption_audio.mp3\"):\n",
    "        \"\"\"\n",
    "        Converts text to speech and plays audio in Jupyter notebook.\n",
    "        Saves audio file locally and uses IPython.display.Audio for playback.\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            print(\"Text-to-speech functionality is not available. Please install 'gtts' and ensure running in an IPython environment.\")\n",
    "            return\n",
    "        \n",
    "        if not text.strip():\n",
    "            print(\"Empty text, nothing to speak.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Generate speech audio file\n",
    "            tts = self.gTTS(text=text, lang='en')\n",
    "            tts.save(filename)\n",
    "            # Play audio in notebook\n",
    "            self.display(self.Audio(filename))\n",
    "            print(f\"Audio saved to {filename} and played.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating or playing audio: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TRAINING LOOP & UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioning:\n",
    "    \"\"\"\n",
    "    Main image captioning model that combines encoder-decoder architecture with attention mechanism.\n",
    "    Uses pre-extracted image features and generates captions using greedy search or beam search.\n",
    "    Includes training loop with early stopping, BLEU evaluation, and visualization capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config, processor: DataProcessor, feature_extractor: ImageFeatureExtractor):\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        self.encoder = Encoder(self.config['embedding_dim'])\n",
    "        self.decoder = Decoder(self.config['embedding_dim'], self.config['units'],\n",
    "                               self.processor.vocab_size, self.config['decoder_dropout'])\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate'])\n",
    "        if self.config['mixed_precision']:\n",
    "            self.optimizer = tf.keras.mixed_precision.LossScaleOptimizer(self.optimizer)\n",
    "\n",
    "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "        self.checkpoint_prefix = os.path.join(self.config['checkpoint_path'], \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(encoder=self.encoder,\n",
    "                                             decoder=self.decoder,\n",
    "                                             optimizer=self.optimizer)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, self.config['checkpoint_path'], max_to_keep=5)\n",
    "\n",
    "        self.tts_speaker = TextToSpeech()\n",
    "\n",
    "        # Initialize history lists (will be loaded if checkpoint exists)\n",
    "        self.train_loss_results = []\n",
    "        self.val_bleu_results = []\n",
    "        self.best_val_bleu = 0.0\n",
    "        self.patience_counter = 0\n",
    "        self.smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "        # Paths for history files\n",
    "        self.train_loss_history_path = os.path.join(self.config['checkpoint_path'], \"train_loss_history.json\")\n",
    "        self.val_bleu_history_path = os.path.join(self.config['checkpoint_path'], \"val_bleu_history.json\")\n",
    "        self.best_val_bleu_path = os.path.join(self.config['checkpoint_path'], \"best_val_bleu.txt\")\n",
    "        self.patience_counter_path = os.path.join(self.config['checkpoint_path'], \"patience_counter.txt\")\n",
    "\n",
    "\n",
    "        if self.checkpoint_manager.latest_checkpoint:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            print(f\"Restored from {self.checkpoint_manager.latest_checkpoint}\")\n",
    "            self._load_history() # Load history after restoring checkpoint\n",
    "        else:\n",
    "            print(\"Initializing from scratch.\")\n",
    "        \n",
    "        # Ensure checkpoint path exists for saving history files\n",
    "        os.makedirs(self.config['checkpoint_path'], exist_ok=True)\n",
    "\n",
    "    def _save_history(self):\n",
    "        \"\"\"Saves training history and best metric/patience to files.\"\"\"\n",
    "        try:\n",
    "            with open(self.train_loss_history_path, 'w') as f:\n",
    "                # Convert numpy floats to native floats for JSON serialization\n",
    "                json.dump([float(x) for x in self.train_loss_results], f)\n",
    "            with open(self.val_bleu_history_path, 'w') as f:\n",
    "                json.dump([float(x) for x in self.val_bleu_results], f)\n",
    "            with open(self.best_val_bleu_path, 'w') as f:\n",
    "                f.write(str(self.best_val_bleu))\n",
    "            with open(self.patience_counter_path, 'w') as f:\n",
    "                f.write(str(self.patience_counter))\n",
    "            print(\"Training history and states saved.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not save training history and states: {e}\")\n",
    "\n",
    "    def _load_history(self):\n",
    "        \"\"\"Loads training history and best metric/patience from files.\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.train_loss_history_path):\n",
    "                with open(self.train_loss_history_path, 'r') as f:\n",
    "                    self.train_loss_results = json.load(f)\n",
    "            if os.path.exists(self.val_bleu_history_path):\n",
    "                with open(self.val_bleu_history_path, 'r') as f:\n",
    "                    self.val_bleu_results = json.load(f)\n",
    "            if os.path.exists(self.best_val_bleu_path):\n",
    "                with open(self.best_val_bleu_path, 'r') as f:\n",
    "                    self.best_val_bleu = float(f.read())\n",
    "            if os.path.exists(self.patience_counter_path):\n",
    "                with open(self.patience_counter_path, 'r') as f:\n",
    "                    self.patience_counter = int(f.read())\n",
    "            print(\"Training history and states loaded.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not load training history and states: {e}\")\n",
    "\n",
    "    def loss_function(self, real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = self.loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, img_tensor, target):\n",
    "        batch_size = tf.shape(target)[0]\n",
    "        loss = 0.0\n",
    "\n",
    "        current_policy = tf.keras.mixed_precision.global_policy()\n",
    "        if current_policy.name == 'mixed_float16':\n",
    "            initial_state_dtype = tf.float16\n",
    "        else:\n",
    "            initial_state_dtype = tf.float32\n",
    "\n",
    "        hidden = tf.zeros((batch_size, self.config['units']), dtype=initial_state_dtype)\n",
    "        cell = tf.zeros((batch_size, self.config['units']), dtype=initial_state_dtype)\n",
    "\n",
    "        dec_input = tf.fill([batch_size, 1], self.processor.tokenizer.word_index['<start>'])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)\n",
    "\n",
    "            attention_sum_square_error = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "            for i in tf.range(1, target.shape[1]):\n",
    "                predictions, hidden, cell, attention_weights = self.decoder(dec_input, features, hidden, cell)\n",
    "\n",
    "                loss += self.loss_function(target[:, i], predictions)\n",
    "\n",
    "                if self.config.get('scheduled_sampling_max_prob', 0.0) > 0:\n",
    "                    random_probs = tf.random.uniform([batch_size], 0, 1, dtype=tf.float32)\n",
    "                    \n",
    "                    threshold = tf.fill([batch_size], self.current_scheduled_sampling_prob)\n",
    "                    \n",
    "                    use_predicted_mask = random_probs < threshold\n",
    "                    \n",
    "                    predicted_ids = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "                    \n",
    "                    true_ids = target[:, i]\n",
    "                    \n",
    "                    dec_input = tf.where(use_predicted_mask, predicted_ids, true_ids)\n",
    "                    \n",
    "                    dec_input = tf.expand_dims(dec_input, 1)\n",
    "                else:\n",
    "                    dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "                attention_sum_square_error += tf.reduce_mean(\n",
    "                    tf.square(tf.cast(tf.reduce_sum(attention_weights, axis=1), tf.float32) - 1.0)\n",
    "                )\n",
    "\n",
    "            total_loss = (loss / tf.cast(target.shape[1] - 1, tf.float32))\n",
    "            total_loss += self.config['attention_reg_lambda'] * attention_sum_square_error / tf.cast(target.shape[1] - 1, tf.float32)\n",
    "\n",
    "            if self.config['mixed_precision']:\n",
    "                scaled_loss = self.optimizer.get_scaled_loss(total_loss)\n",
    "            else:\n",
    "                scaled_loss = total_loss\n",
    "\n",
    "        trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        gradients = tape.gradient(scaled_loss, trainable_variables)\n",
    "\n",
    "        if self.config['mixed_precision']:\n",
    "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
    "\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def evaluate_bleu_score(self, dataset_pairs: List[Tuple[str, List[int]]], num_samples=None):\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        if num_samples is None:\n",
    "            samples_to_evaluate = dataset_pairs\n",
    "        else:\n",
    "            samples_to_evaluate = random.sample(dataset_pairs, min(num_samples, len(dataset_pairs)))\n",
    "\n",
    "        print(f\"\\nEvaluating BLEU on {len(samples_to_evaluate)} samples...\")\n",
    "        for img_path, _ in tqdm.tqdm(samples_to_evaluate):\n",
    "            generated_caption_tokens = self.greedy_inference(img_path)\n",
    "            \n",
    "            if not generated_caption_tokens:\n",
    "                generated_caption_tokens, _ = self.beam_search_inference(img_path, beam_size=3)\n",
    "\n",
    "            if not generated_caption_tokens:\n",
    "                continue\n",
    "\n",
    "            hypotheses.append(generated_caption_tokens)\n",
    "\n",
    "            img_name = os.path.basename(img_path)\n",
    "            raw_captions = self.processor.img_to_cap_map.get(img_name, [])\n",
    "            \n",
    "            img_references = []\n",
    "            for raw_cap in raw_captions:\n",
    "                cleaned_cap = raw_cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                if cleaned_cap:\n",
    "                    img_references.append(cleaned_cap.split())\n",
    "            \n",
    "            if img_references:\n",
    "                references.append(img_references)\n",
    "            else:\n",
    "                hypotheses.pop()\n",
    "\n",
    "        if not references:\n",
    "            print(\"No valid reference captions found for BLEU evaluation after filtering.\")\n",
    "            return {\"bleu-1\": 0.0, \"bleu-2\": 0.0, \"bleu-3\": 0.0, \"bleu-4\": 0.0}\n",
    "\n",
    "        bleu_scores = {}\n",
    "        for n in range(1, 5):\n",
    "            weights = (1.0 / n,) * n + (0.0,) * (4 - n)\n",
    "            bleu_scores[f\"bleu-{n}\"] = corpus_bleu(references, hypotheses, weights=weights,\n",
    "                                                   smoothing_function=self.smoothing_function)\n",
    "            print(f\"BLEU-{n}: {bleu_scores[f'bleu-{n}']:.4f}\")\n",
    "            \n",
    "        return bleu_scores\n",
    "\n",
    "    def greedy_inference(self, image_path: str):\n",
    "        filename = os.path.basename(image_path)\n",
    "        feature_cache_path = os.path.join(self.config['feature_cache_dir'], filename + '.npy')\n",
    "        \n",
    "        if not os.path.exists(feature_cache_path):\n",
    "            print(f\"Error: Feature cache not found for {image_path}\")\n",
    "            return []\n",
    "\n",
    "        img_features = np.load(feature_cache_path)\n",
    "        img_features_tensor = tf.convert_to_tensor(img_features, dtype=tf.float32)\n",
    "        \n",
    "        img_features_tensor = tf.expand_dims(img_features_tensor, 0)\n",
    "\n",
    "        features = self.encoder(img_features_tensor)\n",
    "\n",
    "        hidden = tf.zeros((1, self.config['units']), dtype=tf.float32)\n",
    "        cell = tf.zeros((1, self.config['units']), dtype=tf.float32)\n",
    "\n",
    "        dec_input = tf.expand_dims([self.processor.tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "        result = []\n",
    "        for i in range(self.config['max_caption_length']):\n",
    "            predictions, hidden, cell, _ = self.decoder(dec_input, features, hidden, cell)\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "            predicted_word = self.processor.tokenizer.index_word.get(predicted_id, '<unk>')\n",
    "\n",
    "            if predicted_word == '<end>':\n",
    "                break\n",
    "            if predicted_word not in ('<unk>', '<start>', '<pad>'):\n",
    "                result.append(predicted_word)\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def beam_search_inference(self, image_path: str, beam_size: int = 3, length_penalty_weight: float = 0.7):\n",
    "        filename = os.path.basename(image_path)\n",
    "        feature_cache_path = os.path.join(self.config['feature_cache_dir'], filename + '.npy')\n",
    "        if not os.path.exists(feature_cache_path):\n",
    "            print(f\"Error: Feature cache not found for {image_path}\")\n",
    "            return [], []\n",
    "\n",
    "        img_features = np.load(feature_cache_path)\n",
    "        img_features_tensor = tf.convert_to_tensor(img_features, dtype=tf.float32)\n",
    "        \n",
    "        img_features_tensor = tf.expand_dims(img_features_tensor, 0)\n",
    "\n",
    "        features = self.encoder(img_features_tensor)\n",
    "\n",
    "        start_token = self.processor.tokenizer.word_index['<start>']\n",
    "        end_token = self.processor.tokenizer.word_index['<end>']\n",
    "\n",
    "        beams = [(\n",
    "            [start_token],\n",
    "            0.0,\n",
    "            tf.zeros((1, self.config['units']), dtype=tf.float32),\n",
    "            tf.zeros((1, self.config['units']), dtype=tf.float32),\n",
    "            []\n",
    "        )]\n",
    "\n",
    "        completed_beams = []\n",
    "\n",
    "        for _ in range(self.config['max_caption_length']):\n",
    "            temp_active_beams = []\n",
    "            for seq, score, hidden, cell, alphas in beams:\n",
    "                last_token = seq[-1]\n",
    "\n",
    "                if last_token == end_token:\n",
    "                    completed_beams.append((seq, score, alphas))\n",
    "                    continue\n",
    "\n",
    "                dec_input = tf.expand_dims([last_token], 0)\n",
    "                \n",
    "                predictions, new_hidden, new_cell, attention_weights = self.decoder(dec_input, features, hidden, cell)\n",
    "\n",
    "                predictions = tf.cast(predictions[0], tf.float32)\n",
    "                log_probs = tf.nn.log_softmax(predictions).numpy()\n",
    "\n",
    "                top_k_indices = np.argsort(log_probs)[::-1][:beam_size]\n",
    "\n",
    "                for idx in top_k_indices:\n",
    "                    token_id = int(idx)\n",
    "                    token_log_prob = float(log_probs[idx])\n",
    "                    \n",
    "                    temp_active_beams.append((\n",
    "                        seq + [token_id],\n",
    "                        score + token_log_prob,\n",
    "                        new_hidden,\n",
    "                        new_cell,\n",
    "                        alphas + [attention_weights[0].numpy()]\n",
    "                    ))\n",
    "            \n",
    "            temp_active_beams.sort(key=lambda x: x[1] / ((len(x[0]) ** length_penalty_weight) if len(x[0]) > 0 else 1.0), reverse=True)\n",
    "            beams = temp_active_beams[:beam_size]\n",
    "\n",
    "            if all(b[0][-1] == end_token for b in beams):\n",
    "                completed_beams.extend([(b[0], b[1], b[4]) for b in beams])\n",
    "                break\n",
    "        \n",
    "        completed_beams.extend([(seq, score, alphas) for seq, score, hidden, cell, alphas in beams if seq[-1] != end_token])\n",
    "\n",
    "        if not completed_beams:\n",
    "            return [], []\n",
    "\n",
    "        best_seq, best_score, best_alphas = max(completed_beams, key=lambda x: x[1] / ((len(x[0]) ** length_penalty_weight) if len(x[0]) > 0 else 1.0))\n",
    "\n",
    "        caption_words = [self.processor.tokenizer.index_word.get(i, '<unk>') for i in best_seq]\n",
    "        \n",
    "        filtered_caption_words = [\n",
    "            word for word in caption_words\n",
    "            if word not in ['<start>', '<end>', '<pad>', '<unk>']\n",
    "        ]\n",
    "\n",
    "        return filtered_caption_words, best_alphas\n",
    "\n",
    "    def train(self, train_cached_paths: List[Tuple[str, str, List[int]]],\n",
    "              val_dataset_pairs: List[Tuple[str, List[int]]]):\n",
    "        \n",
    "        def _data_generator():\n",
    "            for orig_img_path, cache_path, caption_ids in train_cached_paths:\n",
    "                features = np.load(cache_path)\n",
    "                features = tf.cast(features, tf.float32)\n",
    "                yield features, np.array(caption_ids, dtype=np.int32)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            _data_generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(289, 768), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.config['max_caption_length'],), dtype=tf.int32)\n",
    "            )\n",
    "        )\n",
    "        train_dataset = train_dataset.shuffle(self.config['buffer_size']).batch(self.config['batch_size'])\n",
    "        train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        self.processor.num_steps_per_epoch = len(train_cached_paths) // self.config['batch_size']\n",
    "        if self.processor.num_steps_per_epoch == 0:\n",
    "            print(\"Warning: num_steps_per_epoch is 0. Batch size might be too large or dataset too small. Setting to 1.\")\n",
    "            self.processor.num_steps_per_epoch = 1\n",
    "\n",
    "        initial_epoch = tf.cast(self.optimizer.iterations, tf.float32) // self.processor.num_steps_per_epoch\n",
    "        initial_epoch = tf.cast(initial_epoch, tf.int32).numpy()\n",
    "\n",
    "        print(f\"Resuming training from epoch {initial_epoch + 1}.\")\n",
    "        print(f\"Targeting a total of {self.config['epochs']} epochs.\")\n",
    "\n",
    "        for epoch in range(initial_epoch, self.config['epochs']):\n",
    "            start = time.time()\n",
    "            total_loss = 0\n",
    "\n",
    "            self.current_scheduled_sampling_prob = (\n",
    "                self.config['scheduled_sampling_max_prob'] * (epoch / max(1, self.config['epochs'] - 1))\n",
    "            )\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config['epochs']} (Scheduled Sampling Prob: {self.current_scheduled_sampling_prob:.3f})\")\n",
    "\n",
    "            for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "                batch_loss = self.train_step(img_tensor, target)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "                if (self.optimizer.iterations.numpy() % 100 == 0) and (self.optimizer.iterations.numpy() > 0):\n",
    "                    print(f'Global Step {self.optimizer.iterations.numpy()} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "            \n",
    "            avg_train_loss = total_loss / self.processor.num_steps_per_epoch\n",
    "            \n",
    "            # Ensure train_loss_results has enough entries for the current epoch\n",
    "            if len(self.train_loss_results) <= epoch:\n",
    "                self.train_loss_results.append(avg_train_loss.numpy())\n",
    "            else:\n",
    "                self.train_loss_results[epoch] = avg_train_loss.numpy()\n",
    "\n",
    "            print(f'Epoch {epoch+1} Loss {avg_train_loss:.4f}')\n",
    "\n",
    "            val_bleu_scores = self.evaluate_bleu_score(val_dataset_pairs, num_samples=min(1000, len(val_dataset_pairs)))\n",
    "            current_val_bleu4 = val_bleu_scores.get('bleu-4', 0.0)\n",
    "            \n",
    "            # Ensure val_bleu_results has enough entries for the current epoch\n",
    "            if len(self.val_bleu_results) <= epoch:\n",
    "                self.val_bleu_results.append(current_val_bleu4)\n",
    "            else:\n",
    "                self.val_bleu_results[epoch] = current_val_bleu4\n",
    "\n",
    "            if current_val_bleu4 > self.best_val_bleu:\n",
    "                self.best_val_bleu = current_val_bleu4\n",
    "                self.checkpoint_manager.save()\n",
    "                self._save_history() # Save history whenever a checkpoint is saved\n",
    "                print(f\"Saving checkpoint at epoch {epoch+1} with BLEU-4: {current_val_bleu4:.4f}\")\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                print(f\"BLEU-4 not improved. Patience counter: {self.patience_counter}/{self.config['patience']}\")\n",
    "                if self.patience_counter >= self.config['patience']:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    break\n",
    "\n",
    "            print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs')\n",
    "\n",
    "    def plot_history(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_results, label='Train Loss')\n",
    "        plt.title('Training Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_bleu_results, label='Validation BLEU-4')\n",
    "        plt.title('Validation BLEU-4 Score per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4 Score')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention(self, image_path: str, caption: List[str], alphas: List[np.ndarray]):\n",
    "        img = Image.open(image_path)\n",
    "        img = np.array(img.resize((299, 299)))\n",
    "\n",
    "        num_words = len(caption)\n",
    "        cols = min(5, num_words)\n",
    "        rows = (num_words + cols - 1) // cols\n",
    "\n",
    "        fig_width = cols * 4\n",
    "        fig_height = rows * 4 + 2\n",
    "\n",
    "        fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "        fig.subplots_adjust(wspace=0.1, hspace=0.3)\n",
    "\n",
    "        for t in range(num_words):\n",
    "            if t >= (rows * cols): \n",
    "                break\n",
    "            \n",
    "            ax = fig.add_subplot(rows, cols, t + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_grid_size = int(np.sqrt(alpha.shape[0]))\n",
    "            alpha_reshaped = alpha.reshape(attention_grid_size, attention_grid_size)\n",
    "\n",
    "            alpha_resized = Image.fromarray(np.uint8(255 * alpha_reshaped)).resize(\n",
    "                (299, 299), resample=Image.Resampling.BICUBIC\n",
    "            )\n",
    "            alpha_resized = np.array(alpha_resized) / 255.0\n",
    "\n",
    "            ax.imshow(alpha_resized, cmap='jet', alpha=0.4, extent=(0, 299, 299, 0))\n",
    "            ax.set_title(f\"{t+1}: '{caption[t]}'\", fontsize=12, color='blue', va='bottom')\n",
    "\n",
    "        plt.suptitle(f\"Attention Map for: {os.path.basename(image_path)}\", fontsize=16, y=0.98)\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.show()\n",
    "\n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        self.tts_speaker.speak(caption, filename)\n",
    "\n",
    "    def demo(self, image_file_name: str):\n",
    "        full_image_path = os.path.join(self.config['image_dir'], image_file_name)\n",
    "        \n",
    "        if not os.path.exists(full_image_path):\n",
    "            print(f\"Error: Image not found at {full_image_path}\")\n",
    "            return\n",
    "\n",
    "        img = Image.open(full_image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {image_file_name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        gt_captions = self.processor.img_to_cap_map.get(image_file_name, [])\n",
    "        print(\"\\nGround Truth Captions:\")\n",
    "        if gt_captions:\n",
    "            for i, cap in enumerate(gt_captions):\n",
    "                clean_cap = cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                print(f\"  {i+1}. {clean_cap}\")\n",
    "        else:\n",
    "            print(\"  No ground truth captions available.\")\n",
    "        \n",
    "        generated_caption_words, attention_weights = self.beam_search_inference(full_image_path, beam_size=3)\n",
    "        generated_caption = \" \".join(generated_caption_words)\n",
    "        print(f\"\\nGenerated Caption (Beam Search): {generated_caption}\")\n",
    "\n",
    "        print(\"\\nPlaying generated caption:\")\n",
    "        self.speak_caption(generated_caption, filename=f\"caption_audio_{os.path.basename(image_file_name).split('.')[0]}.mp3\")\n",
    "\n",
    "        if generated_caption_words and attention_weights:\n",
    "            self.plot_attention(full_image_path, generated_caption_words, attention_weights)\n",
    "        else:\n",
    "            print(\"Could not generate caption or attention for plotting.\")\n",
    "\n",
    "    def run_prediction_samples(self, dataset_pairs: List[Tuple[str, List[int]]], num_samples: int = 10):\n",
    "        \"\"\"\n",
    "        Loops over N random cases from the dataset, prints ground truth captions and predictions,\n",
    "        and provides prediction time statistics.\n",
    "\n",
    "        Args:\n",
    "            dataset_pairs: A list of (image_path, caption_ids) tuples for the dataset split (e.g., test_data).\n",
    "            num_samples: The number of random samples to predict and display.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Running Predictions for {num_samples} Random Samples ---\")\n",
    "\n",
    "        if not dataset_pairs:\n",
    "            print(\"No data available for prediction samples.\")\n",
    "            return\n",
    "\n",
    "        samples_to_predict = random.sample(dataset_pairs, min(num_samples, len(dataset_pairs)))\n",
    "\n",
    "        prediction_times = []\n",
    "\n",
    "        for i, (img_path, _) in enumerate(samples_to_predict):\n",
    "            img_name = os.path.basename(img_path)\n",
    "            \n",
    "            print(f\"\\n--- Prediction Sample {i+1}/{num_samples} (Image: {img_name}) ---\")\n",
    "            \n",
    "            # Ground Truth Captions\n",
    "            gt_captions = self.processor.img_to_cap_map.get(img_name, [])\n",
    "            print(\"Ground Truth Captions:\")\n",
    "            if gt_captions:\n",
    "                for j, cap in enumerate(gt_captions):\n",
    "                    clean_cap = cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                    print(f\"  {j+1}. {clean_cap}\")\n",
    "            else:\n",
    "                print(\"  No ground truth captions available for this image.\")\n",
    "\n",
    "            # Generated Caption and Prediction Time\n",
    "            start_time = time.perf_counter()\n",
    "            generated_caption_words, _ = self.beam_search_inference(img_path, beam_size=3)\n",
    "            end_time = time.perf_counter()\n",
    "            \n",
    "            prediction_time = (end_time - start_time) * 1000 # in milliseconds\n",
    "            prediction_times.append(prediction_time)\n",
    "\n",
    "            generated_caption = \" \".join(generated_caption_words)\n",
    "            print(f\"Generated Caption (Beam Search): {generated_caption}\")\n",
    "            print(f\"Prediction Time: {prediction_time:.2f} ms\")\n",
    "\n",
    "        if prediction_times:\n",
    "            avg_time = np.mean(prediction_times)\n",
    "            std_time = np.std(prediction_times)\n",
    "            print(f\"\\n--- Prediction Statistics ({len(prediction_times)} samples) ---\")\n",
    "            print(f\"Average Prediction Time: {avg_time:.2f} ms\")\n",
    "            print(f\"Standard Deviation of Prediction Time: {std_time:.2f} ms\")\n",
    "        else:\n",
    "            print(\"\\nNo predictions were made.\")\n",
    "\n",
    "    def summary(self):\n",
    "        \"\"\"\n",
    "        Prints the summary of the Encoder and Decoder models.\n",
    "        Builds the models using dummy input tensors if they haven't been built yet.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Model Summaries ---\")\n",
    "\n",
    "        # Encoder Model Summary\n",
    "        print(\"\\nEncoder Model Summary:\")\n",
    "        # Encoder input shape is (batch_size, 289, 768)\n",
    "        dummy_encoder_input = tf.zeros((1, 289, 768), dtype=tf.float32)\n",
    "        # Call the encoder once to build it\n",
    "        _ = self.encoder(dummy_encoder_input)\n",
    "        self.encoder.summary()\n",
    "\n",
    "        # Decoder Model Summary\n",
    "        print(\"\\nDecoder Model Summary:\")\n",
    "        # Decoder's call method signature: call(self, x, features, hidden_state, cell_state)\n",
    "        # We need to provide dummy tensors for all these arguments to build it.\n",
    "        # x: (batch_size, 1) for token ID\n",
    "        # features: (batch_size, 289, embedding_dim)\n",
    "        # hidden_state: (batch_size, units)\n",
    "        # cell_state: (batch_size, units)\n",
    "\n",
    "        batch_size_dummy = 1\n",
    "        embedding_dim_dummy = self.config['embedding_dim']\n",
    "        units_dummy = self.config['units']\n",
    "\n",
    "        dummy_dec_input = tf.zeros((batch_size_dummy, 1), dtype=tf.int32) # Word ID\n",
    "        dummy_features = tf.zeros((batch_size_dummy, 289, embedding_dim_dummy), dtype=tf.float32)\n",
    "        dummy_hidden_state = tf.zeros((batch_size_dummy, units_dummy), dtype=tf.float32)\n",
    "        dummy_cell_state = tf.zeros((batch_size_dummy, units_dummy), dtype=tf.float32)\n",
    "        \n",
    "        # Call the decoder once with dummy inputs to build it\n",
    "        _ = self.decoder(dummy_dec_input, dummy_features, dummy_hidden_state, dummy_cell_state)\n",
    "        self.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DATA AND FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.plot_caption_length_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.plot_top_words(top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.plot_sample_images_with_caption(num_images=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of the built dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.display_samples(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Val, Test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_dataset_splits(train_ratio=0.85, val_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model = ImageFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_manager = ImageFeatureCacheManager(CONFIG, feature_extractor_model)\n",
    "image_name_to_cached_path_map = cache_manager.manage_feature_cache(processor.image_paths)\n",
    "final_train_data, final_val_data, final_test_data = processor.get_data_with_cached_features(image_name_to_cached_path_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CAPTIONING MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ImageCaptioning(CONFIG, processor, feature_extractor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(final_train_data, processor.val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional training runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG['epochs'] = 40\n",
    "trainer.train(final_train_data, processor.val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG['epochs'] = 50\n",
    "trainer.train(final_train_data, processor.val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate_bleu_score(processor.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.run_prediction_samples(processor.test_data, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHOW AND TELL DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processor.test_data:\n",
    "    random_test_img_path, _ = random.choice(processor.test_data)\n",
    "    trainer.demo(os.path.basename(random_test_img_path))\n",
    "else:\n",
    "    print(\"No test data available for demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processor.test_data:\n",
    "    random_test_img_path, _ = random.choice(processor.test_data)\n",
    "    trainer.demo(os.path.basename(random_test_img_path))\n",
    "else:\n",
    "    print(\"No test data available for demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision with other notable work with Flickr-8k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model / Paper (Flickr-8k test split) | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 |\n",
    "|-------------------------------------|:------:|:------:|:------:|:------:|\n",
    "| **My best checkpoint (Attn)**      | â–¢      | â–¢      | â–¢      | â–¢      |\n",
    "| Xu et al., 2015 â€” *Soft-Attention*  | 0.67 | 0.448 | 0.299 | 0.195  |\n",
    "| Xu et al., 2015 â€” *Hard-Attention*  | 0.67 | 0.457 | 0.314 | 0.213  |\n",
    "| Google NIC (Vinyals et al., 2015)   | 0.63 | 0.41  | 0.27  | 0.160* |\n",
    "\n",
    "* BLEU-4 for NIC comes from a later re-implementation that reports full n-gram scores on the official Flickr-8k split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: Kelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,  \n",
    "      Ruslan Salakhutdinov, Richard Zemel & Yoshua Bengio.  \n",
    "      *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*.  \n",
    "      Proceedings of the 32nd International Conference on Machine Learning (ICML 2015),  \n",
    "      PMLR 37, pp. 2048â€“2057. :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "[^2]: Oriol Vinyals, Alexander Toshev, Samy Bengio & Dumitru Erhan.  \n",
    "      *Show and Tell: A Neural Image Caption Generator*.  \n",
    "      Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2015),  \n",
    "      pp. 3156â€“3164. DOI 10.1109/CVPR.2015.7298935. :contentReference[oaicite:1]{index=1}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
