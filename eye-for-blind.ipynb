{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye for Blind – Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "Eye for Blind: An Assistive Image Captioning System with Visual Attention\n",
    "\n",
    "This project implements a deep learning model that generates natural language descriptions of images, particularly aimed at visually impaired users. The model leverages an attention mechanism to selectively focus on image regions when generating each word, mimicking human vision.\n",
    "\n",
    "Inspired by \"Show, Attend and Tell\" (Xu et al., 2015), this implementation:\n",
    "1. Uses a CNN encoder (InceptionV3) to extract image features.\n",
    "2. Applies additive (Bahdanau) attention during decoding.\n",
    "3. Employs a decoder LSTM to generate captions.\n",
    "4. Converts generated captions to speech using gTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf #type: ignore\n",
    "from tensorflow.keras import layers, Model #type: ignore\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay #type: ignore\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy #type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #type: ignore\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction #type: ignore\n",
    "from gtts import gTTS #type: ignore\n",
    "from IPython.display import Audio, display\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'subset_ratio' : 1.0,\n",
    "    'image_dir': '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file': '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    \n",
    "    # GPU Utilization\n",
    "    'batch_size': 128,           # Fully utilize 48GB VRAM; reduce if OOM\n",
    "    'buffer_size': 10000,        # Larger shuffle buffer helps training stability\n",
    "    \n",
    "    # Model Capacity\n",
    "    'max_length': 30,            # Reasonable for captions\n",
    "    'embedding_dim': 512,        # Good for attention + LSTM\n",
    "    'units': 512,                # LSTM/Attention size\n",
    "    \n",
    "    # Training Behavior\n",
    "    'seed': 42,\n",
    "    'epochs': 20,                # Slightly more for small dataset\n",
    "    'patience': 10,              # Early stopping tolerance\n",
    "    'learning_rate': 3e-4,       # Lower for small datasets to reduce overfitting\n",
    "    'grad_clip_value': 5.0,      # Prevent exploding gradients\n",
    "    \n",
    "    # Vocabulary\n",
    "    'vocab_min_count': 3,        # Include more words for small run\n",
    "    \n",
    "    # Output & Precision\n",
    "    'checkpoint_dir': './checkpoints/10pct',\n",
    "    'mixed_precision': False,     # RTX 6000 Ada has 4th-gen Tensor Cores—use them\n",
    "}\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "\n",
    "# Mixed precision policy - RTX 6000 Ada has excellent mixed precision support\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision enabled for RTX 6000 Ada\")\n",
    "\n",
    "# Single GPU setup\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Enable memory growth for RTX 6000 Ada\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    # Use default strategy for single GPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Using single GPU: {physical_devices[0].name}, batch size={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Constants\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.captions_dict = dict()\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        self.test_data = []\n",
    "    \n",
    "    def load_captions(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load and convert pipe-delimited Flickr-style caption file to a dict.\"\"\"\n",
    "        print(f\"Loading captions from {self.config['caption_file']}\")\n",
    "        df = pd.read_csv(self.config['caption_file'], sep='|', header=None, \n",
    "                         names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "        \n",
    "        caption_map = {}\n",
    "        for img, group in df.groupby('image_name'):\n",
    "            caption_map[img] = group['comment'].tolist()\n",
    "        \n",
    "        self.captions_dict = caption_map\n",
    "        print(f\"Loaded {len(caption_map)} images with captions\")\n",
    "        return caption_map\n",
    "    \n",
    "    def display_samples(self, num_samples: int = 3):\n",
    "        \"\"\"Display random images with all their associated captions.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        sample_keys = random.sample(list(self.captions_dict.keys()), min(num_samples, len(self.captions_dict)))\n",
    "\n",
    "        for key in sample_keys:\n",
    "            img_path = os.path.join(self.config['image_dir'], key)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(key)\n",
    "                plt.show()\n",
    "\n",
    "                for cap in self.captions_dict[key]:\n",
    "                    print(f\"- {cap}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {key}: {e}\")\n",
    "\n",
    "    def preprocess_caption(self, caption: str) -> Optional[str]:\n",
    "        \"\"\"Clean and format caption text.\"\"\"\n",
    "        if caption is None or not isinstance(caption, str):\n",
    "            return None\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z0-9.,? ]\", \"\", caption)\n",
    "        return f\"<start> {caption.strip()} <end>\"\n",
    "    \n",
    "    def prepare_captions(self, subset_ratio=1.0):\n",
    "        \"\"\"Process captions, build tokenizer & train/val/test splits.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        # --- 1. clean & tag ----------------------------------------------------\n",
    "        all_captions = []\n",
    "        for caps in self.captions_dict.values():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p:\n",
    "                    all_captions.append(p)\n",
    "\n",
    "        word_counts = Counter(w for cap in all_captions for w in cap.split())\n",
    "        valid_words = {w for w, cnt in word_counts.items()\n",
    "                    if cnt >= self.config['vocab_min_count']}\n",
    "\n",
    "        def keep(c):\n",
    "            return all(w in valid_words or w in ('<start>', '<end>') for w in c.split())\n",
    "\n",
    "        filtered = [c for c in all_captions if keep(c)]\n",
    "\n",
    "        # --- 2. determine max length ------------------------------------------\n",
    "        lengths = [len(c.split()) for c in filtered]\n",
    "        self.config['max_length'] = int(np.percentile(lengths, 95))\n",
    "        print(f\"max_length set to {self.config['max_length']}\")\n",
    "\n",
    "        # --- 3. build tokenizer (NO filters so < and > stay) -------------------\n",
    "        tokenizer = Tokenizer(oov_token=\"<unk>\", filters='', lower=True)\n",
    "        tokenizer.fit_on_texts(filtered)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(f\"vocab size = {self.vocab_size}\")\n",
    "\n",
    "        # --- 4. build (image, caption) list ------------------------------------\n",
    "        pairs = []\n",
    "        for img, caps in self.captions_dict.items():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p and keep(p):\n",
    "                    pairs.append((img, p))\n",
    "\n",
    "        if subset_ratio < 1.0:\n",
    "            pairs = pairs[:int(len(pairs) * subset_ratio)]\n",
    "            print(f\"subset: {len(pairs)} pairs\")\n",
    "\n",
    "        random.shuffle(pairs)\n",
    "        n = len(pairs)\n",
    "        self.train_data, self.val_data, self.test_data = (\n",
    "            pairs[:int(0.8*n)],\n",
    "            pairs[int(0.8*n):int(0.9*n)],\n",
    "            pairs[int(0.9*n):],\n",
    "        )\n",
    "        print(f\"split  →  train {len(self.train_data)} | val {len(self.val_data)} | test {len(self.test_data)}\")\n",
    "\n",
    "        return filtered\n",
    "    \n",
    "    def encode_caption(self, caption: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Convert caption text to sequence of token ids.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call prepare_captions first.\")\n",
    "        \n",
    "        seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=self.config['max_length'], padding='post')[0]\n",
    "        return padded_seq, len(seq)\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image(self, path: str) -> tf.Tensor:\n",
    "        \"\"\"Load and preprocess an image efficiently in graph mode.\"\"\"\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "        img = tf.image.resize(img, [299, 299])\n",
    "        img = tf.ensure_shape(img, [299, 299, 3])\n",
    "        return tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "    def data_generator(self, data):\n",
    "        \"\"\"Generator function for dataset creation.\"\"\"\n",
    "        for img, cap in data:\n",
    "            img_path = os.path.join(self.config['image_dir'], img)\n",
    "            img_tensor = self.load_image(tf.convert_to_tensor(img_path))\n",
    "            token_ids, cap_len = self.encode_caption(cap)\n",
    "            yield img_tensor, token_ids, cap_len\n",
    "    \n",
    "    def build_dataset(self, data, shuffle=True, cache=True):\n",
    "        \"\"\"Create a tf.data.Dataset optimized for single GPU.\"\"\"\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((299, 299, 3), tf.float32),\n",
    "            tf.TensorSpec((self.config['max_length'],), tf.int32),\n",
    "            tf.TensorSpec((), tf.int32)\n",
    "        )\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: self.data_generator(data),\n",
    "            output_signature=output_signature\n",
    "        )\n",
    "\n",
    "        if cache:\n",
    "            ds = ds.cache()\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(self.config['buffer_size'])\n",
    "\n",
    "        ds = ds.batch(self.config['batch_size'])\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"Prepare all datasets for training/validation/testing.\"\"\"\n",
    "        if not self.train_data:\n",
    "            self.prepare_captions()\n",
    "\n",
    "        print(\"Building datasets...\")\n",
    "        train_ds = self.build_dataset(self.train_data)\n",
    "        val_ds = self.build_dataset(self.val_data)\n",
    "        test_ds = self.build_dataset(self.test_data, shuffle=False)\n",
    "        \n",
    "        return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"encoder\")\n",
    "        # Use efficient model loading with feature extraction only\n",
    "        base = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, \n",
    "            weights='imagenet',\n",
    "            input_shape=(299, 299, 3)\n",
    "        )\n",
    "        base.trainable = False\n",
    "        # Use specific layer for feature extraction\n",
    "        output_layer = base.get_layer('mixed10').output\n",
    "        self.cnn = Model(inputs=base.input, outputs=output_layer)\n",
    "        self.reshape = layers.Reshape((-1, 2048))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return self.reshape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "    \n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    \"\"\"\n",
    "    “Show, Attend and Tell”-style soft-attention decoder\n",
    "    ----------------------------------------------------\n",
    "    Implements the formulation from Xu et al. (2015) including βₜ\n",
    "    (a learnable scalar gate that modulates the context vector).\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int, units: int, vocab_size: int):\n",
    "        super().__init__(name=\"decoder\")\n",
    "        self.units = units\n",
    "\n",
    "        # Layers\n",
    "        self.embedding   = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention   = BahdanauAttention(units)\n",
    "        self.f_beta      = layers.Dense(1, activation=\"sigmoid\")   # gate βₜ\n",
    "        self.lstm        = layers.LSTM(\n",
    "            units,\n",
    "            return_sequences=True,\n",
    "            return_state=True)\n",
    "        self.dropout     = layers.Dropout(0.3)\n",
    "\n",
    "        # Deep-output layer (cf. paper, Eq. (9))\n",
    "        self.fc          = layers.Dense(vocab_size)                # W_p\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        x: tf.Tensor,                 # (batch, 1)   previous word ids\n",
    "        features: tf.Tensor,          # (batch, L, 2048) image annotations\n",
    "        hidden: tf.Tensor,            # (batch, units)\n",
    "        cell: tf.Tensor               # (batch, units)\n",
    "    ) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor, tf.Tensor]:\n",
    "        # 1) Attention using h_{t-1}\n",
    "        context, alpha = self.attention(features, hidden)          # (batch, 2048), (batch, L)\n",
    "\n",
    "        # 2) Gating scalar βₜ  — “how much to attend”\n",
    "        beta    = self.f_beta(hidden)                              # (batch, 1)\n",
    "        context = beta * context                                   # (batch, 2048)\n",
    "\n",
    "        # 3) Word embedding\n",
    "        x = self.embedding(x)                                      # (batch, 1, embed_dim)\n",
    "\n",
    "        # 4) Concatenate context → input to LSTM\n",
    "        lstm_input = tf.concat([tf.expand_dims(context, 1), x], -1)  # (batch, 1, 2048+embed)\n",
    "\n",
    "        # 5) Re-cast (mixed-precision safety)\n",
    "        hidden = tf.cast(hidden, lstm_input.dtype)\n",
    "        cell   = tf.cast(cell,   lstm_input.dtype)\n",
    "\n",
    "        # 6) Recurrent update\n",
    "        lstm_out, h_t, c_t = self.lstm(lstm_input, initial_state=[hidden, cell])  # (batch,1,units)\n",
    "\n",
    "        # 7) Deep-output layer  (paper: maxout; we use tanh+linear for simplicity)\n",
    "        lstm_out = tf.squeeze(lstm_out, 1)                         # (batch, units)\n",
    "        deep_out = tf.concat([lstm_out, context], -1)              # (batch, units+2048)\n",
    "        deep_out = self.dropout(deep_out)\n",
    "        logits   = self.fc(deep_out)                               # (batch, vocab)\n",
    "\n",
    "        # Keep time-axis for compatibility with outer code\n",
    "        logits = tf.expand_dims(logits, 1)                         # (batch, 1, vocab)\n",
    "\n",
    "        return logits, h_t, c_t, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    def __init__(self, config, processor):\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.optimizer = None\n",
    "        self.loss_fn = None\n",
    "        self.ckpt_manager = None\n",
    "        self.best_bleu = 0\n",
    "        self.train_loss_log = []\n",
    "        self.val_bleu_log = []\n",
    "        self.smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build model for single GPU - no distribution strategy needed.\"\"\"\n",
    "        print(\"Building model for single GPU...\")\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim=self.config['embedding_dim'], \n",
    "            units=self.config['units'], \n",
    "            vocab_size=self.processor.vocab_size\n",
    "        )\n",
    "        \n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=self.config['learning_rate'],\n",
    "            decay_steps=10000\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        \n",
    "        # Set up checkpointing\n",
    "        ckpt_dir = self.config['checkpoint_dir']\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            encoder=self.encoder, \n",
    "            decoder=self.decoder, \n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=3)\n",
    "        \n",
    "        # Try to restore the latest checkpoint\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Restored from checkpoint: {self.ckpt_manager.latest_checkpoint}\")\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print model summaries for Encoder, Attention, and Decoder.\"\"\"\n",
    "        print(\"Building model summaries...\")\n",
    "\n",
    "        # Dummy inputs\n",
    "        dummy_image = tf.random.uniform((1, 299, 299, 3))\n",
    "        dummy_features = tf.random.uniform((1, 64, 2048))\n",
    "        dummy_hidden = tf.zeros((1, self.config['units']))\n",
    "        dummy_cell = tf.zeros((1, self.config['units']))\n",
    "        dummy_token = tf.zeros((1, 1), dtype=tf.int32)\n",
    "\n",
    "        # --- Encoder Summary ---\n",
    "        print(\"\\nEncoder Summary:\")\n",
    "        self.encoder(dummy_image)\n",
    "        self.encoder.summary()\n",
    "\n",
    "        # --- Bahdanau Attention Summary ---\n",
    "        print(\"\\nBahdanau Attention Summary:\")\n",
    "        attention_layer = BahdanauAttention(self.config['units'])\n",
    "        features_input = tf.keras.Input(shape=(64, 2048), name=\"features\")\n",
    "        hidden_input = tf.keras.Input(shape=(self.config['units'],), name=\"hidden\")\n",
    "        context_vector, attn_weights = attention_layer(features_input, hidden_input)\n",
    "        attention_model = tf.keras.Model(inputs=[features_input, hidden_input], outputs=[context_vector, attn_weights])\n",
    "        attention_model.summary()\n",
    "\n",
    "        # --- Decoder Summary ---\n",
    "        print(\"\\nDecoder Summary:\")\n",
    "        self.decoder(dummy_token, dummy_features, dummy_hidden, dummy_cell)\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self,\n",
    "                   img_tensor: tf.Tensor,\n",
    "                   target:     tf.Tensor,\n",
    "                   cap_len:    tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Single training step with *doubly-stochastic* attention\n",
    "        regularisation from “Show, Attend and Tell”.\n",
    "        \"\"\"\n",
    "        if self.encoder is None or self.decoder is None:\n",
    "            raise ValueError(\"Models not built. Call build_model() first.\")\n",
    "\n",
    "        batch_size = tf.shape(img_tensor)[0]\n",
    "        hidden     = tf.zeros((batch_size, self.config['units']))\n",
    "        cell       = tf.zeros_like(hidden)\n",
    "\n",
    "        # <start> token\n",
    "        start_tok  = self.processor.tokenizer.word_index['<start>']\n",
    "        dec_input  = tf.expand_dims(tf.repeat(start_tok, batch_size), 1)\n",
    "\n",
    "        attention_accum = None   # to store Σ_t α_{t,i}\n",
    "        total_loss      = 0.0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)                    # (B, L, 2048)\n",
    "\n",
    "            # iterate over caption timesteps\n",
    "            for t in range(1, self.config['max_length']):\n",
    "                logits, hidden, cell, alpha = self.decoder(\n",
    "                    dec_input, features, hidden, cell)\n",
    "\n",
    "                # accumulate attention weights for regularisation\n",
    "                if attention_accum is None:\n",
    "                    attention_accum = alpha\n",
    "                else:\n",
    "                    attention_accum += alpha                       # element-wise Σ_t α_{t,i}\n",
    "\n",
    "                # standard X-entropy loss\n",
    "                loss_t = self.loss_fn(target[:, t],\n",
    "                                      tf.squeeze(logits, 1))       # (B,)\n",
    "                mask   = tf.cast(target[:, t] > 0, tf.float32)\n",
    "                total_loss += tf.reduce_sum(loss_t * mask)\n",
    "\n",
    "                # teacher forcing\n",
    "                dec_input = tf.expand_dims(target[:, t], 1)\n",
    "\n",
    "            # normalise by number of real tokens\n",
    "            total_tokens = tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "            ce_loss      = total_loss / total_tokens\n",
    "\n",
    "            # ---- doubly-stochastic attention regulariser (Eq. (14)) ----\n",
    "            lambda_reg   = self.config.get('attention_reg_lambda', 1.0)\n",
    "            # Σ_i (1 − Σ_t α_{t,i})²  averaged over batch\n",
    "            reg_loss = tf.reduce_mean(tf.square(1.0 - attention_accum))\n",
    "            loss     = ce_loss + lambda_reg * reg_loss\n",
    "\n",
    "            # mixed-precision safety\n",
    "            if self.config['mixed_precision']:\n",
    "                loss = tf.cast(loss, tf.float32)\n",
    "\n",
    "        # gradients & update\n",
    "        variables  = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads      = tape.gradient(loss, variables)\n",
    "\n",
    "        if self.config['mixed_precision']:\n",
    "            grads = [tf.cast(g, tf.float32) if g is not None else None\n",
    "                     for g in grads]\n",
    "\n",
    "        # clip to prevent exploding gradients\n",
    "        grads, _ = tf.clip_by_global_norm(grads, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def greedy_decode(self, image_path: str, return_attention=False):\n",
    "        \"\"\"Generate a caption for an image using greedy decoding.\"\"\"\n",
    "        # Convert Python string -> tf.Tensor to match load_image() signature\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image(tf.convert_to_tensor(image_path)),\n",
    "            0\n",
    "        )\n",
    "\n",
    "        features = self.encoder(img_tensor)\n",
    "        hidden  = tf.zeros((1, self.config['units']))\n",
    "        cell    = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims(\n",
    "            [self.processor.tokenizer.word_index['<start>']], 0\n",
    "        )\n",
    "\n",
    "        result, alphas = [], []\n",
    "\n",
    "        for _ in range(self.config['max_length']):\n",
    "            logits, hidden, cell, alpha = self.decoder(\n",
    "                dec_input, features, hidden, cell\n",
    "            )\n",
    "            pred_id = tf.argmax(logits[0, 0]).numpy()\n",
    "            word = self.processor.tokenizer.index_word.get(pred_id, '')\n",
    "\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            if word not in ('<start>', '<unk>'):\n",
    "                result.append(word)\n",
    "\n",
    "            alphas.append(alpha[0].numpy())\n",
    "            dec_input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "        return (result, alphas) if return_attention else result\n",
    "    \n",
    "    def evaluate_bleu(self, test_data, max_samples=None):\n",
    "        \"\"\"Calculate BLEU scores on test data.\"\"\"\n",
    "        refs, hyps = [], []\n",
    "        data_to_eval = test_data[:max_samples] if max_samples else test_data\n",
    "        \n",
    "        for img_name, _ in tqdm.tqdm(data_to_eval):\n",
    "            image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            hyp = self.greedy_decode(image_path)\n",
    "            \n",
    "            # Process ground truth captions\n",
    "            gt = [self.processor.preprocess_caption(c).split() for c in self.processor.captions_dict[img_name][:5]]\n",
    "            gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "            \n",
    "            refs.append(gt)\n",
    "            hyps.append(hyp)\n",
    "        \n",
    "        # Calculate BLEU scores for different n-grams\n",
    "        bleu_scores = {}\n",
    "        for i in range(1, 5):\n",
    "            weights = tuple([1.0/i]*i + [0.0]*(4-i))\n",
    "            score = corpus_bleu(refs, hyps, weights=weights, smoothing_function=self.smoothie)\n",
    "            bleu_scores[f'bleu-{i}'] = score\n",
    "            print(f\"BLEU-{i}: {score:.4f}\")\n",
    "        \n",
    "        return bleu_scores\n",
    "    \n",
    "    def train(self, train_ds, val_data, epochs=None):\n",
    "        \"\"\"Train the model with early stopping.\"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config['epochs']\n",
    "        \n",
    "        patience = self.config['patience']\n",
    "        wait = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            total_loss = 0.0\n",
    "            step = 0\n",
    "            \n",
    "            # Training loop\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            progbar = tf.keras.utils.Progbar(\n",
    "                target=None,\n",
    "                stateful_metrics=['loss']\n",
    "            )\n",
    "            \n",
    "            for batch, (img_tensor, target, cap_len) in enumerate(train_ds):\n",
    "                if batch == 0 and progbar.target is None:\n",
    "                    progbar.target = len(self.processor.train_data) // self.config['batch_size'] + 1\n",
    "                \n",
    "                batch_loss = self.train_step(img_tensor, target, cap_len)\n",
    "                total_loss += batch_loss\n",
    "                progbar.update(batch + 1, values=[('loss', batch_loss)])\n",
    "                step += 1\n",
    "            \n",
    "            # Average loss for the epoch\n",
    "            avg_loss = total_loss / step\n",
    "            self.train_loss_log.append(float(avg_loss))\n",
    "            \n",
    "            # Save checkpoint\n",
    "            self.ckpt_manager.save()\n",
    "            \n",
    "            # Validation on a subset for speed\n",
    "            print(\"Evaluating on validation subset...\")\n",
    "            validation_subset = val_data[:100]\n",
    "            bleu_scores = self.evaluate_bleu(validation_subset)\n",
    "            bleu4 = bleu_scores['bleu-4']\n",
    "            self.val_bleu_log.append(bleu4)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if bleu4 > self.best_bleu:\n",
    "                self.best_bleu = bleu4\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    break\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, BLEU-4 = {bleu4:.4f}, Time = {time.time()-start:.2f}s\", flush=True)\n",
    "        \n",
    "        return self.train_loss_log, self.val_bleu_log\n",
    "    \n",
    "    def plot_attention(self, image_path: str, caption: list, alphas: list):\n",
    "        \"\"\"Visualize attention weights overlaid on the source image.\"\"\"\n",
    "        img = np.array(Image.open(image_path).resize((224, 224)))\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        for t in range(len(caption)):\n",
    "            ax = fig.add_subplot(3, int(np.ceil(len(caption)/3)), t+1)\n",
    "            ax.set_title(caption[t])\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_shape = int(np.sqrt(alpha.size))\n",
    "            alpha = alpha.reshape(attention_shape, attention_shape)\n",
    "            ax.imshow(alpha, cmap='viridis', alpha=0.6, extent=(0, 224, 224, 0))\n",
    "            ax.axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot training history.\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_log, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_bleu_log, label='Val BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4')\n",
    "        plt.title('Validation BLEU-4')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Generate speech audio from caption text.\"\"\"\n",
    "        if not caption:\n",
    "            print(\"Empty caption, nothing to speak\")\n",
    "            return\n",
    "            \n",
    "        tts = gTTS(text=caption, lang='en')\n",
    "        tts.save(filename)\n",
    "        display(Audio(filename))\n",
    "        print(f\"Audio saved to {filename}\")\n",
    "    \n",
    "    def demo(self, image_path, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Run a full demonstration of the model.\"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Generating caption for: {image_path}\")\n",
    "        \n",
    "        # Display the image\n",
    "        img = Image.open(image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate caption with attention\n",
    "        words, attention = self.greedy_decode(image_path, return_attention=True)\n",
    "        caption = \" \".join(words)\n",
    "        print(f\"Generated caption: {caption}\")\n",
    "        \n",
    "        # Plot attention\n",
    "        self.plot_attention(image_path, words, attention)\n",
    "        \n",
    "        # Generate speech\n",
    "        self.speak_caption(caption, filename=filename)\n",
    "        \n",
    "        return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize processor and load data\n",
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.load_captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.display_samples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.prepare_captions(subset_ratio=CONFIG['subset_ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds, val_ds, _ = processor.prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train model\n",
    "model = ImageCaptioningModel(CONFIG, processor)\n",
    "model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to train the model\n",
    "model.train(train_ds, processor.val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set:\")\n",
    "model.evaluate_bleu(processor.test_data[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo with a sample image\n",
    "sample_img = os.path.join(CONFIG['image_dir'], processor.test_data[0][0])\n",
    "model.demo(sample_img, filename='caption_audio01.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 34\n",
    "sample_pair = random.choice(processor.test_data)\n",
    "sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "model.demo(sample_img, filename='caption_audio02.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor.display_samples(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- 1-image over-fit check ---------------------------------\n",
    "one_pair   = [processor.train_data[0]]          # (img, caption)\n",
    "one_ds     = processor.build_dataset(one_pair,\n",
    "                                     shuffle=False, cache=False\n",
    "                                    ).repeat()  # infinite\n",
    "\n",
    "OVF_CFG = CONFIG.copy()\n",
    "OVF_CFG.update({\n",
    "    'epochs'       : 1,       # we’ll drive the loop ourselves\n",
    "    'batch_size'   : 1,\n",
    "    'checkpoint_dir': './checkpoints/onefit'\n",
    "})\n",
    "\n",
    "one_model = ImageCaptioningModel(OVF_CFG, processor)\n",
    "one_model.build_model()\n",
    "\n",
    "def show_image(path, title=''):\n",
    "    img = Image.open(path)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    if title: plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "img_path = os.path.join(OVF_CFG['image_dir'], one_pair[0][0])\n",
    "show_image(img_path, 'Single-image over-fit target')\n",
    "\n",
    "# run ~1 000 gradient steps\n",
    "steps = 1000\n",
    "for step, (img_t, tgt, cap_len) in zip(range(steps), one_ds):\n",
    "    loss = one_model.train_step(img_t, tgt, cap_len)\n",
    "    if (step+1) % 100 == 0:\n",
    "        print(f\"step {step+1}: loss={loss.numpy():.3f}\")\n",
    "        print(\"→\", \" \".join(one_model.greedy_decode(img_path)))\n",
    "\n",
    "print(\"\\nFinal caption:\")\n",
    "print(\" \".join(one_model.greedy_decode(img_path)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
