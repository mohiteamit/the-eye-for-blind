{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye for Blind â€“ Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "Eye for Blind: An Assistive Image Captioning System with Visual Attention\n",
    "\n",
    "This project implements a deep learning model that generates natural language descriptions of images, particularly aimed at visually impaired users. The model leverages an attention mechanism to selectively focus on image regions when generating each word, mimicking human vision.\n",
    "\n",
    "Inspired by \"Show, Attend and Tell\" (Xu et al., 2015), this implementation:\n",
    "1. Uses a CNN encoder (InceptionV3) to extract image features.\n",
    "2. Applies additive (Bahdanau) attention during decoding.\n",
    "3. Employs a decoder LSTM to generate captions.\n",
    "4. Converts generated captions to speech using gTTS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = 'data/flickr30k_images'                            # directory containing images\n",
    "CAPTION_FILE = 'data/flickr30k_images/results.csv'             # pipe-separated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Enable memory growth for GPUs\n",
    "physical_gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in physical_gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "def load_captions(path: str) -> dict:\n",
    "    \"\"\"Load and convert pipe-delimited Flickr-style caption file to a dict.\"\"\"\n",
    "    df = pd.read_csv(path, sep='\\|', header=None, names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "    df['image_name'] = df['image_name'].str.strip()\n",
    "    df['comment'] = df['comment'].str.strip()\n",
    "    caption_map = {}\n",
    "    for img, group in df.groupby('image_name'):\n",
    "        caption_map[img] = group['comment'].tolist()\n",
    "    return caption_map\n",
    "\n",
    "captions_dict = load_captions(CAPTION_FILE)\n",
    "\n",
    "def display_samples(image_dir: str, captions: dict, num_samples: int = 3):\n",
    "    sample_keys = random.sample(list(captions.keys()), num_samples)\n",
    "    for key in sample_keys:\n",
    "        img_path = os.path.join(image_dir, key)\n",
    "        img = Image.open(img_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(key)\n",
    "        plt.show()\n",
    "        for cap in captions[key][:5]:\n",
    "            print(f\"- {cap}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(IMAGE_DIR, captions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "MAX_LEN = 30\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "\n",
    "def preprocess_caption(caption: str) -> str:\n",
    "    if caption is None or not isinstance(caption, str):\n",
    "        return None\n",
    "    caption = caption.lower()\n",
    "    caption = re.sub(r\"[^a-z0-9 ]\", \"\", caption)\n",
    "    return f\"<start> {caption.strip()} <end>\"\n",
    "\n",
    "# Prepare captions and vocabulary\n",
    "all_captions = []\n",
    "for caps in captions_dict.values():\n",
    "    for c in caps:\n",
    "        p = preprocess_caption(c)\n",
    "        if p:\n",
    "            all_captions.append(p)\n",
    "\n",
    "word_counts = Counter(word for cap in all_captions for word in cap.split())\n",
    "\n",
    "def keep_caption(caption: str) -> bool:\n",
    "    words = caption.split()\n",
    "    for w in words:\n",
    "        if w in ('<start>', '<end>'):\n",
    "            continue\n",
    "        if word_counts.get(w, 0) < 5:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "filtered_captions = [c for c in all_captions if keep_caption(c)]\n",
    "\n",
    "tokenizer = Tokenizer(oov_token=\"<unk>\")\n",
    "tokenizer.fit_on_texts(filtered_captions)\n",
    "# Ensure special tokens\n",
    "for token in ['<start>', '<end>']:\n",
    "    if token not in tokenizer.word_index:\n",
    "        tokenizer.word_index[token] = len(tokenizer.word_index) + 1\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "def encode_caption(caption: str):\n",
    "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "    return pad_sequences([seq], maxlen=MAX_LEN, padding='post')[0], len(seq)\n",
    "\n",
    "def load_image(path: str) -> tf.Tensor:\n",
    "    \"\"\"Load and preprocess an image with aspect-preserving resize and padding.\"\"\"\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
    "    img = tf.image.resize_with_pad(img, 299, 299)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "# Create train/val/test splits\n",
    "image_caption_pairs = []\n",
    "for img, caps in captions_dict.items():\n",
    "    for c in caps:\n",
    "        p = preprocess_caption(c)\n",
    "        if p and keep_caption(p):\n",
    "            image_caption_pairs.append((img, p))\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(image_caption_pairs)\n",
    "num_total = len(image_caption_pairs)\n",
    "train_split = int(0.8 * num_total)\n",
    "val_split = int(0.9 * num_total)\n",
    "train_data = image_caption_pairs[:train_split]\n",
    "val_data = image_caption_pairs[train_split:val_split]\n",
    "test_data = image_caption_pairs[val_split:]\n",
    "\n",
    "def data_generator(data):\n",
    "    for img, cap in data:\n",
    "        img_tensor = load_image(os.path.join(IMAGE_DIR, img))\n",
    "        token_ids, cap_len = encode_caption(cap)\n",
    "        yield img_tensor, token_ids, cap_len\n",
    "\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(299, 299, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(MAX_LEN,), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    ")\n",
    "\n",
    "def build_tf_dataset(data):\n",
    "    return (tf.data.Dataset\n",
    "            .from_generator(lambda: data_generator(data), output_signature=output_signature)\n",
    "            .shuffle(1024)\n",
    "            .padded_batch(BATCH_SIZE)\n",
    "            .prefetch(AUTOTUNE))\n",
    "\n",
    "# train_ds = build_tf_dataset(train_data)\n",
    "# val_ds = build_tf_dataset(val_data)\n",
    "\n",
    "train_ds = build_tf_dataset(train_data[:256])\n",
    "val_ds = build_tf_dataset(val_data[:64])\n",
    "test_data = test_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        base.trainable = False\n",
    "        self.cnn = Model(inputs=base.input, outputs=base.get_layer('mixed10').output)\n",
    "        self.reshape = layers.Reshape((-1, 2048))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.cnn(x)\n",
    "        return self.reshape(x)\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.fc = layers.Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "\n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context, attn = self.attention(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context, 1), x], axis=-1)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=[hidden, cell])\n",
    "        logits = self.fc(output)\n",
    "        return logits, state_h, state_c, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "def decode_tokens(tokens):\n",
    "    words = [tokenizer.index_word.get(idx, '') for idx in tokens if idx != 0]\n",
    "    return ' '.join([w for w in words if w not in ('<start>', '<end>')])\n",
    "\n",
    "def greedy_decode(image_path: str, return_attention=False):\n",
    "    img_tensor = tf.expand_dims(load_image(image_path), 0)\n",
    "    features = encoder(img_tensor)\n",
    "    hidden = tf.zeros((1, 512))\n",
    "    cell = tf.zeros_like(hidden)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    result, alphas = [], []\n",
    "    for _ in range(MAX_LEN):\n",
    "        logits, hidden, cell, alpha = decoder(dec_input, features, hidden, cell)\n",
    "        predicted_id = tf.argmax(logits[0,0]).numpy()\n",
    "        word = tokenizer.index_word.get(predicted_id, '')\n",
    "        if word == '<end>': break\n",
    "        result.append(word)\n",
    "        alphas.append(alpha[0].numpy())\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return (result, alphas) if return_attention else result\n",
    "\n",
    "def evaluate_bleu(test_data):\n",
    "    refs, hyps = [], []\n",
    "    for img_name, caps in tqdm.tqdm(test_data):\n",
    "        image_path = os.path.join(IMAGE_DIR, img_name)\n",
    "        hyp = greedy_decode(image_path)\n",
    "        gt = [preprocess_caption(c).split() for c in captions_dict[img_name][:5]]\n",
    "        gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "        refs.append(gt)\n",
    "        hyps.append(hyp)\n",
    "    for i in range(1,5):\n",
    "        weights = tuple([1.0/i]*i + [0.0]*(4-i))\n",
    "        score = corpus_bleu(refs, hyps, weights=weights, smoothing_function=smoothie)\n",
    "        print(f\"BLEU-{i}: {score:.4f}\")\n",
    "\n",
    "def plot_attention(image_path: str, caption: list, alphas: list):\n",
    "    img = np.array(Image.open(image_path).resize((224,224)))\n",
    "    fig = plt.figure(figsize=(15,8))\n",
    "    for t in range(len(caption)):\n",
    "        ax = fig.add_subplot(3, int(np.ceil(len(caption)/3)), t+1)\n",
    "        ax.set_title(caption[t])\n",
    "        ax.imshow(img)\n",
    "        alpha = np.array(alphas[t]).reshape(8,8)\n",
    "        ax.imshow(alpha, cmap='viridis', alpha=0.6, extent=(0,224,224,0))\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def speak_caption(caption: str, filename=\"tts_output.mp3\"):\n",
    "    tts = gTTS(text=caption, lang='en')\n",
    "    tts.save(filename)\n",
    "    display(Audio(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "encoder = Encoder()\n",
    "decoder = Decoder(embedding_dim=512, units=512, vocab_size=vocab_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=CosineDecay(1e-3, decay_steps=10000))\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "ckpt_dir = './checkpoints'\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_mgr = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=3)\n",
    "\n",
    "train_loss_log = []\n",
    "val_bleu_log = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(img_tensor, target, cap_len):\n",
    "    loss = 0.0\n",
    "    hidden = tf.zeros((img_tensor.shape[0], 512))\n",
    "    cell = tf.zeros_like(hidden)\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * img_tensor.shape[0], 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "        for t in range(1, MAX_LEN):\n",
    "            logits, hidden, cell, _ = decoder(dec_input, features, hidden, cell)\n",
    "            loss_ = loss_fn(target[:, t], tf.squeeze(logits, 1))\n",
    "            mask = tf.cast(target[:, t] > 0, tf.float32)\n",
    "            loss += tf.reduce_sum(loss_ * mask)\n",
    "            dec_input = tf.expand_dims(target[:, t], 1)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    grads, _ = tf.clip_by_global_norm(grads, 5.0)\n",
    "    optimizer.apply_gradients(zip(grads, variables))\n",
    "    return loss / tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "\n",
    "EPOCHS = 10\n",
    "patience = 3\n",
    "best_bleu = 0\n",
    "wait = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0.0\n",
    "    for img_tensor, target, cap_len in train_ds:\n",
    "        total_loss += train_step(img_tensor, target, cap_len)\n",
    "    train_loss_log.append(float(total_loss))\n",
    "    ckpt_mgr.save()\n",
    "\n",
    "    # Validation\n",
    "    refs, hyps = [], []\n",
    "    for img_name, caps in val_data[:100]:\n",
    "        image_path = os.path.join(IMAGE_DIR, img_name)\n",
    "        hyp = greedy_decode(image_path)\n",
    "        gt = [preprocess_caption(c).split() for c in captions_dict[img_name][:5]]\n",
    "        gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "        refs.append(gt)\n",
    "        hyps.append(hyp)\n",
    "    bleu4 = corpus_bleu(refs, hyps, smoothing_function=smoothie)\n",
    "    val_bleu_log.append(bleu4)\n",
    "\n",
    "    if bleu4 > best_bleu:\n",
    "        best_bleu = bleu4\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, BLEU-4: {bleu4:.4f}, Time: {time.time()-start:.2f}s\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final evaluation on full test set:\")\n",
    "evaluate_bleu(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and BLEU\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_loss_log, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(val_bleu_log, label='Val BLEU-4')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BLEU-4')\n",
    "plt.title('Validation BLEU-4')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Inference & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = os.path.join(IMAGE_DIR, test_data[0][0])\n",
    "result_words, attention = greedy_decode(sample_img, return_attention=True)\n",
    "print(\"Generated caption:\", \" \".join(result_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_text = \" \".join(result_words)  # assuming result_words from greedy_decode\n",
    "\n",
    "# Convert to speech and save\n",
    "tts = gTTS(text=caption_text, lang='en')\n",
    "tts.save(\"caption_audio.mp3\")\n",
    "\n",
    "# Display inline audio player in Jupyter\n",
    "display(Audio(\"caption_audio.mp3\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
