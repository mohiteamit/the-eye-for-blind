{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eye for Blind â€“ Image Captioning with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Objective\n",
    "\n",
    "Eye for Blind: An Assistive Image Captioning System with Visual Attention\n",
    "\n",
    "This project implements a deep learning model that generates natural language descriptions of images, particularly aimed at visually impaired users. The model leverages an attention mechanism to selectively focus on image regions when generating each word, mimicking human vision.\n",
    "\n",
    "Inspired by \"Show, Attend and Tell\" (Xu et al., 2015), this implementation:\n",
    "1. Uses a CNN encoder (InceptionV3) to extract image features.\n",
    "2. Applies additive (Bahdanau) attention during decoding.\n",
    "3. Employs a decoder LSTM to generate captions.\n",
    "4. Converts generated captions to speech using gTTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# 0 (default): All messages (INFO, WARNING, ERROR) are logged.\n",
    "# 1: INFO messages are not printed.\n",
    "# 2: INFO and WARNING messages are not printed.\n",
    "# 3: INFO, WARNING, and ERROR messages are not printed.\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf #type: ignore\n",
    "from tensorflow.keras import layers, Model #type: ignore\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay #type: ignore\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy #type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #type: ignore\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction #type: ignore\n",
    "from gtts import gTTS #type: ignore\n",
    "from IPython.display import Audio, display\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'subset_ratio' : 1.0,\n",
    "    'image_dir': '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file': '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    \n",
    "    # GPU Utilization\n",
    "    'batch_size': 128,           # Fully utilize 48GB VRAM; reduce if OOM\n",
    "    'buffer_size': 10000,        # Larger shuffle buffer helps training stability\n",
    "    \n",
    "    # Model Capacity\n",
    "    'max_length': 30,            # Reasonable for captions\n",
    "    'embedding_dim': 512,        # Good for attention + LSTM\n",
    "    'units': 512,                # LSTM/Attention size\n",
    "    \n",
    "    # Training Behavior\n",
    "    'seed': 42,\n",
    "    'epochs': 20,                # Slightly more for small dataset\n",
    "    'patience': 8,               # Early stopping tolerance\n",
    "    'learning_rate': 3e-4,       # Lower for small datasets to reduce overfitting\n",
    "    'grad_clip_value': 5.0,      # Prevent exploding gradients\n",
    "    'scheduled_sampling_max_prob' : 0.25,    # final Îµ\n",
    "    \n",
    "    # Vocabulary\n",
    "    'vocab_min_count': 3,        # Include more words for small run\n",
    "    \n",
    "    # Output & Precision\n",
    "    'checkpoint_dir': './checkpoints/10pct',\n",
    "    'mixed_precision': False,     # RTX 6000 Ada has 4th-gen Tensor Coresâ€”use them\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "\n",
    "# Mixed precision policy - RTX 6000 Ada has excellent mixed precision support\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"Mixed precision enabled for RTX 6000 Ada\")\n",
    "\n",
    "# Single GPU setup\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    # Enable memory growth for RTX 6000 Ada\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    \n",
    "    # Use default strategy for single GPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Using single GPU: {physical_devices[0].name}, batch size={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"No GPUs found, using CPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "# Constants\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.captions_dict = dict()\n",
    "        self.tokenizer = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_data = []\n",
    "        self.val_data = []\n",
    "        self.test_data = []\n",
    "    \n",
    "    def load_captions(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load and convert pipe-delimited Flickr-style caption file to a dict.\"\"\"\n",
    "        print(f\"Loading captions from {self.config['caption_file']}\")\n",
    "        df = pd.read_csv(self.config['caption_file'], sep='|', header=None, \n",
    "                         names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "        \n",
    "        caption_map = {}\n",
    "        for img, group in df.groupby('image_name'):\n",
    "            caption_map[img] = group['comment'].tolist()\n",
    "        \n",
    "        self.captions_dict = caption_map\n",
    "        print(f\"Loaded {len(caption_map)} images with captions\")\n",
    "        return caption_map\n",
    "    \n",
    "    def display_samples(self, num_samples: int = 3):\n",
    "        \"\"\"Display random images with all their associated captions.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        sample_keys = random.sample(list(self.captions_dict.keys()), min(num_samples, len(self.captions_dict)))\n",
    "\n",
    "        for key in sample_keys:\n",
    "            img_path = os.path.join(self.config['image_dir'], key)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(key)\n",
    "                plt.show()\n",
    "\n",
    "                for cap in self.captions_dict[key]:\n",
    "                    print(f\"- {cap}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {key}: {e}\")\n",
    "\n",
    "    def preprocess_caption(self, caption: str) -> Optional[str]:\n",
    "        \"\"\"Clean and format caption text.\"\"\"\n",
    "        if caption is None or not isinstance(caption, str):\n",
    "            return None\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z0-9.,? ]\", \"\", caption)\n",
    "        return f\"<start> {caption.strip()} <end>\"\n",
    "\n",
    "    def prepare_captions(self, subset_ratio=1.0):\n",
    "        \"\"\"Process captions, build tokenizer & train/val/test splits.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        # --- 1. clean & tag ----------------------------------------------------\n",
    "        all_captions = []\n",
    "        for caps in self.captions_dict.values():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p:\n",
    "                    all_captions.append(p)\n",
    "\n",
    "        word_counts = Counter(w for cap in all_captions for w in cap.split())\n",
    "        valid_words = {w for w, cnt in word_counts.items()\n",
    "                    if cnt >= self.config['vocab_min_count']}\n",
    "\n",
    "        def keep(c):\n",
    "            return all(w in valid_words or w in ('<start>', '<end>') for w in c.split())\n",
    "\n",
    "        filtered = [c for c in all_captions if keep(c)]\n",
    "\n",
    "        # --- 2. determine max length ------------------------------------------\n",
    "        lengths = [len(c.split()) for c in filtered]\n",
    "        self.config['max_length'] = int(np.percentile(lengths, 95))\n",
    "        print(f\"max_length set to {self.config['max_length']}\")\n",
    "\n",
    "        # --- 3. build tokenizer (NO filters so < and > stay) -------------------\n",
    "        tokenizer = Tokenizer(oov_token=\"<unk>\", filters='', lower=True)\n",
    "        tokenizer.fit_on_texts(filtered)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        print(f\"vocab size = {self.vocab_size}\")\n",
    "\n",
    "        # --- 4. build (image, caption) list ------------------------------------\n",
    "        pairs = []\n",
    "        for img, caps in self.captions_dict.items():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p and keep(p):\n",
    "                    pairs.append((img, p))\n",
    "\n",
    "        if subset_ratio < 1.0:\n",
    "            pairs = pairs[:int(len(pairs) * subset_ratio)]\n",
    "            print(f\"subset: {len(pairs)} pairs\")\n",
    "\n",
    "        random.shuffle(pairs)\n",
    "        n = len(pairs)\n",
    "        self.train_data, self.val_data, self.test_data = (\n",
    "            pairs[:int(0.8*n)],\n",
    "            pairs[int(0.8*n):int(0.9*n)],\n",
    "            pairs[int(0.9*n):],\n",
    "        )\n",
    "        print(f\"split  â†’  train {len(self.train_data)} | val {len(self.val_data)} | test {len(self.test_data)}\")\n",
    "\n",
    "        return filtered\n",
    "        \n",
    "    def encode_caption(self, caption: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Convert caption text to sequence of token ids.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call prepare_captions first.\")\n",
    "        \n",
    "        seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=self.config['max_length'], padding='post')[0]\n",
    "        return padded_seq, len(seq)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def _base_decode(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)          # [0,1]\n",
    "        return img                                                   # (h,w,3)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image_train(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Augment + preprocess (training only).\"\"\"\n",
    "        img = self._base_decode(path)\n",
    "        img = tf.image.random_flip_left_right(img)                   # aug â‘ \n",
    "\n",
    "        # resize shorter sideâ†’342 then *random* crop 299Ã—299\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = 342. / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.random_crop(img, size=[299, 299, 3])          # aug â‘¡\n",
    "\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [299, 299, 3])\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image_eval(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Deterministic centre-crop (validation / inference).\"\"\"\n",
    "        img = self._base_decode(path)\n",
    "\n",
    "        # resize shorter sideâ†’342 then *central* crop 299Ã—299\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = 342. / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.resize_with_crop_or_pad(img, 299, 299)\n",
    "\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [299, 299, 3])\n",
    "\n",
    "    def data_generator(self, data):\n",
    "        \"\"\"Generator for *training* / *val* datasets.\"\"\"\n",
    "        for img, cap in data:\n",
    "            img_path = os.path.join(self.config['image_dir'], img)\n",
    "            img_tensor = self.load_image_train(tf.convert_to_tensor(img_path))\n",
    "            token_ids, cap_len = self.encode_caption(cap)\n",
    "            yield img_tensor, token_ids, cap_len\n",
    "    \n",
    "    def build_dataset(self, data, shuffle=True, cache=True, training: bool=True):\n",
    "        \"\"\"Create a tf.data.Dataset optimized for single GPU.\"\"\"\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((299, 299, 3), tf.float32),\n",
    "            tf.TensorSpec((self.config['max_length'],), tf.int32),\n",
    "            tf.TensorSpec((), tf.int32)\n",
    "        )\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: self.data_generator(data),\n",
    "            output_signature=output_signature\n",
    "        )\n",
    "\n",
    "        if cache:\n",
    "            ds = ds.cache()\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(self.config['buffer_size'])\n",
    "\n",
    "        ds = ds.batch(self.config['batch_size'])\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"Prepare all datasets for training/validation/testing.\"\"\"\n",
    "        if not self.train_data:\n",
    "            self.prepare_captions()\n",
    "\n",
    "        print(\"Building datasets...\")\n",
    "        train_ds = self.build_dataset(self.train_data)\n",
    "        val_ds = self.build_dataset(self.val_data)\n",
    "        test_ds = self.build_dataset(self.test_data, shuffle=False)\n",
    "        \n",
    "        return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    \"\"\"\n",
    "    Inception-V3 feature extractor with an optional\n",
    "    `unfreeze_top_layers()` helper for later fine-tuning.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"encoder\")\n",
    "        base = tf.keras.applications.InceptionV3(\n",
    "            include_top=False, weights='imagenet',\n",
    "            input_shape=(299, 299, 3))\n",
    "        base.trainable = False                                      # phase-1: frozen\n",
    "        self.cnn = Model(inputs=base.input, outputs=base.get_layer('mixed10').output)\n",
    "        self.reshape = layers.Reshape((-1, 2048))                  # L=64 for 8Ã—8 grid\n",
    "\n",
    "    def unfreeze_top_layers(self, n: int = 2):\n",
    "        \"\"\"\n",
    "        Fine-tune: unfreeze the last *n* Inception blocks (default: mixed9 & mixed10).\n",
    "        Call **after** initial caption training for best accuracy.\n",
    "        \"\"\"\n",
    "        for layer in self.cnn.layers[-n:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    def call(self, x):                                             # (B,299,299,3)\n",
    "        x = self.cnn(x)                                            # (B,8,8,2048)\n",
    "        return self.reshape(x)                                     # (B,64,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "    \n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = tf.reduce_sum(attention_weights * features, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    \"\"\"\n",
    "    Attention decoder with:\n",
    "      â€¢ Î²-gate\n",
    "      â€¢ **max-out** deep-output layer  (improves accuracy a bit)\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim: int, units: int, vocab_size: int):\n",
    "        super().__init__(name=\"decoder\")\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.f_beta    = layers.Dense(1, activation=\"sigmoid\")          # Î²â‚œ\n",
    "        self.lstm      = layers.LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.dropout   = layers.Dropout(0.3)\n",
    "\n",
    "        # max-out projection: 2 Ã— units â†’ reduce_max â†’ units\n",
    "        self.deep_proj = layers.Dense(units * 2)                        # W_o\n",
    "        self.fc        = layers.Dense(vocab_size)                       # final soft-max\n",
    "\n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context, alpha = self.attention(features, hidden)               # (B,2048)\n",
    "        context = self.f_beta(hidden) * context                         # gated\n",
    "\n",
    "        x = self.embedding(x)                                           # (B,1,E)\n",
    "        lstm_input = tf.concat([tf.expand_dims(context, 1), x], -1)     # (B,1,2048+E)\n",
    "\n",
    "        hidden = tf.cast(hidden, lstm_input.dtype)\n",
    "        cell   = tf.cast(cell,   lstm_input.dtype)\n",
    "\n",
    "        lstm_out, h_t, c_t = self.lstm(lstm_input, initial_state=[hidden, cell])\n",
    "        lstm_out = tf.squeeze(lstm_out, 1)                               # (B,units)\n",
    "\n",
    "        # ---------- max-out deep-output layer ----------\n",
    "        proj = self.deep_proj(tf.concat([lstm_out, context], -1))        # (B,2*units)\n",
    "        proj = tf.reshape(proj, (-1, self.units, 2))                     # (B,units,2)\n",
    "        maxout = tf.reduce_max(proj, axis=-1)                            # (B,units)\n",
    "        maxout = self.dropout(maxout)\n",
    "\n",
    "        logits = self.fc(maxout)                                         # (B,vocab)\n",
    "        return tf.expand_dims(logits, 1), h_t, c_t, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    def __init__(self, config, processor):\n",
    "        self.config          = config\n",
    "        self.processor       = processor\n",
    "        self.encoder         = None\n",
    "        self.decoder         = None\n",
    "        self.optimizer       = None\n",
    "        self.loss_fn         = None\n",
    "        self.ckpt_manager    = None\n",
    "\n",
    "        self.best_bleu       = 0.0\n",
    "        self.train_loss_log  = []\n",
    "        self.train_bleu_log  = []\n",
    "        self.val_bleu_log    = []\n",
    "        self.bleu_subset_idx = None  \n",
    "\n",
    "        self.smoothie = SmoothingFunction().method4\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build model for single GPU - no distribution strategy needed.\"\"\"\n",
    "        print(\"Building model for single GPU...\")\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim=self.config['embedding_dim'], \n",
    "            units=self.config['units'], \n",
    "            vocab_size=self.processor.vocab_size\n",
    "        )\n",
    "        \n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=self.config['learning_rate'],\n",
    "            decay_steps=10000\n",
    "        )\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "        self.loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "        \n",
    "        # Set up checkpointing\n",
    "        ckpt_dir = self.config['checkpoint_dir']\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            encoder=self.encoder, \n",
    "            decoder=self.decoder, \n",
    "            optimizer=self.optimizer\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(ckpt, ckpt_dir, max_to_keep=3)\n",
    "        \n",
    "        # Try to restore the latest checkpoint\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Restored from checkpoint: {self.ckpt_manager.latest_checkpoint}\")\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print model summaries for Encoder, Attention, and Decoder.\"\"\"\n",
    "        print(\"Building model summaries...\")\n",
    "\n",
    "        # Dummy inputs\n",
    "        dummy_image = tf.random.uniform((1, 299, 299, 3))\n",
    "        dummy_features = tf.random.uniform((1, 64, 2048))\n",
    "        dummy_hidden = tf.zeros((1, self.config['units']))\n",
    "        dummy_cell = tf.zeros((1, self.config['units']))\n",
    "        dummy_token = tf.zeros((1, 1), dtype=tf.int32)\n",
    "\n",
    "        # --- Encoder Summary ---\n",
    "        print(\"\\nEncoder Summary:\")\n",
    "        self.encoder(dummy_image)\n",
    "        self.encoder.summary()\n",
    "\n",
    "        # --- Bahdanau Attention Summary ---\n",
    "        print(\"\\nBahdanau Attention Summary:\")\n",
    "        attention_layer = BahdanauAttention(self.config['units'])\n",
    "        features_input = tf.keras.Input(shape=(64, 2048), name=\"features\")\n",
    "        hidden_input = tf.keras.Input(shape=(self.config['units'],), name=\"hidden\")\n",
    "        context_vector, attn_weights = attention_layer(features_input, hidden_input)\n",
    "        attention_model = tf.keras.Model(inputs=[features_input, hidden_input], outputs=[context_vector, attn_weights])\n",
    "        attention_model.summary()\n",
    "\n",
    "        # --- Decoder Summary ---\n",
    "        print(\"\\nDecoder Summary:\")\n",
    "        self.decoder(dummy_token, dummy_features, dummy_hidden, dummy_cell)\n",
    "        self.decoder.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self,\n",
    "                   img_tensor: tf.Tensor,\n",
    "                   target:     tf.Tensor,\n",
    "                   cap_len:    tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Single step with:\n",
    "        â€¢ Î²-gated attention + doubly-stochastic regulariser   (already present)\n",
    "        â€¢ **Scheduled sampling** controlled by `self.ss_prob`.\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(img_tensor)[0]\n",
    "        hidden     = tf.zeros((batch_size, self.config['units']))\n",
    "        cell       = tf.zeros_like(hidden)\n",
    "\n",
    "        start_tok  = self.processor.tokenizer.word_index['<start>']\n",
    "        dec_input  = tf.expand_dims(tf.repeat(start_tok, batch_size), 1)\n",
    "\n",
    "        attention_accum = None\n",
    "        total_ce_loss   = 0.0\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)  # (B, L, 2048)\n",
    "\n",
    "            for t in tf.range(1, self.config['max_length']):\n",
    "                logits, hidden, cell, alpha = self.decoder(\n",
    "                    dec_input, features, hidden, cell)\n",
    "\n",
    "                # accumulate Î± for doubly-stochastic term\n",
    "                attention_accum = (alpha if attention_accum is None\n",
    "                                   else attention_accum + alpha)\n",
    "\n",
    "                # CE loss\n",
    "                ce_t  = self.loss_fn(target[:, t], tf.squeeze(logits, 1))\n",
    "                mask  = tf.cast(target[:, t] > 0, tf.float32)\n",
    "                total_ce_loss += tf.reduce_sum(ce_t * mask)\n",
    "\n",
    "                # ---- scheduled sampling decision ----\n",
    "                # predicted tokens\n",
    "                pred_ids = tf.argmax(logits, -1, output_type=tf.int32)  # (B,1) â†’ (B,)\n",
    "                pred_ids = tf.squeeze(pred_ids, -1)\n",
    "\n",
    "                # Bernoulli mask: 1 â†’ use prediction\n",
    "                ss_mask = tf.random.uniform((batch_size,)) < self.ss_prob\n",
    "                next_ids = tf.where(ss_mask, pred_ids, target[:, t])\n",
    "\n",
    "                dec_input = tf.expand_dims(next_ids, 1)\n",
    "\n",
    "            # normalise CE by real tokens\n",
    "            total_tokens = tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "            ce_loss      = total_ce_loss / total_tokens\n",
    "\n",
    "            # doubly-stochastic regulariser\n",
    "            lambda_reg = self.config.get('attention_reg_lambda', 1.0)\n",
    "            reg_loss   = tf.reduce_mean(tf.square(1.0 - attention_accum))\n",
    "            loss       = ce_loss + lambda_reg * reg_loss\n",
    "\n",
    "            if self.config['mixed_precision']:\n",
    "                loss = tf.cast(loss, tf.float32)\n",
    "\n",
    "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads     = tape.gradient(loss, variables)\n",
    "\n",
    "        if self.config['mixed_precision']:\n",
    "            grads = [tf.cast(g, tf.float32) if g is not None else None\n",
    "                     for g in grads]\n",
    "\n",
    "        grads, _ = tf.clip_by_global_norm(grads, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def beam_search_decode(self,\n",
    "                           image_path: str,\n",
    "                           beam_size: int = 5,\n",
    "                           length_penalty: float = 0.7,\n",
    "                           return_attention: bool = False):\n",
    "        \"\"\"Beam-search with deterministic crop.\"\"\"\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0\n",
    "        )\n",
    "        base_features = self.encoder(img_tensor)       # (1,L,2048)\n",
    "\n",
    "        start_id = self.processor.tokenizer.word_index['<start>']\n",
    "        end_id   = self.processor.tokenizer.word_index['<end>']\n",
    "\n",
    "        beams = [{'seq':[start_id],\n",
    "                  'score':0.0,\n",
    "                  'hidden':tf.zeros((1,self.config['units'])),\n",
    "                  'cell':tf.zeros((1,self.config['units'])),\n",
    "                  'alphas':[]}]\n",
    "\n",
    "        completed = []\n",
    "        for _ in range(self.config['max_length']):\n",
    "            candidates = []\n",
    "            for b in beams:\n",
    "                last_id = b['seq'][-1]\n",
    "                if last_id == end_id:\n",
    "                    completed.append(b); continue\n",
    "                dec_in = tf.expand_dims([last_id], 0)\n",
    "                logits, h, c, alpha = self.decoder(dec_in, base_features,\n",
    "                                                   b['hidden'], b['cell'])\n",
    "                log_probs = tf.nn.log_softmax(logits[0,0])\n",
    "                top_ids = tf.math.top_k(log_probs, k=beam_size).indices.numpy()\n",
    "                for tok in top_ids:\n",
    "                    tok = int(tok)\n",
    "                    candidates.append({\n",
    "                        'seq':   b['seq']+[tok],\n",
    "                        'score': b['score']+float(log_probs[tok]),\n",
    "                        'hidden':h,\n",
    "                        'cell':  c,\n",
    "                        'alphas':b['alphas']+[alpha[0].numpy()]})\n",
    "            if not candidates: break\n",
    "            def lp(b): return b['score']/(len(b['seq'])**length_penalty)\n",
    "            candidates.sort(key=lp, reverse=True)\n",
    "            beams = candidates[:beam_size]\n",
    "            if len(completed) >= beam_size: break\n",
    "\n",
    "        best = max(completed+beams,\n",
    "                   key=lambda b: b['score']/(len(b['seq'])**length_penalty))\n",
    "        words = [self.processor.tokenizer.index_word.get(i,'')\n",
    "                 for i in best['seq']\n",
    "                 if self.processor.tokenizer.index_word.get(i,'') not in\n",
    "                 ('<start>','<end>','<unk>')]\n",
    "        return (words, best['alphas']) if return_attention else words\n",
    "\n",
    "    def greedy_decode(self, image_path: str, return_attention=False):\n",
    "        \"\"\"Generate caption via greedy decoding (deterministic crop).\"\"\"\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0\n",
    "        )\n",
    "\n",
    "        features = self.encoder(img_tensor)\n",
    "        hidden = tf.zeros((1, self.config['units']))\n",
    "        cell   = tf.zeros_like(hidden)\n",
    "        dec_input = tf.expand_dims(\n",
    "            [self.processor.tokenizer.word_index['<start>']], 0\n",
    "        )\n",
    "\n",
    "        result, alphas = [], []\n",
    "        for _ in range(self.config['max_length']):\n",
    "            logits, hidden, cell, alpha = self.decoder(\n",
    "                dec_input, features, hidden, cell\n",
    "            )\n",
    "            pred_id = tf.argmax(logits[0, 0]).numpy()\n",
    "            word = self.processor.tokenizer.index_word.get(pred_id, '')\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            if word not in ('<start>', '<unk>'):\n",
    "                result.append(word)\n",
    "            alphas.append(alpha[0].numpy())\n",
    "            dec_input = tf.expand_dims([pred_id], 0)\n",
    "\n",
    "        return (result, alphas) if return_attention else result\n",
    "\n",
    "    def evaluate_bleu(self, test_data, max_samples=None):\n",
    "        \"\"\"Calculate BLEU scores on test data.\"\"\"\n",
    "        refs, hyps = [], []\n",
    "        data_to_eval = test_data[:max_samples] if max_samples else test_data\n",
    "        \n",
    "        for img_name, _ in tqdm.tqdm(data_to_eval):\n",
    "            image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            hyp = self.greedy_decode(image_path)\n",
    "            \n",
    "            # Process ground truth captions\n",
    "            gt = [self.processor.preprocess_caption(c).split() for c in self.processor.captions_dict[img_name][:5]]\n",
    "            gt = [[w for w in cap if w not in ('<start>', '<end>')] for cap in gt]\n",
    "            \n",
    "            refs.append(gt)\n",
    "            hyps.append(hyp)\n",
    "        \n",
    "        # Calculate BLEU scores for different n-grams\n",
    "        bleu_scores = {}\n",
    "        for i in range(1, 5):\n",
    "            weights = tuple([1.0/i]*i + [0.0]*(4-i))\n",
    "            score = corpus_bleu(refs, hyps, weights=weights, smoothing_function=self.smoothie)\n",
    "            bleu_scores[f'bleu-{i}'] = score\n",
    "            print(f\"BLEU-{i}: {score:.4f}\")\n",
    "        \n",
    "        return bleu_scores\n",
    "    \n",
    "    def train(self, train_ds, val_data, epochs=None, subset_size: int = 200):\n",
    "        \"\"\"\n",
    "        Train with:\n",
    "          â€¢ fixed |subset_size| random train images for quick BLEU tracking\n",
    "          â€¢ full validation split for early-stop decision\n",
    "        \"\"\"\n",
    "        if epochs is None:\n",
    "            epochs = self.config['epochs']\n",
    "\n",
    "        # pick subset indices once\n",
    "        if self.bleu_subset_idx is None:\n",
    "            total_train = len(self.processor.train_data)\n",
    "            subset_size = min(subset_size, total_train)\n",
    "            self.bleu_subset_idx = random.sample(range(total_train), subset_size)\n",
    "\n",
    "        def _subset(data, idx):\n",
    "            return [data[i] for i in idx]\n",
    "\n",
    "        patience      = self.config['patience']\n",
    "        wait          = 0\n",
    "        self.ss_max_prob = self.config.get('scheduled_sampling_max_prob', 0.0)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # scheduled-sampling Îµ\n",
    "            self.ss_prob = self.ss_max_prob * epoch / max(1, epochs - 1)\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}  (Îµ = {self.ss_prob:.3f})\")\n",
    "\n",
    "            start, total_loss, step = time.time(), 0.0, 0\n",
    "            progbar = tf.keras.utils.Progbar(None, stateful_metrics=['loss'])\n",
    "\n",
    "            for batch, (img_tensor, target, cap_len) in enumerate(train_ds):\n",
    "                if batch == 0 and progbar.target is None:\n",
    "                    progbar.target = len(self.processor.train_data) // self.config['batch_size'] + 1\n",
    "\n",
    "                batch_loss = self.train_step(img_tensor, target, cap_len)\n",
    "                total_loss += batch_loss\n",
    "                progbar.update(batch + 1, values=[('loss', batch_loss)])\n",
    "                step += 1\n",
    "\n",
    "            avg_loss = total_loss / step\n",
    "            self.train_loss_log.append(float(avg_loss))\n",
    "\n",
    "            # ------ quick BLEU on fixed TRAIN subset ------\n",
    "            train_subset = _subset(self.processor.train_data, self.bleu_subset_idx)\n",
    "            train_bleu   = self.evaluate_bleu(train_subset)['bleu-4']\n",
    "            self.train_bleu_log.append(train_bleu)\n",
    "\n",
    "            # ------ BLEU on *full* VAL split ------\n",
    "            val_bleu = self.evaluate_bleu(val_data)['bleu-4']   # no slicing\n",
    "            self.val_bleu_log.append(val_bleu)\n",
    "\n",
    "            # ------ checkpoint & early-stopping ------\n",
    "            self.ckpt_manager.save()\n",
    "            if val_bleu > self.best_bleu:\n",
    "                self.best_bleu = val_bleu\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: loss={avg_loss:.4f}  \"\n",
    "                  f\"train-BLEU={train_bleu:.4f}  val-BLEU={val_bleu:.4f}  \"\n",
    "                  f\"time={time.time()-start:.1f}s\", flush=True)\n",
    "\n",
    "        return self.train_loss_log, self.val_bleu_log\n",
    "    \n",
    "    def plot_attention(self, image_path: str, caption: list, alphas: list):\n",
    "        \"\"\"Visualize attention weights overlaid on the source image.\"\"\"\n",
    "        img = np.array(Image.open(image_path).resize((224, 224)))\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "        \n",
    "        for t in range(len(caption)):\n",
    "            ax = fig.add_subplot(3, int(np.ceil(len(caption)/3)), t+1)\n",
    "            ax.set_title(caption[t])\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_shape = int(np.sqrt(alpha.size))\n",
    "            alpha = alpha.reshape(attention_shape, attention_shape)\n",
    "            ax.imshow(alpha, cmap='viridis', alpha=0.6, extent=(0, 224, 224, 0))\n",
    "            ax.axis('off')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot loss curve **and** both train/val BLEU-4 curves.\"\"\"\n",
    "        plt.figure(figsize=(14, 5))\n",
    "\n",
    "        # --- left: training loss ---\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_log, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cross-Entropy Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # --- right: BLEU-4 ---\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if self.train_bleu_log:\n",
    "            plt.plot(self.train_bleu_log, label='Train BLEU-4')\n",
    "        plt.plot(self.val_bleu_log,   label='Val BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4')\n",
    "        plt.title('BLEU-4 Scores')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Generate speech audio from caption text.\"\"\"\n",
    "        if not caption:\n",
    "            print(\"Empty caption, nothing to speak\")\n",
    "            return\n",
    "            \n",
    "        tts = gTTS(text=caption, lang='en')\n",
    "        tts.save(filename)\n",
    "        display(Audio(filename))\n",
    "        print(f\"Audio saved to {filename}\")\n",
    "    \n",
    "    def demo(self,\n",
    "             image_path: str,\n",
    "             filename: str = \"caption_audio.mp3\",\n",
    "             beam_size: int = 5,\n",
    "             length_penalty: float = 0.7):\n",
    "        \"\"\"\n",
    "        End-to-end demo (beam-search inference) in the following order:\n",
    "          1. Original image\n",
    "          2. Ground-truth captions\n",
    "          3. Generated caption\n",
    "          4. Audio playback\n",
    "          5. Attention heat-maps\n",
    "        \"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return\n",
    "\n",
    "        # ---------- 1. original image ----------\n",
    "        img = Image.open(image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # ---------- 2. ground-truth captions ----------\n",
    "        img_name = os.path.basename(image_path)\n",
    "        gt_caps = self.processor.captions_dict.get(img_name, [])\n",
    "        if gt_caps:\n",
    "            print(\"Ground-truth captions:\")\n",
    "            for cap in gt_caps:\n",
    "                print(f\"- {cap}\")\n",
    "        else:\n",
    "            print(\"No ground-truth captions found.\")\n",
    "\n",
    "        # ---------- 3. caption generation ----------\n",
    "        words, attention = self.beam_search_decode(\n",
    "            image_path,\n",
    "            beam_size=beam_size,\n",
    "            length_penalty=length_penalty,\n",
    "            return_attention=True\n",
    "        )\n",
    "        caption = \" \".join(words)\n",
    "        print(\"\\nGenerated caption:\")\n",
    "        print(caption)\n",
    "\n",
    "        # ---------- 4. audio ----------\n",
    "        self.speak_caption(caption, filename=filename)\n",
    "\n",
    "        # ---------- 5. attention plot ----------\n",
    "        self.plot_attention(image_path, words, attention)\n",
    "\n",
    "\n",
    "    def prime_dataset(self, ds, steps: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Pre-fill a tf.data shuffle buffer so the first training epoch\n",
    "        starts without the usual â€œFilling up shuffle buffer â€¦â€ pause.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        ds    : the *un-iterated* tf.data.Dataset youâ€™ll pass to train()\n",
    "        steps : number of iterator steps to advance; default uses\n",
    "                buffer_size // batch_size + 1 from config.\n",
    "        \"\"\"\n",
    "        if steps is None:\n",
    "            steps = self.config['buffer_size'] // self.config['batch_size'] + 1\n",
    "\n",
    "        it = iter(ds)\n",
    "        for _ in range(steps):\n",
    "            try:\n",
    "                next(it)\n",
    "            except StopIteration:  # dataset shorter than requested priming\n",
    "                break\n",
    "\n",
    "    def fine_tune_cnn(self,\n",
    "                      train_ds,\n",
    "                      val_data,\n",
    "                      layers_to_unfreeze: int = 2,\n",
    "                      lr: float = 1e-5,\n",
    "                      epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Phase-2 fine-tuning of the top Inception blocks.\n",
    "        Call after initial caption training for an extra accuracy bump.\n",
    "        \"\"\"\n",
    "        print(f\"\\nUnfreezing top {layers_to_unfreeze} Inception blocks â€¦\")\n",
    "        self.encoder.unfreeze_top_layers(layers_to_unfreeze)\n",
    "\n",
    "        # New, low learning-rate optimiser\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        print(f\"Fine-tuning CNN for {epochs} epoch(s) at lr={lr} â€¦\")\n",
    "        self.train(train_ds, val_data, epochs=epochs)\n",
    "\n",
    "        print(\"CNN fine-tune finished.\")\n",
    "\n",
    "    def attention_diagnostics(\n",
    "        self,\n",
    "        image_path: str,\n",
    "        beam_size: int = 5,\n",
    "        length_penalty: float = 0.7,\n",
    "        k: int = 3,\n",
    "        focus_threshold: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        On-demand visual/quantitative checks for attention quality.\n",
    "\n",
    "        â€¢ Attention shift per token           (Î”Î±â‚œ  = L1 distance)\n",
    "        â€¢ Focus strength / entropy            (max Î±â‚œ, ğ“—(Î±â‚œ))\n",
    "        â€¢ Caption-token heat-maps             (re-uses plot_attention)\n",
    "        â€¢ Î± coverage over the image           (Î£â‚œ Î±â‚œ)\n",
    "        â€¢ Top-k Î± values per token            (printed)\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        image_path       : input image\n",
    "        beam_size        : beam width for caption\n",
    "        length_penalty   : length norm for beam search\n",
    "        k                : number of top-Î± scores to list\n",
    "        focus_threshold  : max Î±â‚œ â‰¥ threshold â‡’ â€œstrong, focusedâ€\n",
    "        \"\"\"\n",
    "\n",
    "        # â”€â”€ generate caption & collect Î±â€™s â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        words, alpha_list = self.beam_search_decode(\n",
    "            image_path,\n",
    "            beam_size=beam_size,\n",
    "            length_penalty=length_penalty,\n",
    "            return_attention=True,\n",
    "        )\n",
    "        alphas = np.stack(alpha_list)          # (T, L)\n",
    "        T, L = alphas.shape\n",
    "        grid = int(np.sqrt(L))                 # 8 for Inception-V3 (8 Ã— 8)\n",
    "\n",
    "        # â”€â”€ metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        shifts     = np.concatenate([[0.0], np.sum(np.abs(np.diff(alphas, 0)), 1)])\n",
    "        entropies  = -np.sum(alphas * np.log(alphas + 1e-9), 1)\n",
    "        focused    = alphas.max(1) >= focus_threshold\n",
    "        coverage   = alphas.sum(0) / T        # mean Î± at each spatial cell\n",
    "\n",
    "        # â”€â”€ 1) per-token heat-maps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        self.plot_attention(image_path, words, alpha_list)\n",
    "\n",
    "        # â”€â”€ 2) coverage heat-map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.title(\"Î± coverage\")\n",
    "        plt.imshow(coverage.reshape(grid, grid), cmap=\"magma\", origin=\"upper\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "        # â”€â”€ 3) shifts & entropy line-plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        plt.figure(figsize=(10, 3))\n",
    "        plt.plot(shifts, label=\"Î”Î± (shift)\")\n",
    "        plt.plot(entropies, label=\"entropy ğ“—(Î±)\")\n",
    "        plt.xticks(range(T), words, rotation=45, ha=\"right\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # â”€â”€ 4) top-k Î± values per token â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(f\"\\nTop-{k} Î± values per token:\")\n",
    "        for w, a in zip(words, alphas):\n",
    "            topk = np.sort(a)[-k:][::-1]\n",
    "            print(f\"{w:<15} {topk}\")\n",
    "\n",
    "        # â”€â”€ 5) highlight strong, focused tokens â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        print(f\"\\nTokens with strong focus (max Î± â‰¥ {focus_threshold:.2f}):\")\n",
    "        for i, (w, flag) in enumerate(zip(words, focused)):\n",
    "            if flag:\n",
    "                print(f\"  {i:02d} â€“ {w}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = processor.load_captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.display_samples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.prepare_captions(subset_ratio=CONFIG['subset_ratio'])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, _ = processor.prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel(CONFIG, processor)\n",
    "model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prime_dataset(train_ds, steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_val = random.sample(processor.val_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(train_ds, processor.val_data)\n",
    "model.train(train_ds, reduced_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fine_tune_cnn(train_ds, processor.val_data, layers_to_unfreeze=8, lr=1e-5, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on test set:\")\n",
    "model.evaluate_bleu(processor.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pair = random.choice(processor.test_data)\n",
    "sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "model.demo(sample_img, filename='caption_audio01.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.attention_diagnostics(\n",
    "    sample_img,\n",
    "    beam_size=5,          # optional\n",
    "    length_penalty=0.7,   # optional\n",
    "    k=3,                  # optional\n",
    "    focus_threshold=0.5   # optional\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_pair = random.choice(processor.test_data)\n",
    "# sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "# model.demo(sample_img, filename='caption_audio02.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_pair = random.choice(processor.test_data)\n",
    "# sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "# model.demo(sample_img, filename='caption_audio03.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_pair = random.choice(processor.test_data)\n",
    "# sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "# model.demo(sample_img, filename='caption_audio04.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_pair = random.choice(processor.test_data)\n",
    "# sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "# model.demo(sample_img, filename='caption_audio05.mp3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
