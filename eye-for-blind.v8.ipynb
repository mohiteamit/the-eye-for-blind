{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, math, random, json, pickle, itertools, warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import collections\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import time\n",
    "# \n",
    "# import random\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# from typing import Dict, List, Tuple, Optional\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, Model\n",
    "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "# import tqdm\n",
    "# from gtts import gTTS\n",
    "# from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, object] = {\n",
    "    'image_dir': '/home/flickr8k/Images',\n",
    "    'caption_file': '/home/flickr8k/captions_8k.csv',\n",
    "    'feature_cache_dir': '/home/flickr8k/cache',\n",
    "    'num_examples': None,\n",
    "    'max_caption_length': 50,\n",
    "    'min_word_frequency': 5,\n",
    "\n",
    "    'embedding_dim': 256,\n",
    "    'units': 512,\n",
    "    'decoder_dropout': 0.5,\n",
    "\n",
    "    'learning_rate': 5e-5,\n",
    "    'epochs': 20,\n",
    "    'batch_size': 64,\n",
    "\n",
    "    'buffer_size': 1000,\n",
    "    'patience': 5,\n",
    "    'checkpoint_path': './checkpoints/lstm_attention_flickr8k',\n",
    "    'mixed_precision': True,\n",
    "\n",
    "    'attention_reg_lambda': 0.5,\n",
    "    'grad_clip_value': 5.0, # Added missing grad_clip_value\n",
    "    'scheduled_sampling_max_prob': 0.2, # Added missing scheduled_sampling_max_prob\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ENV SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA RTX 6000 Ada Generation, compute capability 8.9\n",
      "[AMP] mixed_float16 policy active\n",
      "Using GPU: /physical_device:GPU:0 | batch=64\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"[AMP] mixed_float16 policy active\")\n",
    "else:\n",
    "    print(\"[AMP] disabled – using float32 throughout\")\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"Using GPU: {physical_devices[0].name} | batch={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"GPU not found – fallback to CPU\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer: Optional[Tokenizer] = None\n",
    "        self.img_to_cap_map: Dict[str, List[str]] = collections.defaultdict(list)\n",
    "        self.image_paths: List[str] = []\n",
    "        self.all_captions: List[str] = []\n",
    "        self.train_data: List[Tuple[str, List[int]]] = []\n",
    "        self.val_data: List[Tuple[str, List[int]]] = []\n",
    "        self.test_data: List[Tuple[str, List[int]]] = []\n",
    "        self.max_caption_length = 0\n",
    "        self.vocab_size = 0\n",
    "        self.num_steps_per_epoch = 0\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        print(\"Loading and preprocessing captions...\")\n",
    "        df = pd.read_csv(self.config['caption_file'], header=None, names=['image_name', 'comment'], engine='python')\n",
    "\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "\n",
    "        temp_img_to_cap_map = collections.defaultdict(list)\n",
    "        all_unique_img_names_from_csv = df['image_name'].unique()\n",
    "\n",
    "        print(f\"Checking {len(all_unique_img_names_from_csv)} unique image files from CSV...\")\n",
    "        found_images_count = 0\n",
    "        \n",
    "        existing_image_files = set(os.listdir(self.config['image_dir']))\n",
    "\n",
    "        for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"Validating images & processing captions\"):\n",
    "            img_name = row['image_name']\n",
    "            caption = row['comment']\n",
    "            \n",
    "            if img_name in existing_image_files:\n",
    "                temp_img_to_cap_map[img_name].append(self.preprocess_text(caption))\n",
    "                if img_name not in self.img_to_cap_map:\n",
    "                    found_images_count += 1\n",
    "                self.img_to_cap_map[img_name] = temp_img_to_cap_map[img_name]\n",
    "            \n",
    "        if found_images_count < len(all_unique_img_names_from_csv):\n",
    "            print(f\"Warning: {len(all_unique_img_names_from_csv) - found_images_count} images mentioned in CSV were not found in {self.config['image_dir']}. They have been discarded.\")\n",
    "        \n",
    "        self.image_paths = sorted(list(self.img_to_cap_map.keys()))\n",
    "\n",
    "        if self.config['num_examples']:\n",
    "            if len(self.image_paths) > self.config['num_examples']:\n",
    "                self.image_paths = random.sample(self.image_paths, self.config['num_examples'])\n",
    "                self.img_to_cap_map = {img: self.img_to_cap_map[img] for img in self.image_paths}\n",
    "                print(f\"Using a subset of {len(self.image_paths)} images due to 'num_examples' config.\")\n",
    "\n",
    "        self.all_captions = []\n",
    "        for img_name in self.image_paths:\n",
    "            self.all_captions.extend(self.img_to_cap_map[img_name])\n",
    "\n",
    "        print(f\"Total valid images (with captions): {len(self.image_paths)}\")\n",
    "        print(f\"Total valid captions: {len(self.all_captions)}\")\n",
    "\n",
    "        self.tokenizer = Tokenizer(num_words=None, oov_token=\"<unk>\",\n",
    "                                   filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ',\n",
    "                                   lower=True)\n",
    "        self.tokenizer.fit_on_texts(self.all_captions)\n",
    "\n",
    "        word_counts = collections.Counter(word for caption in self.all_captions for word in caption.split())\n",
    "        filtered_word_index = {\n",
    "            word: index for word, index in self.tokenizer.word_index.items()\n",
    "            if word_counts[word] >= self.config['min_word_frequency'] or word in ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        }\n",
    "        self.tokenizer.word_index = filtered_word_index\n",
    "        self.tokenizer.index_word = {v: k for k, v in filtered_word_index.items()}\n",
    "\n",
    "        if '<pad>' not in self.tokenizer.word_index:\n",
    "            self.tokenizer.word_index['<pad>'] = len(self.tokenizer.word_index) + 1\n",
    "            self.tokenizer.index_word[len(self.tokenizer.index_word) + 1] = '<pad>'\n",
    "            \n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        print(f\"Vocabulary size after pruning (min_word_frequency={self.config['min_word_frequency']}): {self.vocab_size}\")\n",
    "\n",
    "        all_seqs = self.tokenizer.texts_to_sequences(self.all_captions)\n",
    "        self.max_caption_length = max(len(s) for s in all_seqs)\n",
    "        self.config['max_caption_length'] = self.max_caption_length\n",
    "        print(f\"Max caption length: {self.max_caption_length}\")\n",
    "\n",
    "    def preprocess_text(self, caption: str) -> str:\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z ]\", \"\", caption)\n",
    "        caption = re.sub(r'\\s+', ' ', caption).strip()\n",
    "        caption = '<start> ' + caption + ' <end>'\n",
    "        return caption\n",
    "\n",
    "    def create_dataset_splits(self, train_ratio=0.8, val_ratio=0.1):\n",
    "        random.shuffle(self.image_paths)\n",
    "        num_images = len(self.image_paths)\n",
    "        num_train = int(train_ratio * num_images)\n",
    "        num_val = int(val_ratio * num_images)\n",
    "\n",
    "        train_image_paths = self.image_paths[:num_train]\n",
    "        val_image_paths = self.image_paths[num_train:num_train + num_val]\n",
    "        test_image_paths = self.image_paths[num_train + num_val:]\n",
    "\n",
    "        print(f\"Train images: {len(train_image_paths)}, Val images: {len(val_image_paths)}, Test images: {len(test_image_paths)}\")\n",
    "\n",
    "        self.train_data = self._create_pairs(train_image_paths)\n",
    "        self.val_data = self._create_pairs(val_image_paths)\n",
    "        self.test_data = self._create_pairs(test_image_paths)\n",
    "\n",
    "        print(f\"Train pairs: {len(self.train_data)}, Val pairs: {len(self.val_data)}, Test pairs: {len(self.test_data)}\")\n",
    "\n",
    "        self.num_steps_per_epoch = len(self.train_data) // self.config['batch_size']\n",
    "\n",
    "    def _create_pairs(self, image_names: List[str]) -> List[Tuple[str, List[int]]]:\n",
    "        pairs = []\n",
    "        for img_name in image_names:\n",
    "            full_img_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            for caption in self.img_to_cap_map[img_name]:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                padded_seq = pad_sequences([seq], maxlen=self.max_caption_length, padding='post')[0]\n",
    "                pairs.append((full_img_path, list(padded_seq)))\n",
    "        return pairs\n",
    "\n",
    "    def get_data_with_cached_features(self, image_name_to_cached_path_map: Dict[str, str]) -> Tuple[List, List, List]:\n",
    "        def _reconstruct_list(data_list):\n",
    "            reconstructed = []\n",
    "            for original_img_path, caption_ids in data_list:\n",
    "                basename = os.path.basename(original_img_path)\n",
    "                cached_path = image_name_to_cached_path_map.get(basename)\n",
    "                if cached_path:\n",
    "                    reconstructed.append((original_img_path, cached_path, caption_ids))\n",
    "            return reconstructed\n",
    "\n",
    "        final_train = _reconstruct_list(self.train_data)\n",
    "        final_val = _reconstruct_list(self.val_data)\n",
    "        final_test = _reconstruct_list(self.test_data)\n",
    "\n",
    "        print(f\"Adjusted train pairs (after feature caching check): {len(final_train)}\")\n",
    "        print(f\"Adjusted val pairs (after feature caching check): {len(final_val)}\")\n",
    "        print(f\"Adjusted test pairs (after feature caching check): {len(final_test)}\")\n",
    "        \n",
    "        return final_train, final_val, final_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMAGE FEATURE EXTRACTION (CACHE FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor(Model):\n",
    "    def __init__(self, target_size=(299, 299)):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.inception_v3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        self.inception_v3.trainable = False\n",
    "        self.feature_extractor = Model(inputs=self.inception_v3.input,\n",
    "                                       outputs=self.inception_v3.get_layer('mixed7').output)\n",
    "\n",
    "    @tf.function\n",
    "    def load_and_preprocess_image(self, image_path: tf.Tensor) -> tf.Tensor:\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.target_size)\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img\n",
    "\n",
    "class ImageFeatureCacheManager:\n",
    "    def __init__(self, config, feature_extractor: ImageFeatureExtractor):\n",
    "        self.config = config\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.cache_dir = config['feature_cache_dir']\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def manage_feature_cache(self, image_names: List[str]) -> Dict[str, str]:\n",
    "        print(f\"\\nManaging image feature cache in: {self.cache_dir}\")\n",
    "        print(f\"Checking/extracting features for {len(image_names)} unique images.\")\n",
    "\n",
    "        image_name_to_cached_path = {}\n",
    "        images_to_extract = []\n",
    "\n",
    "        for img_name in image_names:\n",
    "            cache_file = os.path.join(self.cache_dir, img_name + '.npy')\n",
    "            if not os.path.exists(cache_file):\n",
    "                images_to_extract.append(img_name)\n",
    "            image_name_to_cached_path[img_name] = cache_file\n",
    "\n",
    "        if images_to_extract:\n",
    "            print(f\"Found {len(images_to_extract)} images whose features need extraction...\")\n",
    "            \n",
    "            full_paths_for_extraction = [os.path.join(self.config['image_dir'], img_name) for img_name in images_to_extract]\n",
    "\n",
    "            for i, img_path in enumerate(tqdm.tqdm(full_paths_for_extraction, desc=\"Extracting & Caching Features\")):\n",
    "                img_name = os.path.basename(img_path)\n",
    "                cache_path = os.path.join(self.cache_dir, img_name + '.npy')\n",
    "                try:\n",
    "                    img_tensor_processed = self.feature_extractor.load_and_preprocess_image(tf.constant(img_path))\n",
    "                    features = self.feature_extractor.feature_extractor(tf.expand_dims(img_tensor_processed, 0))\n",
    "                    features_flat = tf.reshape(features, (features.shape[0], -1, features.shape[3]))\n",
    "                    np.save(cache_path, features_flat.numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing {img_path}: {e}. Skipping feature caching for this image.\")\n",
    "                    if img_name in image_name_to_cached_path:\n",
    "                        del image_name_to_cached_path[img_name]\n",
    "                    continue\n",
    "        else:\n",
    "            print(\"All image features already cached. Skipping extraction.\")\n",
    "            \n",
    "        print(\"Image feature cache management complete.\")\n",
    "        return image_name_to_cached_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = layers.LSTM(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform',\n",
    "                                   dropout=dropout)\n",
    "        self.fc1 = layers.Dense(self.units)\n",
    "        self.fc2 = layers.Dense(vocab_size, dtype='float32')\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden_state, cell_state):\n",
    "        context_vector, attention_weights = self.attention(features, hidden_state)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, new_hidden_state, new_cell_state = self.lstm(x, initial_state=[hidden_state, cell_state])\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc1(output)\n",
    "        logits = self.fc2(x)\n",
    "        return logits, new_hidden_state, new_cell_state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TEXT-TO-SPEECH UTILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSpeech:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.gTTS = gTTS\n",
    "            self.Audio = Audio\n",
    "            self.display = display\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"WARNING: gTTS or IPython.display not found. Speech functionality will be disabled.\")\n",
    "            self.available = False\n",
    "\n",
    "    def speak(self, text: str, filename: str = \"caption_audio.mp3\"):\n",
    "        if not self.available:\n",
    "            print(\"Text-to-speech functionality is not available. Please install 'gtts' and ensure running in an IPython environment.\")\n",
    "            return\n",
    "        \n",
    "        if not text.strip():\n",
    "            print(\"Empty text, nothing to speak.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            tts = self.gTTS(text=text, lang='en')\n",
    "            tts.save(filename)\n",
    "            self.display(self.Audio(filename))\n",
    "            print(f\"Audio saved to {filename} and played.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating or playing audio: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TRAINING LOOP & UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningTrainer:\n",
    "    def __init__(self, config, processor: DataProcessor, feature_extractor: ImageFeatureExtractor):\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        self.encoder = Encoder(self.config['embedding_dim'])\n",
    "        self.decoder = Decoder(self.config['embedding_dim'], self.config['units'],\n",
    "                               self.processor.vocab_size, self.config['decoder_dropout'])\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate'])\n",
    "        if self.config['mixed_precision']:\n",
    "            self.optimizer = tf.keras.mixed_precision.LossScaleOptimizer(self.optimizer)\n",
    "\n",
    "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "        self.checkpoint_prefix = os.path.join(self.config['checkpoint_path'], \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(encoder=self.encoder,\n",
    "                                              decoder=self.decoder,\n",
    "                                              optimizer=self.optimizer)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, self.config['checkpoint_path'], max_to_keep=5)\n",
    "\n",
    "        self.tts_speaker = TextToSpeech()\n",
    "\n",
    "        self.train_loss_results = []\n",
    "        self.val_bleu_results = []\n",
    "        self.best_val_bleu = 0.0\n",
    "        self.smoothing_function = SmoothingFunction().method4\n",
    "        self.patience_counter = 0\n",
    "\n",
    "\n",
    "        if self.checkpoint_manager.latest_checkpoint:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            print(f\"Restored from {self.checkpoint_manager.latest_checkpoint}\")\n",
    "        else:\n",
    "            print(\"Initializing from scratch.\")\n",
    "\n",
    "    def loss_function(self, real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = self.loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, img_tensor, target):\n",
    "        batch_size = tf.shape(target)[0]\n",
    "        loss = 0.0\n",
    "\n",
    "        hidden = tf.zeros((batch_size, self.config['units']), dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16)\n",
    "        cell = tf.zeros((batch_size, self.config['units']), dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16)\n",
    "\n",
    "        dec_input = tf.expand_dims([self.processor.tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)\n",
    "\n",
    "            attention_sum_square_error = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "            for i in tf.range(1, target.shape[1]):\n",
    "                predictions, hidden, cell, attention_weights = self.decoder(dec_input, features, hidden, cell)\n",
    "\n",
    "                loss += self.loss_function(target[:, i], predictions)\n",
    "\n",
    "                if self.config.get('scheduled_sampling_max_prob', 0.0) > 0:\n",
    "                    prob = tf.random.uniform([], 0, 1)\n",
    "                    if prob < self.current_scheduled_sampling_prob:\n",
    "                        predicted_id = tf.argmax(predictions, axis=1)\n",
    "                        dec_input = tf.expand_dims(predicted_id, 1)\n",
    "                    else:\n",
    "                        dec_input = tf.expand_dims(target[:, i], 1)\n",
    "                else:\n",
    "                    dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "                attention_sum_square_error += tf.reduce_mean(tf.square(tf.reduce_sum(attention_weights, axis=1) - 1.0))\n",
    "\n",
    "            total_loss = (loss / tf.cast(target.shape[1], tf.float32))\n",
    "            total_loss += self.config['attention_reg_lambda'] * attention_sum_square_error / tf.cast(target.shape[1], tf.float32)\n",
    "\n",
    "            if self.config['mixed_precision']:\n",
    "                scaled_loss = self.optimizer.get_scaled_loss(total_loss)\n",
    "            else:\n",
    "                scaled_loss = total_loss\n",
    "\n",
    "        trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        gradients = tape.gradient(scaled_loss, trainable_variables)\n",
    "\n",
    "        if self.config['mixed_precision']:\n",
    "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
    "\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def evaluate_bleu_score(self, dataset_pairs: List[Tuple[str, List[int]]], num_samples=None):\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        if num_samples is None:\n",
    "            samples_to_evaluate = dataset_pairs\n",
    "        else:\n",
    "            samples_to_evaluate = random.sample(dataset_pairs, min(num_samples, len(dataset_pairs)))\n",
    "\n",
    "        print(f\"\\nEvaluating BLEU on {len(samples_to_evaluate)} samples...\")\n",
    "        for img_path, _ in tqdm.tqdm(samples_to_evaluate):\n",
    "            generated_caption_tokens = self.greedy_inference(img_path)\n",
    "            if not generated_caption_tokens:\n",
    "                continue\n",
    "            hypotheses.append(generated_caption_tokens)\n",
    "\n",
    "            img_name = os.path.basename(img_path)\n",
    "            raw_captions = self.processor.img_to_cap_map.get(img_name, [])\n",
    "            \n",
    "            img_references = []\n",
    "            for raw_cap in raw_captions:\n",
    "                cleaned_cap = raw_cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                if cleaned_cap:\n",
    "                    img_references.append(cleaned_cap.split())\n",
    "            \n",
    "            if img_references:\n",
    "                references.append(img_references)\n",
    "            else:\n",
    "                hypotheses.pop()\n",
    "\n",
    "        if not references:\n",
    "            print(\"No valid reference captions found for BLEU evaluation.\")\n",
    "            return {\"bleu-1\": 0.0, \"bleu-2\": 0.0, \"bleu-3\": 0.0, \"bleu-4\": 0.0}\n",
    "\n",
    "        bleu_scores = {}\n",
    "        for n in range(1, 5):\n",
    "            weights = (1.0 / n,) * n + (0.0,) * (4 - n)\n",
    "            bleu_scores[f\"bleu-{n}\"] = corpus_bleu(references, hypotheses, weights=weights,\n",
    "                                                    smoothing_function=self.smoothing_function)\n",
    "            print(f\"BLEU-{n}: {bleu_scores[f'bleu-{n}']:.4f}\")\n",
    "        \n",
    "        return bleu_scores\n",
    "\n",
    "    def greedy_inference(self, image_path: str):\n",
    "        filename = os.path.basename(image_path)\n",
    "        feature_cache_path = os.path.join(self.config['feature_cache_dir'], filename + '.npy')\n",
    "        \n",
    "        if not os.path.exists(feature_cache_path):\n",
    "            print(f\"Error: Feature cache not found for {image_path}\")\n",
    "            return []\n",
    "\n",
    "        img_features = np.load(feature_cache_path)\n",
    "        img_features_tensor = tf.convert_to_tensor(img_features, dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16)\n",
    "\n",
    "        features = self.encoder(img_features_tensor)\n",
    "\n",
    "        hidden = tf.zeros((1, self.config['units']), dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16)\n",
    "        cell = tf.zeros((1, self.config['units']), dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16)\n",
    "\n",
    "        dec_input = tf.expand_dims([self.processor.tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "        result = []\n",
    "        for i in range(self.config['max_caption_length']):\n",
    "            predictions, hidden, cell, _ = self.decoder(dec_input, features, hidden, cell)\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "            predicted_word = self.processor.tokenizer.index_word.get(predicted_id, '<unk>')\n",
    "\n",
    "            if predicted_word == '<end>':\n",
    "                break\n",
    "            if predicted_word not in ('<unk>', '<start>', '<pad>'):\n",
    "                result.append(predicted_word)\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def beam_search_inference(self, image_path: str, beam_size: int = 3, length_penalty_weight: float = 0.7):\n",
    "        filename = os.path.basename(image_path)\n",
    "        feature_cache_path = os.path.join(self.config['feature_cache_dir'], filename + '.npy')\n",
    "        if not os.path.exists(feature_cache_path):\n",
    "            print(f\"Error: Feature cache not found for {image_path}\")\n",
    "            return [], []\n",
    "\n",
    "        img_features = np.load(feature_cache_path)\n",
    "        img_features_tensor = tf.convert_to_tensor(img_features, dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16)\n",
    "        features = self.encoder(img_features_tensor)\n",
    "\n",
    "        start_token = self.processor.tokenizer.word_index['<start>']\n",
    "        end_token = self.processor.tokenizer.word_index['<end>']\n",
    "\n",
    "        beams = [(\n",
    "            [start_token],\n",
    "            0.0,\n",
    "            tf.zeros((1, self.config['units']), dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16),\n",
    "            tf.zeros((1, self.config['units']), dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16),\n",
    "            []\n",
    "        )]\n",
    "\n",
    "        completed_beams = []\n",
    "\n",
    "        for _ in range(self.config['max_caption_length']):\n",
    "            new_beams = []\n",
    "            for seq, score, hidden, cell, alphas in beams:\n",
    "                last_token = seq[-1]\n",
    "\n",
    "                if last_token == end_token:\n",
    "                    completed_beams.append((seq, score, alphas))\n",
    "                    continue\n",
    "\n",
    "                dec_input = tf.expand_dims([last_token], 0)\n",
    "                predictions, new_hidden, new_cell, attention_weights = self.decoder(dec_input, features, hidden, cell)\n",
    "\n",
    "                predictions = tf.cast(predictions[0], tf.float32)\n",
    "                log_probs = tf.nn.log_softmax(predictions).numpy()\n",
    "\n",
    "                top_k_indices = np.argsort(log_probs)[::-1][:beam_size]\n",
    "\n",
    "                for idx in top_k_indices:\n",
    "                    token_id = int(idx)\n",
    "                    token_log_prob = float(log_probs[idx])\n",
    "                    \n",
    "                    new_beams.append((\n",
    "                        seq + [token_id],\n",
    "                        score + token_log_prob,\n",
    "                        new_hidden,\n",
    "                        new_cell,\n",
    "                        alphas + [attention_weights[0].numpy()]\n",
    "                    ))\n",
    "\n",
    "            new_beams.sort(key=lambda x: x[1] / (len(x[0]) ** length_penalty_weight), reverse=True)\n",
    "            beams = new_beams[:beam_size]\n",
    "\n",
    "            if len(completed_beams) >= beam_size:\n",
    "                break\n",
    "        \n",
    "        completed_beams.extend([(seq, score, alphas) for seq, score, _, _, alphas in beams])\n",
    "\n",
    "        if not completed_beams:\n",
    "             return [], []\n",
    "\n",
    "        best_seq, best_score, best_alphas = max(completed_beams, key=lambda x: x[1] / (len(x[0]) ** length_penalty_weight))\n",
    "\n",
    "        caption_words = [self.processor.tokenizer.index_word.get(i, '<unk>') for i in best_seq]\n",
    "        \n",
    "        filtered_caption_words = [\n",
    "            word for word in caption_words\n",
    "            if word not in ['<start>', '<end>', '<pad>', '<unk>']\n",
    "        ]\n",
    "\n",
    "        return filtered_caption_words, best_alphas\n",
    "\n",
    "    def train(self, train_cached_paths: List[Tuple[str, str, List[int]]],\n",
    "              val_dataset_pairs: List[Tuple[str, List[int]]]):\n",
    "        \n",
    "        # Generator function for the TensorFlow dataset\n",
    "        def _data_generator():\n",
    "            for orig_img_path, cache_path, caption_ids in train_cached_paths:\n",
    "                # Ensure the loaded feature array has the expected shape (64, 768)\n",
    "                features = np.load(cache_path)\n",
    "                features = features.reshape(features.shape[-2], features.shape[-1]) # Ensure (64, 768)\n",
    "                yield features, np.array(caption_ids, dtype=np.int32)\n",
    "\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            _data_generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(64, 768), dtype=tf.float32 if not self.config['mixed_precision'] else tf.float16),\n",
    "                tf.TensorSpec(shape=(self.config['max_caption_length'],), dtype=tf.int32)\n",
    "            )\n",
    "        )\n",
    "        train_dataset = train_dataset.shuffle(self.config['buffer_size']).batch(self.config['batch_size'])\n",
    "        train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            start = time.time()\n",
    "            total_loss = 0\n",
    "\n",
    "            self.current_scheduled_sampling_prob = (\n",
    "                self.config['scheduled_sampling_max_prob'] * (epoch / max(1, self.config['epochs'] - 1))\n",
    "            )\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config['epochs']} (Scheduled Sampling Prob: {self.current_scheduled_sampling_prob:.3f})\")\n",
    "\n",
    "\n",
    "            for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "                batch_loss = self.train_step(img_tensor, target)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "            \n",
    "            avg_train_loss = total_loss / self.processor.num_steps_per_epoch\n",
    "            self.train_loss_results.append(avg_train_loss.numpy())\n",
    "            print(f'Epoch {epoch+1} Loss {avg_train_loss:.4f}')\n",
    "\n",
    "            val_bleu_scores = self.evaluate_bleu_score(val_dataset_pairs, num_samples=1000)\n",
    "            current_val_bleu4 = val_bleu_scores.get('bleu-4', 0.0)\n",
    "            self.val_bleu_results.append(current_val_bleu4)\n",
    "\n",
    "            if current_val_bleu4 > self.best_val_bleu:\n",
    "                self.best_val_bleu = current_val_bleu4\n",
    "                self.checkpoint_manager.save()\n",
    "                print(f\"Saving checkpoint at epoch {epoch+1} with BLEU-4: {current_val_bleu4:.4f}\")\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                print(f\"BLEU-4 not improved. Patience counter: {self.patience_counter}/{self.config['patience']}\")\n",
    "                if self.patience_counter >= self.config['patience']:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    break\n",
    "\n",
    "            print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs')\n",
    "\n",
    "    def plot_history(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_results, label='Train Loss')\n",
    "        plt.title('Training Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_bleu_results, label='Validation BLEU-4')\n",
    "        plt.title('Validation BLEU-4 Score per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4 Score')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention(self, image_path: str, caption: List[str], alphas: List[np.ndarray]):\n",
    "        img = Image.open(image_path)\n",
    "        img = np.array(img.resize((299, 299)))\n",
    "\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "        for t in range(len(caption)):\n",
    "            if t > 24:\n",
    "                break\n",
    "            \n",
    "            ax = fig.add_subplot(5, 5, t + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_grid_size = int(np.sqrt(alpha.shape[0]))\n",
    "            alpha_reshaped = alpha.reshape(attention_grid_size, attention_grid_size)\n",
    "\n",
    "            alpha_resized = Image.fromarray(np.uint8(255 * alpha_reshaped)).resize(\n",
    "                (299, 299), resample=Image.BICUBIC\n",
    "            )\n",
    "            alpha_resized = np.array(alpha_resized) / 255.0\n",
    "\n",
    "            ax.imshow(alpha_resized, cmap='jet', alpha=0.5, extent=(0, 299, 299, 0))\n",
    "            ax.set_title(f\"{t+1}: '{caption[t]}'\", fontsize=10, color='blue')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f\"Attention Map for: {os.path.basename(image_path)}\", fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        self.tts_speaker.speak(caption, filename)\n",
    "\n",
    "    def demo(self, image_file_name: str):\n",
    "        full_image_path = os.path.join(self.config['image_dir'], image_file_name)\n",
    "        \n",
    "        if not os.path.exists(full_image_path):\n",
    "            print(f\"Error: Image not found at {full_image_path}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n--- Demo for {image_file_name}\")\n",
    "        \n",
    "        img = Image.open(full_image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {image_file_name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        gt_captions = self.processor.img_to_cap_map.get(image_file_name, [])\n",
    "        print(\"\\nGround Truth Captions:\")\n",
    "        if gt_captions:\n",
    "            for i, cap in enumerate(gt_captions):\n",
    "                clean_cap = cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                print(f\"  {i+1}. {clean_cap}\")\n",
    "        else:\n",
    "            print(\"  No ground truth captions available.\")\n",
    "        \n",
    "        generated_caption_words, attention_weights = self.beam_search_inference(full_image_path, beam_size=3)\n",
    "        generated_caption = \" \".join(generated_caption_words)\n",
    "        print(f\"\\nGenerated Caption (Beam Search): {generated_caption}\")\n",
    "\n",
    "        print(\"\\nPlaying generated caption:\")\n",
    "        self.speak_caption(generated_caption, filename=f\"caption_audio_{os.path.basename(image_file_name).split('.')[0]}.mp3\")\n",
    "\n",
    "        if generated_caption_words and attention_weights:\n",
    "            self.plot_attention(full_image_path, generated_caption_words, attention_weights)\n",
    "        else:\n",
    "            print(\"Could not generate caption or attention for plotting.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing captions...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Expected 2 fields in line 87, saw 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/the-eye-for-blind/eye-for-blind.v8.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://7066d43078490.notebooks.jarvislabs.net/home/the-eye-for-blind/eye-for-blind.v8.ipynb#Y144sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m processor\u001b[39m.\u001b[39;49mload_and_preprocess_data()\n",
      "\u001b[1;32m/home/the-eye-for-blind/eye-for-blind.v8.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://7066d43078490.notebooks.jarvislabs.net/home/the-eye-for-blind/eye-for-blind.v8.ipynb#Y144sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_and_preprocess_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://7066d43078490.notebooks.jarvislabs.net/home/the-eye-for-blind/eye-for-blind.v8.ipynb#Y144sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading and preprocessing captions...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://7066d43078490.notebooks.jarvislabs.net/home/the-eye-for-blind/eye-for-blind.v8.ipynb#Y144sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mcaption_file\u001b[39;49m\u001b[39m'\u001b[39;49m], header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mimage_name\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcomment\u001b[39;49m\u001b[39m'\u001b[39;49m], engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpython\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://7066d43078490.notebooks.jarvislabs.net/home/the-eye-for-blind/eye-for-blind.v8.ipynb#Y144sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mimage_name\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mimage_name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell://7066d43078490.notebooks.jarvislabs.net/home/the-eye-for-blind/eye-for-blind.v8.ipynb#Y144sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcomment\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    580\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 581\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/readers.py:1255\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1253\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1254\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1255\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[1;32m   1256\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1257\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/python_parser.py:270\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    267\u001b[0m     indexnamerow \u001b[39m=\u001b[39m content[\u001b[39m0\u001b[39m]\n\u001b[1;32m    268\u001b[0m     content \u001b[39m=\u001b[39m content[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 270\u001b[0m alldata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_rows_to_cols(content)\n\u001b[1;32m    271\u001b[0m data, columns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclude_implicit_index(alldata)\n\u001b[1;32m    273\u001b[0m conv_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_data(data)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/python_parser.py:1013\u001b[0m, in \u001b[0;36mPythonParser._rows_to_cols\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m   1007\u001b[0m             reason \u001b[39m=\u001b[39m (\n\u001b[1;32m   1008\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mError could possibly be due to quotes being \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1009\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mignored when a multi-char delimiter is used.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1010\u001b[0m             )\n\u001b[1;32m   1011\u001b[0m             msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m reason\n\u001b[0;32m-> 1013\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_alert_malformed(msg, row_num \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m   1015\u001b[0m \u001b[39m# see gh-13320\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m zipped_content \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(lib\u001b[39m.\u001b[39mto_object_array(content, min_width\u001b[39m=\u001b[39mcol_len)\u001b[39m.\u001b[39mT)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/parsers/python_parser.py:739\u001b[0m, in \u001b[0;36mPythonParser._alert_malformed\u001b[0;34m(self, msg, row_num)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[39mAlert a user about a malformed row, depending on value of\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[39m`self.on_bad_lines` enum.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[39m    even though we 0-index internally.\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_bad_lines \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBadLineHandleMethod\u001b[39m.\u001b[39mERROR:\n\u001b[0;32m--> 739\u001b[0m     \u001b[39mraise\u001b[39;00m ParserError(msg)\n\u001b[1;32m    740\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_bad_lines \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBadLineHandleMethod\u001b[39m.\u001b[39mWARN:\n\u001b[1;32m    741\u001b[0m     base \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkipping line \u001b[39m\u001b[39m{\u001b[39;00mrow_num\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mParserError\u001b[0m: Expected 2 fields in line 87, saw 3"
     ]
    }
   ],
   "source": [
    "processor.load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_dataset_splits()\n",
    "feature_extractor_model = ImageFeatureExtractor()\n",
    "\n",
    "cache_manager = ImageFeatureCacheManager(CONFIG, feature_extractor_model)\n",
    "image_name_to_cached_path_map = cache_manager.manage_feature_cache(processor.image_paths)\n",
    "final_train_data, final_val_data, final_test_data = processor.get_data_with_cached_features(image_name_to_cached_path_map)\n",
    "trainer = ImageCaptioningTrainer(CONFIG, processor, feature_extractor_model)\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train(final_train_data, processor.val_data)\n",
    "\n",
    "trainer.plot_history()\n",
    "print(\"\\n--- Final Evaluation on Test Set\")\n",
    "trainer.evaluate_bleu_score(processor.test_data)\n",
    "print(\"\\n--- Running a Demo\")\n",
    "if processor.test_data:\n",
    "    random_test_img_path, _ = random.choice(processor.test_data)\n",
    "    trainer.demo(os.path.basename(random_test_img_path))\n",
    "else:\n",
    "    print(\"No test data available for demo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
