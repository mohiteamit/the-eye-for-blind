{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, random, collections, tqdm\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "from gtts import gTTS\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, object] = {\n",
    "    'image_dir': '/home/flickr8k/Images',\n",
    "    'caption_file': '/home/flickr8k/captions.txt',\n",
    "    'feature_cache_dir': '/home/flickr8k/cache',\n",
    "    'num_examples': None,\n",
    "    'max_caption_length': 50,\n",
    "    'min_word_frequency': 5,\n",
    "\n",
    "    'embedding_dim': 256,\n",
    "    'units': 512,\n",
    "    'decoder_dropout': 0.5,\n",
    "\n",
    "    'learning_rate': 5e-5,\n",
    "    'epochs': 30,\n",
    "    'batch_size': 64,\n",
    "\n",
    "    'buffer_size': 1000,\n",
    "    'patience': 5,\n",
    "    'checkpoint_path': './checkpoints/lstm_attention_flickr8k',\n",
    "    'mixed_precision': True,\n",
    "\n",
    "    'attention_reg_lambda': 0.5,\n",
    "    'grad_clip_value': 5.0,\n",
    "    'scheduled_sampling_max_prob': 0.2,\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ENV SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "\n",
    "if CONFIG['mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"[AMP] mixed_float16 policy active\")\n",
    "else:\n",
    "    print(\"[AMP] disabled - using float32 throughout\")\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"Using GPU: {physical_devices[0].name} | batch={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"GPU not found - fallback to CPU\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer: Optional[Tokenizer] = None\n",
    "        self.img_to_cap_map: Dict[str, List[str]] = collections.defaultdict(list)\n",
    "        self.image_paths: List[str] = []\n",
    "        self.all_captions: List[str] = []\n",
    "        self.train_data: List[Tuple[str, List[int]]] = []\n",
    "        self.val_data: List[Tuple[str, List[int]]] = []\n",
    "        self.test_data: List[Tuple[str, List[int]]] = []\n",
    "        self.max_caption_length = 0\n",
    "        self.vocab_size = 0\n",
    "        self.num_steps_per_epoch = 0\n",
    "\n",
    "    def load_and_preprocess_data(self):\n",
    "        print(\"Loading and preprocessing captions...\")\n",
    "        df = pd.read_csv(self.config['caption_file'], engine='python')\n",
    "        df['image'] = df['image'].str.strip()\n",
    "        df['caption'] = df['caption'].str.strip()\n",
    "\n",
    "        temp_img_to_cap_map = collections.defaultdict(list)\n",
    "        all_unique_img_names_from_csv = df['image'].unique()\n",
    "\n",
    "        print(f\"Checking {len(all_unique_img_names_from_csv)} unique image files from CSV...\")\n",
    "        found_images_count = 0\n",
    "        \n",
    "        existing_image_files = set(os.listdir(self.config['image_dir']))\n",
    "\n",
    "        for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0], desc=\"Validating images & processing captions\"):\n",
    "            img_name = row['image']\n",
    "            caption = row['caption']\n",
    "            \n",
    "            if img_name in existing_image_files:\n",
    "                temp_img_to_cap_map[img_name].append(self.preprocess_text(caption))\n",
    "                if img_name not in self.img_to_cap_map:\n",
    "                    found_images_count += 1\n",
    "                self.img_to_cap_map[img_name] = temp_img_to_cap_map[img_name]\n",
    "            \n",
    "        if found_images_count < len(all_unique_img_names_from_csv):\n",
    "            print(f\"Warning: {len(all_unique_img_names_from_csv) - found_images_count} images mentioned in CSV were not found in {self.config['image_dir']}. They have been discarded.\")\n",
    "        \n",
    "        self.image_paths = sorted(list(self.img_to_cap_map.keys()))\n",
    "\n",
    "        if self.config['num_examples']:\n",
    "            if len(self.image_paths) > self.config['num_examples']:\n",
    "                self.image_paths = random.sample(self.image_paths, self.config['num_examples'])\n",
    "                self.img_to_cap_map = {img: self.img_to_cap_map[img] for img in self.image_paths}\n",
    "                print(f\"Using a subset of {len(self.image_paths)} images due to 'num_examples' config.\")\n",
    "\n",
    "        self.all_captions = []\n",
    "        for img_name in self.image_paths:\n",
    "            self.all_captions.extend(self.img_to_cap_map[img_name])\n",
    "\n",
    "        print(f\"Total valid images (with captions): {len(self.image_paths)}\")\n",
    "        print(f\"Total valid captions: {len(self.all_captions)}\")\n",
    "\n",
    "        self.tokenizer = Tokenizer(num_words=None, oov_token=\"<unk>\",\n",
    "                                   filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ',\n",
    "                                   lower=True)\n",
    "        self.tokenizer.fit_on_texts(self.all_captions)\n",
    "\n",
    "        word_counts = collections.Counter(word for caption in self.all_captions for word in caption.split())\n",
    "        filtered_word_index = {\n",
    "            word: index for word, index in self.tokenizer.word_index.items()\n",
    "            if word_counts[word] >= self.config['min_word_frequency'] or word in ['<pad>', '<start>', '<end>', '<unk>']\n",
    "        }\n",
    "        self.tokenizer.word_index = filtered_word_index\n",
    "        self.tokenizer.index_word = {v: k for k, v in filtered_word_index.items()}\n",
    "\n",
    "        if '<pad>' not in self.tokenizer.word_index:\n",
    "            self.tokenizer.word_index['<pad>'] = len(self.tokenizer.word_index) + 1\n",
    "            self.tokenizer.index_word[len(self.tokenizer.index_word) + 1] = '<pad>'\n",
    "            \n",
    "        self.vocab_size = len(self.tokenizer.word_index) + 1\n",
    "        print(f\"Vocabulary size after pruning (min_word_frequency={self.config['min_word_frequency']}): {self.vocab_size}\")\n",
    "\n",
    "        all_seqs = self.tokenizer.texts_to_sequences(self.all_captions)\n",
    "        self.max_caption_length = max(len(s) for s in all_seqs)\n",
    "        self.config['max_caption_length'] = self.max_caption_length\n",
    "        print(f\"Max caption length: {self.max_caption_length}\")\n",
    "        \n",
    "    def preprocess_text(self, caption: str) -> str:\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z ]\", \"\", caption)\n",
    "        caption = re.sub(r'\\s+', ' ', caption).strip()\n",
    "        caption = '<start> ' + caption + ' <end>'\n",
    "        return caption\n",
    "\n",
    "    def create_dataset_splits(self, train_ratio=0.8, val_ratio=0.1):\n",
    "        random.shuffle(self.image_paths)\n",
    "        num_images = len(self.image_paths)\n",
    "        num_train = int(train_ratio * num_images)\n",
    "        num_val = int(val_ratio * num_images)\n",
    "\n",
    "        train_image_paths = self.image_paths[:num_train]\n",
    "        val_image_paths = self.image_paths[num_train:num_train + num_val]\n",
    "        test_image_paths = self.image_paths[num_train + num_val:]\n",
    "\n",
    "        print(f\"Train images: {len(train_image_paths)}, Val images: {len(val_image_paths)}, Test images: {len(test_image_paths)}\")\n",
    "\n",
    "        self.train_data = self._create_pairs(train_image_paths)\n",
    "        self.val_data = self._create_pairs(val_image_paths)\n",
    "        self.test_data = self._create_pairs(test_image_paths)\n",
    "\n",
    "        print(f\"Train pairs: {len(self.train_data)}, Val pairs: {len(self.val_data)}, Test pairs: {len(self.test_data)}\")\n",
    "\n",
    "        self.num_steps_per_epoch = len(self.train_data) // self.config['batch_size']\n",
    "\n",
    "    def _create_pairs(self, image_names: List[str]) -> List[Tuple[str, List[int]]]:\n",
    "        pairs = []\n",
    "        for img_name in image_names:\n",
    "            full_img_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            for caption in self.img_to_cap_map[img_name]:\n",
    "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "                padded_seq = pad_sequences([seq], maxlen=self.max_caption_length, padding='post')[0]\n",
    "                pairs.append((full_img_path, list(padded_seq)))\n",
    "        return pairs\n",
    "\n",
    "    def get_data_with_cached_features(self, image_name_to_cached_path_map: Dict[str, str]) -> Tuple[List, List, List]:\n",
    "        def _reconstruct_list(data_list):\n",
    "            reconstructed = []\n",
    "            for original_img_path, caption_ids in data_list:\n",
    "                basename = os.path.basename(original_img_path)\n",
    "                cached_path = image_name_to_cached_path_map.get(basename)\n",
    "                if cached_path and os.path.exists(cached_path): # Added check if cached file actually exists\n",
    "                    reconstructed.append((original_img_path, cached_path, caption_ids))\n",
    "            return reconstructed\n",
    "\n",
    "        final_train = _reconstruct_list(self.train_data)\n",
    "        final_val = _reconstruct_list(self.val_data)\n",
    "        final_test = _reconstruct_list(self.test_data)\n",
    "\n",
    "        print(f\"Adjusted train pairs (after feature caching check): {len(final_train)}\")\n",
    "        print(f\"Adjusted val pairs (after feature caching check): {len(final_val)}\")\n",
    "        print(f\"Adjusted test pairs (after feature caching check): {len(final_test)}\")\n",
    "        \n",
    "        return final_train, final_val, final_test\n",
    "\n",
    "    def display_samples(self, num_samples: int = 5):\n",
    "        print(f\"\\n--- Displaying {num_samples} Random Dataset Samples ---\")\n",
    "\n",
    "        # Get a list of all image paths that have associated captions\n",
    "        available_image_paths = sorted(list(self.img_to_cap_map.keys()))\n",
    "\n",
    "        if not available_image_paths:\n",
    "            print(\"No image paths with captions available to display.\")\n",
    "            return\n",
    "\n",
    "        # Randomly select a few image names\n",
    "        samples_to_display = random.sample(available_image_paths, min(num_samples, len(available_image_paths)))\n",
    "\n",
    "        for i, img_name in enumerate(samples_to_display):\n",
    "            full_image_path = os.path.join(self.config['image_dir'], img_name)\n",
    "\n",
    "            if not os.path.exists(full_image_path):\n",
    "                print(f\"Warning: Image file not found for {img_name} at {full_image_path}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n--- Sample {i+1}/{num_samples} ---\")\n",
    "            print(f\"Image Name: {img_name}\")\n",
    "\n",
    "            try:\n",
    "                img = Image.open(full_image_path)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(img)\n",
    "                plt.title(f\"Image: {img_name}\")\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading or displaying image {img_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            gt_captions = self.img_to_cap_map.get(img_name, [])\n",
    "            print(\"Ground Truth Captions:\")\n",
    "            if gt_captions:\n",
    "                for j, cap in enumerate(gt_captions):\n",
    "                    # Clean up <start> and <end> tokens for display\n",
    "                    clean_cap = cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                    print(f\"  {j+1}. {clean_cap}\")\n",
    "            else:\n",
    "                print(\"  No ground truth captions available for this image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IMAGE FEATURE EXTRACTION (CACHE FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor(Model):\n",
    "    def __init__(self, target_size=(299, 299)):\n",
    "        super().__init__()\n",
    "        self.target_size = target_size\n",
    "        self.inception_v3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n",
    "        self.inception_v3.trainable = False\n",
    "        self.feature_extractor = Model(inputs=self.inception_v3.input,\n",
    "                                       outputs=self.inception_v3.get_layer('mixed7').output)\n",
    "\n",
    "    @tf.function\n",
    "    def load_and_preprocess_image(self, image_path: tf.Tensor) -> tf.Tensor:\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.target_size)\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageFeatureCacheManager:\n",
    "    def __init__(self, config, feature_extractor: ImageFeatureExtractor):\n",
    "        self.config = config\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.cache_dir = config['feature_cache_dir']\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "    def manage_feature_cache(self, image_names: List[str]) -> Dict[str, str]:\n",
    "        print(f\"\\nManaging image feature cache in: {self.cache_dir}\")\n",
    "        print(f\"Checking/extracting features for {len(image_names)} unique images.\")\n",
    "\n",
    "        image_name_to_cached_path = {}\n",
    "        images_to_extract = []\n",
    "\n",
    "        for img_name in image_names:\n",
    "            cache_file = os.path.join(self.cache_dir, img_name + '.npy')\n",
    "            if not os.path.exists(cache_file):\n",
    "                images_to_extract.append(img_name)\n",
    "            image_name_to_cached_path[img_name] = cache_file\n",
    "\n",
    "        if images_to_extract:\n",
    "            print(f\"Found {len(images_to_extract)} images whose features need extraction...\")\n",
    "            \n",
    "            full_paths_for_extraction = [os.path.join(self.config['image_dir'], img_name) for img_name in images_to_extract]\n",
    "\n",
    "            batch_size_extraction = 16 # Adjust based on your GPU memory\n",
    "            for i in tqdm.tqdm(range(0, len(full_paths_for_extraction), batch_size_extraction), desc=\"Extracting & Caching Features\"):\n",
    "                batch_paths = full_paths_for_extraction[i:i+batch_size_extraction]\n",
    "                \n",
    "                try:\n",
    "                    img_tensors_processed = tf.stack([self.feature_extractor.load_and_preprocess_image(tf.constant(p)) for p in batch_paths])\n",
    "                    \n",
    "                    features_batch = self.feature_extractor.feature_extractor(img_tensors_processed)\n",
    "                    \n",
    "                    # Reshape to (batch_size, 17*17, 768)\n",
    "                    features_flat_batch = tf.reshape(features_batch, (tf.shape(features_batch)[0], -1, tf.shape(features_batch)[3]))\n",
    "                    \n",
    "                    for j, img_path in enumerate(batch_paths):\n",
    "                        img_name = os.path.basename(img_path)\n",
    "                        cache_path = os.path.join(self.cache_dir, img_name + '.npy')\n",
    "                        # Save each image's features separately as (289, 768)\n",
    "                        np.save(cache_path, features_flat_batch[j].numpy())\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError processing a batch during feature extraction: {e}. Skipping images in this batch.\")\n",
    "                    for img_path_in_batch in batch_paths:\n",
    "                        img_name_in_batch = os.path.basename(img_path_in_batch)\n",
    "                        # Remove from map if feature extraction failed, so it's not used\n",
    "                        if img_name_in_batch in image_name_to_cached_path:\n",
    "                            del image_name_to_cached_path[img_name_in_batch]\n",
    "                    continue\n",
    "        else:\n",
    "            print(\"All image features already cached. Skipping extraction.\")\n",
    "            \n",
    "        print(\"Image feature cache management complete.\")\n",
    "        return image_name_to_cached_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights * features\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, tf.squeeze(attention_weights, -1)\n",
    "\n",
    "class Decoder(Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size, dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = layers.LSTM(self.units,\n",
    "                                 return_sequences=True,\n",
    "                                 return_state=True,\n",
    "                                 recurrent_initializer='glorot_uniform',\n",
    "                                 dropout=dropout)\n",
    "\n",
    "        self.fc1 = layers.Dense(self.units)\n",
    "        self.fc2 = layers.Dense(vocab_size, dtype='float32')\n",
    "\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, features, hidden_state, cell_state):\n",
    "        context_vector, attention_weights = self.attention(features, hidden_state)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        if tf.keras.mixed_precision.global_policy().compute_dtype == 'float16':\n",
    "            hidden_state = tf.cast(hidden_state, tf.float16)\n",
    "            cell_state = tf.cast(cell_state, tf.float16)\n",
    "            x = tf.cast(x, tf.float16)\n",
    "\n",
    "        output, new_hidden_state, new_cell_state = self.lstm(x, initial_state=[hidden_state, cell_state])\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        x = self.fc1(output)\n",
    "\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return logits, new_hidden_state, new_cell_state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TEXT-TO-SPEECH UTILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToSpeech:\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.gTTS = gTTS\n",
    "            self.Audio = Audio\n",
    "            self.display = display\n",
    "            self.available = True\n",
    "        except ImportError:\n",
    "            print(\"WARNING: gTTS or IPython.display not found. Speech functionality will be disabled.\")\n",
    "            self.available = False\n",
    "\n",
    "    def speak(self, text: str, filename: str = \"caption_audio.mp3\"):\n",
    "        if not self.available:\n",
    "            print(\"Text-to-speech functionality is not available. Please install 'gtts' and ensure running in an IPython environment.\")\n",
    "            return\n",
    "        \n",
    "        if not text.strip():\n",
    "            print(\"Empty text, nothing to speak.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            tts = self.gTTS(text=text, lang='en')\n",
    "            tts.save(filename)\n",
    "            self.display(self.Audio(filename))\n",
    "            print(f\"Audio saved to {filename} and played.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating or playing audio: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TRAINING LOOP & UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioning:\n",
    "    def __init__(self, config, processor: DataProcessor, feature_extractor: ImageFeatureExtractor):\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        self.encoder = Encoder(self.config['embedding_dim'])\n",
    "        self.decoder = Decoder(self.config['embedding_dim'], self.config['units'],\n",
    "                               self.processor.vocab_size, self.config['decoder_dropout'])\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate'])\n",
    "        if self.config['mixed_precision']:\n",
    "            self.optimizer = tf.keras.mixed_precision.LossScaleOptimizer(self.optimizer)\n",
    "\n",
    "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "\n",
    "        self.checkpoint_prefix = os.path.join(self.config['checkpoint_path'], \"ckpt\")\n",
    "        self.checkpoint = tf.train.Checkpoint(encoder=self.encoder,\n",
    "                                             decoder=self.decoder,\n",
    "                                             optimizer=self.optimizer)\n",
    "        self.checkpoint_manager = tf.train.CheckpointManager(self.checkpoint, self.config['checkpoint_path'], max_to_keep=5)\n",
    "\n",
    "        self.tts_speaker = TextToSpeech()\n",
    "\n",
    "        self.train_loss_results = []\n",
    "        self.val_bleu_results = []\n",
    "        self.best_val_bleu = 0.0\n",
    "        self.smoothing_function = SmoothingFunction().method4\n",
    "        self.patience_counter = 0\n",
    "\n",
    "\n",
    "        if self.checkpoint_manager.latest_checkpoint:\n",
    "            self.checkpoint.restore(self.checkpoint_manager.latest_checkpoint)\n",
    "            print(f\"Restored from {self.checkpoint_manager.latest_checkpoint}\")\n",
    "        else:\n",
    "            print(\"Initializing from scratch.\")\n",
    "\n",
    "    def loss_function(self, real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0)) # Mask out padded tokens (0)\n",
    "        loss_ = self.loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    def evaluate_bleu_score(self, dataset_pairs: List[Tuple[str, List[int]]], num_samples=None):\n",
    "        references = []\n",
    "        hypotheses = []\n",
    "\n",
    "        if num_samples is None:\n",
    "            samples_to_evaluate = dataset_pairs\n",
    "        else:\n",
    "            samples_to_evaluate = random.sample(dataset_pairs, min(num_samples, len(dataset_pairs)))\n",
    "\n",
    "        print(f\"\\nEvaluating BLEU on {len(samples_to_evaluate)} samples...\")\n",
    "        for img_path, _ in tqdm.tqdm(samples_to_evaluate):\n",
    "            # Try greedy inference first\n",
    "            generated_caption_tokens = self.greedy_inference(img_path)\n",
    "            \n",
    "            # Fallback to beam search if greedy fails or produces empty string, or if it's generally preferred\n",
    "            if not generated_caption_tokens: # Can replace with `if True:` to always use beam search\n",
    "                generated_caption_tokens, _ = self.beam_search_inference(img_path, beam_size=3)\n",
    "\n",
    "            if not generated_caption_tokens:\n",
    "                continue # Skip if no caption could be generated\n",
    "\n",
    "            hypotheses.append(generated_caption_tokens)\n",
    "\n",
    "            img_name = os.path.basename(img_path)\n",
    "            raw_captions = self.processor.img_to_cap_map.get(img_name, [])\n",
    "            \n",
    "            img_references = []\n",
    "            for raw_cap in raw_captions:\n",
    "                # Remove <start> and <end> tokens for BLEU calculation\n",
    "                cleaned_cap = raw_cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                if cleaned_cap:\n",
    "                    img_references.append(cleaned_cap.split())\n",
    "            \n",
    "            if img_references:\n",
    "                references.append(img_references)\n",
    "            else:\n",
    "                # If no valid reference captions, remove the corresponding hypothesis\n",
    "                hypotheses.pop()\n",
    "\n",
    "        if not references:\n",
    "            print(\"No valid reference captions found for BLEU evaluation after filtering.\")\n",
    "            return {\"bleu-1\": 0.0, \"bleu-2\": 0.0, \"bleu-3\": 0.0, \"bleu-4\": 0.0}\n",
    "\n",
    "        bleu_scores = {}\n",
    "        for n in range(1, 5):\n",
    "            weights = (1.0 / n,) * n + (0.0,) * (4 - n)\n",
    "            bleu_scores[f\"bleu-{n}\"] = corpus_bleu(references, hypotheses, weights=weights,\n",
    "                                                   smoothing_function=self.smoothing_function)\n",
    "            print(f\"BLEU-{n}: {bleu_scores[f'bleu-{n}']:.4f}\")\n",
    "            \n",
    "        return bleu_scores\n",
    "\n",
    "    def greedy_inference(self, image_path: str):\n",
    "        filename = os.path.basename(image_path)\n",
    "        feature_cache_path = os.path.join(self.config['feature_cache_dir'], filename + '.npy')\n",
    "        \n",
    "        if not os.path.exists(feature_cache_path):\n",
    "            print(f\"Error: Feature cache not found for {image_path}\")\n",
    "            return []\n",
    "\n",
    "        img_features = np.load(feature_cache_path)\n",
    "        # Ensure img_features is cast to the correct dtype for inference\n",
    "        img_features_tensor = tf.convert_to_tensor(img_features, dtype=tf.float32) # Always load as float32\n",
    "\n",
    "        # Add a batch dimension of 1 for inference\n",
    "        img_features_tensor = tf.expand_dims(img_features_tensor, 0) # Shape: (1, 289, 768)\n",
    "\n",
    "        features = self.encoder(img_features_tensor) # Encoder output: (1, 289, embedding_dim)\n",
    "\n",
    "        hidden = tf.zeros((1, self.config['units']), dtype=tf.float32)\n",
    "        cell = tf.zeros((1, self.config['units']), dtype=tf.float32)\n",
    "\n",
    "        dec_input = tf.expand_dims([self.processor.tokenizer.word_index['<start>']], 0) # Shape: (1, 1)\n",
    "\n",
    "        result = []\n",
    "        for i in range(self.config['max_caption_length']):\n",
    "            predictions, hidden, cell, _ = self.decoder(dec_input, features, hidden, cell)\n",
    "            predicted_id = tf.argmax(predictions[0]).numpy() # predictions[0] converts from (1, vocab_size) to (vocab_size,)\n",
    "            predicted_word = self.processor.tokenizer.index_word.get(predicted_id, '<unk>')\n",
    "\n",
    "            if predicted_word == '<end>':\n",
    "                break\n",
    "            # Only append valid words, not special tokens\n",
    "            if predicted_word not in ('<unk>', '<start>', '<pad>'):\n",
    "                result.append(predicted_word)\n",
    "\n",
    "            dec_input = tf.expand_dims([predicted_id], 0) # Next input for decoder: (1, 1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def beam_search_inference(self, image_path: str, beam_size: int = 3, length_penalty_weight: float = 0.7):\n",
    "            filename = os.path.basename(image_path)\n",
    "            feature_cache_path = os.path.join(self.config['feature_cache_dir'], filename + '.npy')\n",
    "            if not os.path.exists(feature_cache_path):\n",
    "                print(f\"Error: Feature cache not found for {image_path}\")\n",
    "                return [], []\n",
    "\n",
    "            img_features = np.load(feature_cache_path)\n",
    "            img_features_tensor = tf.convert_to_tensor(img_features, dtype=tf.float32)\n",
    "            \n",
    "            img_features_tensor = tf.expand_dims(img_features_tensor, 0)\n",
    "\n",
    "            features = self.encoder(img_features_tensor)\n",
    "\n",
    "            start_token = self.processor.tokenizer.word_index['<start>']\n",
    "            end_token = self.processor.tokenizer.word_index['<end>']\n",
    "\n",
    "            # Each beam is (sequence_of_token_ids, log_probability, hidden_state, cell_state, list_of_attention_weights)\n",
    "            beams = [(\n",
    "                [start_token],\n",
    "                0.0,\n",
    "                tf.zeros((1, self.config['units']), dtype=tf.float32),\n",
    "                tf.zeros((1, self.config['units']), dtype=tf.float32),\n",
    "                []\n",
    "            )]\n",
    "\n",
    "            completed_beams = []\n",
    "\n",
    "            for _ in range(self.config['max_caption_length']):\n",
    "                new_beams = []\n",
    "                for seq, score, hidden, cell, alphas in beams:\n",
    "                    last_token = seq[-1]\n",
    "\n",
    "                    if last_token == end_token:\n",
    "                        completed_beams.append((seq, score, alphas))\n",
    "                        continue\n",
    "\n",
    "                    dec_input = tf.expand_dims([last_token], 0)\n",
    "                    \n",
    "                    predictions, new_hidden, new_cell, attention_weights = self.decoder(dec_input, features, hidden, cell)\n",
    "\n",
    "                    predictions = tf.cast(predictions[0], tf.float32)\n",
    "                    log_probs = tf.nn.log_softmax(predictions).numpy()\n",
    "\n",
    "                    top_k_indices = np.argsort(log_probs)[::-1][:beam_size]\n",
    "\n",
    "                    for idx in top_k_indices:\n",
    "                        token_id = int(idx)\n",
    "                        token_log_prob = float(log_probs[idx])\n",
    "                        \n",
    "                        new_beams.append((\n",
    "                            seq + [token_id],\n",
    "                            score + token_log_prob,\n",
    "                            new_hidden,\n",
    "                            new_cell,\n",
    "                            alphas + [attention_weights[0].numpy()]\n",
    "                        ))\n",
    "                \n",
    "                combined_beams = completed_beams + new_beams\n",
    "                combined_beams.sort(key=lambda x: x[1] / ((len(x[0]) ** length_penalty_weight) if len(x[0]) > 0 else 1.0), reverse=True)\n",
    "                beams = combined_beams[:beam_size]\n",
    "                \n",
    "                final_active_beams = []\n",
    "                for item in beams:\n",
    "                    if len(item) == 5: # It's an active beam\n",
    "                        seq, score, _, _, alphas = item\n",
    "                        if seq[-1] != end_token:\n",
    "                            final_active_beams.append((seq, score, alphas))\n",
    "                    elif len(item) == 3: # It's already a completed beam\n",
    "                        completed_beams.append(item) # Re-add it if it was previously considered completed\n",
    "\n",
    "                temp_active_beams = []\n",
    "                for seq, score, hidden, cell, alphas in new_beams: # This `new_beams` contains 5-tuples from current step\n",
    "                    if seq[-1] == end_token:\n",
    "                        completed_beams.append((seq, score, alphas))\n",
    "                    else:\n",
    "                        temp_active_beams.append((seq, score, hidden, cell, alphas))\n",
    "\n",
    "                # Select top K from the current pool of active beams and sort them.\n",
    "                # This ensures `beams` always contains 5-element tuples.\n",
    "                temp_active_beams.sort(key=lambda x: x[1] / ((len(x[0]) ** length_penalty_weight) if len(x[0]) > 0 else 1.0), reverse=True)\n",
    "                beams = temp_active_beams[:beam_size]\n",
    "\n",
    "                # Early stopping check: if all top K beams are completed, we can stop\n",
    "                if all(b[0][-1] == end_token for b in beams):\n",
    "                    completed_beams.extend([(b[0], b[1], b[4]) for b in beams]) # Add remaining completed ones\n",
    "                    break\n",
    "\n",
    "            completed_beams.extend([(seq, score, alphas) for seq, score, hidden, cell, alphas in beams if seq[-1] != end_token])\n",
    "\n",
    "\n",
    "            if not completed_beams:\n",
    "                return [], []\n",
    "\n",
    "            # Select the best beam (highest score with length penalty) from completed_beams\n",
    "            best_seq, best_score, best_alphas = max(completed_beams, key=lambda x: x[1] / ((len(x[0]) ** length_penalty_weight) if len(x[0]) > 0 else 1.0))\n",
    "\n",
    "            caption_words = [self.processor.tokenizer.index_word.get(i, '<unk>') for i in best_seq]\n",
    "            \n",
    "            filtered_caption_words = [\n",
    "                word for word in caption_words\n",
    "                if word not in ['<start>', '<end>', '<pad>', '<unk>']\n",
    "            ]\n",
    "\n",
    "            return filtered_caption_words, best_alphas\n",
    "\n",
    "    def plot_history(self):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_results, label='Train Loss')\n",
    "        plt.title('Training Loss per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_bleu_results, label='Validation BLEU-4')\n",
    "        plt.title('Validation BLEU-4 Score per Epoch')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4 Score')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_attention(self, image_path: str, caption: List[str], alphas: List[np.ndarray]):\n",
    "            img = Image.open(image_path)\n",
    "            img = np.array(img.resize((299, 299)))\n",
    "\n",
    "            num_words = len(caption)\n",
    "            cols = min(5, num_words)\n",
    "            rows = (num_words + cols - 1) // cols\n",
    "\n",
    "            fig_width = cols * 4\n",
    "            fig_height = rows * 4 + 2\n",
    "\n",
    "            fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "            fig.subplots_adjust(wspace=0.1, hspace=0.3)\n",
    "\n",
    "            for t in range(num_words):\n",
    "                if t >= (rows * cols): \n",
    "                    break\n",
    "                \n",
    "                ax = fig.add_subplot(rows, cols, t + 1)\n",
    "                ax.imshow(img)\n",
    "                ax.axis('off')\n",
    "\n",
    "                alpha = np.array(alphas[t])\n",
    "                attention_grid_size = int(np.sqrt(alpha.shape[0]))\n",
    "                alpha_reshaped = alpha.reshape(attention_grid_size, attention_grid_size)\n",
    "\n",
    "                alpha_resized = Image.fromarray(np.uint8(255 * alpha_reshaped)).resize(\n",
    "                    (299, 299), resample=Image.Resampling.BICUBIC\n",
    "                )\n",
    "                alpha_resized = np.array(alpha_resized) / 255.0\n",
    "\n",
    "                # Use a different colormap like 'jet' or 'viridis' for better visibility over dark images.\n",
    "                # Decrease the alpha value to make the original image more visible underneath.\n",
    "                ax.imshow(alpha_resized, cmap='jet', alpha=0.4, extent=(0, 299, 299, 0))\n",
    "                ax.set_title(f\"{t+1}: '{caption[t]}'\", fontsize=12, color='blue', va='bottom')\n",
    "\n",
    "            plt.suptitle(f\"Attention Map for: {os.path.basename(image_path)}\", fontsize=16, y=0.98)\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.show()\n",
    "\n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        self.tts_speaker.speak(caption, filename)\n",
    "\n",
    "        def demo(self, image_file_name: str):\n",
    "            full_image_path = os.path.join(self.config['image_dir'], image_file_name)\n",
    "            \n",
    "            if not os.path.exists(full_image_path):\n",
    "                print(f\"Error: Image not found at {full_image_path}\")\n",
    "                return\n",
    "\n",
    "            img = Image.open(full_image_path)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Image: {image_file_name}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "            gt_captions = self.processor.img_to_cap_map.get(image_file_name, [])\n",
    "            print(\"\\nGround Truth Captions:\")\n",
    "            if gt_captions:\n",
    "                for i, cap in enumerate(gt_captions):\n",
    "                    clean_cap = cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                    print(f\"  {i+1}. {clean_cap}\")\n",
    "            else:\n",
    "                print(\"  No ground truth captions available.\")\n",
    "            \n",
    "            generated_caption_words, attention_weights = self.beam_search_inference(full_image_path, beam_size=3)\n",
    "            generated_caption = \" \".join(generated_caption_words)\n",
    "            print(f\"\\nGenerated Caption (Beam Search): {generated_caption}\")\n",
    "\n",
    "            print(\"\\nPlaying generated caption:\")\n",
    "            self.speak_caption(generated_caption, filename=f\"caption_audio_{os.path.basename(image_file_name).split('.')[0]}.mp3\")\n",
    "\n",
    "            if generated_caption_words and attention_weights:\n",
    "                self.plot_attention(full_image_path, generated_caption_words, attention_weights)\n",
    "            else:\n",
    "                print(\"Could not generate caption or attention for plotting.\")\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(self, img_tensor, target):\n",
    "        batch_size = tf.shape(target)[0]\n",
    "        loss = 0.0\n",
    "\n",
    "        current_policy = tf.keras.mixed_precision.global_policy()\n",
    "        if current_policy.name == 'mixed_float16':\n",
    "            initial_state_dtype = tf.float16\n",
    "        else:\n",
    "            initial_state_dtype = tf.float32\n",
    "\n",
    "        hidden = tf.zeros((batch_size, self.config['units']), dtype=initial_state_dtype)\n",
    "        cell = tf.zeros((batch_size, self.config['units']), dtype=initial_state_dtype)\n",
    "\n",
    "        dec_input = tf.fill([batch_size, 1], self.processor.tokenizer.word_index['<start>'])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.encoder(img_tensor)\n",
    "\n",
    "            attention_sum_square_error = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "            for i in tf.range(1, target.shape[1]):\n",
    "                predictions, hidden, cell, attention_weights = self.decoder(dec_input, features, hidden, cell)\n",
    "\n",
    "                loss += self.loss_function(target[:, i], predictions)\n",
    "\n",
    "                if self.config.get('scheduled_sampling_max_prob', 0.0) > 0:\n",
    "                    random_probs = tf.random.uniform([batch_size], 0, 1, dtype=tf.float32)\n",
    "                    \n",
    "                    threshold = tf.fill([batch_size], self.current_scheduled_sampling_prob)\n",
    "                    \n",
    "                    use_predicted_mask = random_probs < threshold\n",
    "                    \n",
    "                    predicted_ids = tf.argmax(predictions, axis=1, output_type=tf.int32)\n",
    "                    \n",
    "                    true_ids = target[:, i]\n",
    "                    \n",
    "                    dec_input = tf.where(use_predicted_mask, predicted_ids, true_ids)\n",
    "                    \n",
    "                    dec_input = tf.expand_dims(dec_input, 1)\n",
    "                else:\n",
    "                    dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "                attention_sum_square_error += tf.reduce_mean(\n",
    "                    tf.square(tf.cast(tf.reduce_sum(attention_weights, axis=1), tf.float32) - 1.0)\n",
    "                )\n",
    "\n",
    "            total_loss = (loss / tf.cast(target.shape[1] - 1, tf.float32))\n",
    "            total_loss += self.config['attention_reg_lambda'] * attention_sum_square_error / tf.cast(target.shape[1] - 1, tf.float32)\n",
    "\n",
    "            if self.config['mixed_precision']:\n",
    "                scaled_loss = self.optimizer.get_scaled_loss(total_loss)\n",
    "            else:\n",
    "                scaled_loss = total_loss\n",
    "\n",
    "        trainable_variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        gradients = tape.gradient(scaled_loss, trainable_variables)\n",
    "\n",
    "        if self.config['mixed_precision']:\n",
    "            gradients = self.optimizer.get_unscaled_gradients(gradients)\n",
    "\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, train_cached_paths: List[Tuple[str, str, List[int]]],\n",
    "              val_dataset_pairs: List[Tuple[str, List[int]]]):\n",
    "        \n",
    "        def _data_generator():\n",
    "            for orig_img_path, cache_path, caption_ids in train_cached_paths:\n",
    "                features = np.load(cache_path)\n",
    "                features = tf.cast(features, tf.float32)\n",
    "                yield features, np.array(caption_ids, dtype=np.int32)\n",
    "\n",
    "        train_dataset = tf.data.Dataset.from_generator(\n",
    "            _data_generator,\n",
    "            output_signature=(\n",
    "                tf.TensorSpec(shape=(289, 768), dtype=tf.float32),\n",
    "                tf.TensorSpec(shape=(self.config['max_caption_length'],), dtype=tf.int32)\n",
    "            )\n",
    "        )\n",
    "        train_dataset = train_dataset.shuffle(self.config['buffer_size']).batch(self.config['batch_size'])\n",
    "        train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "        for epoch in range(self.config['epochs']):\n",
    "            start = time.time()\n",
    "            total_loss = 0\n",
    "\n",
    "            self.current_scheduled_sampling_prob = (\n",
    "                self.config['scheduled_sampling_max_prob'] * (epoch / max(1, self.config['epochs'] - 1))\n",
    "            )\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.config['epochs']} (Scheduled Sampling Prob: {self.current_scheduled_sampling_prob:.3f})\")\n",
    "\n",
    "            self.processor.num_steps_per_epoch = len(train_cached_paths) // self.config['batch_size']\n",
    "            if self.processor.num_steps_per_epoch == 0:\n",
    "                print(\"Warning: num_steps_per_epoch is 0. Batch size might be too large or dataset too small. Setting to 1.\")\n",
    "                self.processor.num_steps_per_epoch = 1\n",
    "\n",
    "            for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "                batch_loss = self.train_step(img_tensor, target)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    print(f'Epoch {epoch+1} Batch {batch} Loss {batch_loss.numpy():.4f}')\n",
    "            \n",
    "            avg_train_loss = total_loss / self.processor.num_steps_per_epoch\n",
    "            self.train_loss_results.append(avg_train_loss.numpy())\n",
    "            print(f'Epoch {epoch+1} Loss {avg_train_loss:.4f}')\n",
    "\n",
    "            val_bleu_scores = self.evaluate_bleu_score(val_dataset_pairs, num_samples=min(1000, len(val_dataset_pairs)))\n",
    "            current_val_bleu4 = val_bleu_scores.get('bleu-4', 0.0)\n",
    "            self.val_bleu_results.append(current_val_bleu4)\n",
    "\n",
    "            if current_val_bleu4 > self.best_val_bleu:\n",
    "                self.best_val_bleu = current_val_bleu4\n",
    "                self.checkpoint_manager.save()\n",
    "                print(f\"Saving checkpoint at epoch {epoch+1} with BLEU-4: {current_val_bleu4:.4f}\")\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                print(f\"BLEU-4 not improved. Patience counter: {self.patience_counter}/{self.config['patience']}\")\n",
    "                if self.patience_counter >= self.config['patience']:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
    "                    break\n",
    "\n",
    "            print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs')\n",
    "\n",
    "    def demo(self, image_file_name: str):\n",
    "        full_image_path = os.path.join(self.config['image_dir'], image_file_name)\n",
    "        \n",
    "        if not os.path.exists(full_image_path):\n",
    "            print(f\"Error: Image not found at {full_image_path}\")\n",
    "            return\n",
    "\n",
    "        img = Image.open(full_image_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Image: {image_file_name}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        gt_captions = self.processor.img_to_cap_map.get(image_file_name, [])\n",
    "        print(\"\\nGround Truth Captions:\")\n",
    "        if gt_captions:\n",
    "            for i, cap in enumerate(gt_captions):\n",
    "                clean_cap = cap.replace('<start>', '').replace('<end>', '').strip()\n",
    "                print(f\"  {i+1}. {clean_cap}\")\n",
    "        else:\n",
    "            print(\"  No ground truth captions available.\")\n",
    "        \n",
    "        generated_caption_words, attention_weights = self.beam_search_inference(full_image_path, beam_size=3)\n",
    "        generated_caption = \" \".join(generated_caption_words)\n",
    "        print(f\"\\nGenerated Caption (Beam Search): {generated_caption}\")\n",
    "\n",
    "        print(\"\\nPlaying generated caption:\")\n",
    "        self.speak_caption(generated_caption, filename=f\"caption_audio_{os.path.basename(image_file_name).split('.')[0]}.mp3\")\n",
    "\n",
    "        if generated_caption_words and attention_weights:\n",
    "            self.plot_attention(full_image_path, generated_caption_words, attention_weights)\n",
    "        else:\n",
    "            print(\"Could not generate caption or attention for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.display_samples(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.create_dataset_splits(train_ratio=0.85, val_ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model = ImageFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_manager = ImageFeatureCacheManager(CONFIG, feature_extractor_model)\n",
    "image_name_to_cached_path_map = cache_manager.manage_feature_cache(processor.image_paths)\n",
    "final_train_data, final_val_data, final_test_data = processor.get_data_with_cached_features(image_name_to_cached_path_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = ImageCaptioning(CONFIG, processor, feature_extractor_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(final_train_data, processor.val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate_bleu_score(processor.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processor.test_data:\n",
    "    random_test_img_path, _ = random.choice(processor.test_data)\n",
    "    trainer.demo(os.path.basename(random_test_img_path))\n",
    "else:\n",
    "    print(\"No test data available for demo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processor.test_data:\n",
    "    random_test_img_path, _ = random.choice(processor.test_data)\n",
    "    trainer.demo(os.path.basename(random_test_img_path))\n",
    "else:\n",
    "    print(\"No test data available for demo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
