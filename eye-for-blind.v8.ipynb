{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "# 0 (default): All messages (INFO, WARNING, ERROR) are logged.\n",
    "# 1: INFO messages are not printed.\n",
    "# 2: INFO and WARNING messages are not printed.\n",
    "# 3: INFO, WARNING, and ERROR messages are not printed.\n",
    "\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import tensorflow as tf #type: ignore\n",
    "from tensorflow.keras import layers, Model #type: ignore\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay #type: ignore\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy #type: ignore\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #type: ignore\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #type: ignore\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction #type: ignore\n",
    "from gtts import gTTS #type: ignore\n",
    "from IPython.display import Audio, display\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, object] = {\n",
    "    # Data\n",
    "    'image_dir': '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file': '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    'subset_ratio': 1.0,\n",
    "\n",
    "    # Vocabulary\n",
    "    'vocab_min_count': 5,\n",
    "    'max_length': 30,\n",
    "\n",
    "    # Model\n",
    "    'embedding_dim': 512,\n",
    "    'units': 1024,\n",
    "    'decoder_dropout': 0.3,\n",
    "\n",
    "    # Optimiser / schedule\n",
    "    'learning_rate': 5e-4,\n",
    "    'grad_clip_value': 10.0,\n",
    "    'scheduled_sampling_max_prob': 0.15,\n",
    "    'mixed_precision': True,  # ← flip to True for AMP\n",
    "\n",
    "    # Training loop\n",
    "    'epochs': 30,\n",
    "    'batch_size': 128,\n",
    "    'buffer_size': 10_000,\n",
    "    'early_stop': True,\n",
    "    'patience': 20,\n",
    "\n",
    "    # Checkpoints\n",
    "    'checkpoint_dir': './checkpoints/split_by_image',\n",
    "    'save_checkpoints': True,\n",
    "    'delete_old_checkpoints': True,\n",
    "\n",
    "    # Regulariser\n",
    "    'attention_reg_lambda': 1.0,\n",
    "    'seed': 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ENV‑SETUP & MIXED‑PRECISION POLICY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "\n",
    "# AMP policy (RTX 6000 Ada or any Tensor Core GPU)\n",
    "if CONFIG['mixed_precision']:\n",
    "    try:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Mixed precision requires TensorFlow ≥ 2.4\")\n",
    "\n",
    "    policy = mixed_precision.Policy('mixed_float16')\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print(\"[AMP] mixed_float16 policy active ✨\")\n",
    "else:\n",
    "    print(\"[AMP] disabled – using float32 throughout\")\n",
    "\n",
    "# Device placement (single‑GPU)\n",
    "physical = tf.config.list_physical_devices('GPU')\n",
    "if physical:\n",
    "    for gpu in physical:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(f\"Using {physical[0].name} | batch={CONFIG['batch_size']}\")\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"GPU not found – fallback to CPU\")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.captions_dict: Dict[str, List[str]] = {}\n",
    "        self.tokenizer: Optional[Tokenizer] = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_data: List[Tuple[str, str]] = []\n",
    "        self.val_data:   List[Tuple[str, str]] = []\n",
    "        self.test_data:  List[Tuple[str, str]] = []\n",
    "    \n",
    "    def load_captions(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Load and convert pipe-delimited Flickr-style caption file to a dict.\"\"\"\n",
    "        print(f\"Loading captions from {self.config['caption_file']}\")\n",
    "        df = pd.read_csv(self.config['caption_file'], sep='|', header=None, \n",
    "                         names=['image_name', 'comment_number', 'comment'], engine='python')\n",
    "        df['image_name'] = df['image_name'].str.strip()\n",
    "        df['comment'] = df['comment'].str.strip()\n",
    "        \n",
    "        caption_map = {}\n",
    "        for img, group in df.groupby('image_name'):\n",
    "            caption_map[img] = group['comment'].tolist()\n",
    "        \n",
    "        self.captions_dict = caption_map\n",
    "        print(f\"Loaded {len(caption_map)} images with captions\")\n",
    "        return caption_map\n",
    "    \n",
    "    def display_samples(self, num_samples: int = 3):\n",
    "        \"\"\"Display random images with all their associated captions.\"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        sample_keys = random.sample(list(self.captions_dict.keys()), min(num_samples, len(self.captions_dict)))\n",
    "\n",
    "        for key in sample_keys:\n",
    "            img_path = os.path.join(self.config['image_dir'], key)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.imshow(img)\n",
    "                plt.axis('off')\n",
    "                plt.title(key)\n",
    "                plt.show()\n",
    "\n",
    "                for cap in self.captions_dict[key]:\n",
    "                    print(f\"- {cap}\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image {key}: {e}\")\n",
    "\n",
    "    def preprocess_caption(self, caption: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean and tag a single caption.\n",
    "\n",
    "        Returns the processed caption string, or an empty string if the\n",
    "        source caption is None / NaN / not a string.\n",
    "        \"\"\"\n",
    "        if caption is None or not isinstance(caption, str) or caption.strip() == \"\":\n",
    "            return \"\"\n",
    "        caption = caption.lower()\n",
    "        caption = re.sub(r\"[^a-z0-9.,? ]\", \"\", caption)\n",
    "        return f\"<start> {caption.strip()} <end>\"\n",
    "\n",
    "    def prepare_captions(self, subset_ratio: float = 1.0):\n",
    "        \"\"\"\n",
    "        Build tokenizer, prune vocab, then create\n",
    "        (image, caption) pairs and **image-wise** splits.\n",
    "        \"\"\"\n",
    "        if not self.captions_dict:\n",
    "            self.load_captions()\n",
    "\n",
    "        # ----- 1. clean + tag all captions -----------------------------------\n",
    "        all_captions = []\n",
    "        for caps in self.captions_dict.values():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p:\n",
    "                    all_captions.append(p)\n",
    "\n",
    "        # ----- 2. vocabulary pruning ----------------------------------------\n",
    "        word_counts = Counter(w for cap in all_captions for w in cap.split())\n",
    "        valid_words = {w for w, cnt in word_counts.items()\n",
    "                       if cnt >= self.config['vocab_min_count']}\n",
    "\n",
    "        def keep(cap: str) -> bool:\n",
    "            return all(w in valid_words or w in ('<start>', '<end>')\n",
    "                       for w in cap.split())\n",
    "\n",
    "        filtered = [c for c in all_captions if keep(c)]\n",
    "\n",
    "        # ----- 3. max caption length (95-th percentile) ----------------------\n",
    "        lengths = [len(c.split()) for c in filtered]\n",
    "        self.config['max_length'] = int(np.percentile(lengths, 95))\n",
    "        print(f\"max_length set to {self.config['max_length']}\")\n",
    "\n",
    "        # ----- 4. tokenizer ---------------------------------------------------\n",
    "        tok = Tokenizer(oov_token=\"<unk>\", filters='', lower=True)\n",
    "        tok.fit_on_texts(filtered)\n",
    "        self.tokenizer = tok\n",
    "        self.vocab_size = len(tok.word_index) + 1\n",
    "        print(f\"vocab size = {self.vocab_size}\")\n",
    "\n",
    "        # ----- 5. build ALL (img, caption) pairs ------------------------------\n",
    "        pairs = []\n",
    "        for img, caps in self.captions_dict.items():\n",
    "            for c in caps:\n",
    "                p = self.preprocess_caption(c)\n",
    "                if p and keep(p):\n",
    "                    pairs.append((img, p))\n",
    "\n",
    "        # ----- 6. optional hard subset of images -----------------------------\n",
    "        all_imgs = list({img for img, _ in pairs})\n",
    "\n",
    "        if 'force_subset_images' in self.config and self.config['force_subset_images']:\n",
    "            wanted = self.config['force_subset_images']\n",
    "            if wanted > len(all_imgs):\n",
    "                raise ValueError(f\"Requested {wanted} images, only {len(all_imgs)} available.\")\n",
    "            all_imgs = random.sample(all_imgs, wanted)\n",
    "            print(f\"Forced subset → {wanted} images\")\n",
    "\n",
    "        elif subset_ratio < 1.0:\n",
    "            k = int(len(all_imgs) * subset_ratio)\n",
    "            all_imgs = random.sample(all_imgs, k)\n",
    "            print(f\"Subset ratio {subset_ratio:.2f} → {k} images\")\n",
    "\n",
    "        # keep only pairs whose image made the cut\n",
    "        img_set = set(all_imgs)\n",
    "        pairs = [(img, cap) for img, cap in pairs if img in img_set]\n",
    "\n",
    "        # ----- 7. IMAGE-WISE shuffle + split ----------------------------------\n",
    "        random.shuffle(all_imgs)\n",
    "        n = len(all_imgs)\n",
    "        n_train = int(0.9 * n)\n",
    "        n_val   = int(0.05 * n)\n",
    "        train_imgs = set(all_imgs[:n_train])\n",
    "        val_imgs   = set(all_imgs[n_train:n_train + n_val])\n",
    "        test_imgs  = set(all_imgs[n_train + n_val:])\n",
    "\n",
    "        def assign(target_set):\n",
    "            return [(img, cap) for img, cap in pairs if img in target_set]\n",
    "\n",
    "        self.train_data = assign(train_imgs)\n",
    "        self.val_data   = assign(val_imgs)\n",
    "        self.test_data  = assign(test_imgs)\n",
    "\n",
    "        print(f\"split (by image) → \"\n",
    "              f\"train {len(train_imgs)} imgs / {len(self.train_data)} pairs | \"\n",
    "              f\"val {len(val_imgs)} / {len(self.val_data)} | \"\n",
    "              f\"test {len(test_imgs)} / {len(self.test_data)}\")\n",
    "\n",
    "        return filtered\n",
    "\n",
    "        \n",
    "    def encode_caption(self, caption: str) -> Tuple[np.ndarray, int]:\n",
    "        \"\"\"Convert caption text to sequence of token ids.\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not initialized. Call prepare_captions first.\")\n",
    "        \n",
    "        seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
    "        padded_seq = pad_sequences([seq], maxlen=self.config['max_length'], padding='post')[0]\n",
    "        return padded_seq, len(seq)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def _base_decode(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)          # [0,1]\n",
    "        return img                                                   # (h,w,3)\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image_train(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Augment + preprocess (training only).\"\"\"\n",
    "        img = self._base_decode(path)\n",
    "        img = tf.image.random_flip_left_right(img)                   # aug ①\n",
    "\n",
    "        # resize shorter side→342 then *random* crop 299×299\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = 342. / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.random_crop(img, size=[299, 299, 3])          # aug ②\n",
    "\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [299, 299, 3])\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])\n",
    "    def load_image_eval(self, path: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Deterministic centre-crop (validation / inference).\"\"\"\n",
    "        img = self._base_decode(path)\n",
    "\n",
    "        # resize shorter side→342 then *central* crop 299×299\n",
    "        shape = tf.shape(img)[:2]\n",
    "        scale = 342. / tf.cast(tf.reduce_min(shape), tf.float32)\n",
    "        new_hw = tf.cast(tf.cast(shape, tf.float32) * scale, tf.int32)\n",
    "        img = tf.image.resize(img, new_hw)\n",
    "        img = tf.image.resize_with_crop_or_pad(img, 299, 299)\n",
    "\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "        return tf.ensure_shape(img, [299, 299, 3])\n",
    "\n",
    "    def data_generator(self, data):\n",
    "        \"\"\"\n",
    "        Yield (image_tensor, token_ids, caption_len, filename)\n",
    "        so the filename is always available for debug/analysis.\n",
    "        \"\"\"\n",
    "        for img, cap in data:\n",
    "            img_path  = os.path.join(self.config['image_dir'], img)\n",
    "            img_tensor = self.load_image_train(tf.convert_to_tensor(img_path))\n",
    "            token_ids, cap_len = self.encode_caption(cap)\n",
    "            yield img_tensor, token_ids, cap_len, img \n",
    "    \n",
    "    def build_dataset(self, data,\n",
    "                      shuffle=True, cache=True, training: bool = True):\n",
    "\n",
    "        output_signature = (\n",
    "            tf.TensorSpec((299, 299, 3), tf.float32),               # image\n",
    "            tf.TensorSpec((self.config['max_length'],), tf.int32),  # token ids\n",
    "            tf.TensorSpec((), tf.int32),                            # caption len\n",
    "            tf.TensorSpec((), tf.string)                            # filename\n",
    "        )\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: self.data_generator(data),\n",
    "            output_signature=output_signature)\n",
    "\n",
    "        if cache:\n",
    "            ds = ds.cache()\n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(self.config['buffer_size'])\n",
    "\n",
    "        ds = ds.batch(self.config['batch_size'])\n",
    "        ds = ds.prefetch(AUTOTUNE)\n",
    "        return ds\n",
    "\n",
    "    def prepare_datasets(self):\n",
    "        \"\"\"Prepare all datasets for training/validation/testing.\"\"\"\n",
    "        if not self.train_data:\n",
    "            self.prepare_captions()\n",
    "\n",
    "        print(\"Building datasets...\")\n",
    "        train_ds = self.build_dataset(self.train_data)\n",
    "        val_ds = self.build_dataset(self.val_data)\n",
    "        test_ds = self.build_dataset(self.test_data, shuffle=False)\n",
    "        \n",
    "        return train_ds, val_ds, test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MODEL BUILDING BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    \"\"\"Frozen Inception‑V3 feature extractor (optionally fine‑tunable).\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"encoder\")\n",
    "        base = tf.keras.applications.InceptionV3(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(299, 299, 3))\n",
    "        base.trainable = False\n",
    "        self.cnn = Model(inputs=base.input,\n",
    "                         outputs=base.get_layer('mixed10').output)\n",
    "        self.reshape = layers.Reshape((-1, 2048))  # (B,64,2048)\n",
    "\n",
    "    def unfreeze_top_layers(self, n=2):\n",
    "        for layer in self.cnn.layers[-n:]:\n",
    "            layer.trainable = True\n",
    "\n",
    "    def call(self, x):                  # (B,299,299,3)\n",
    "        x = self.cnn(x)                 # (B,8,8,2048)\n",
    "        return self.reshape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__(name=\"attention\")\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.W2 = layers.Dense(units)\n",
    "        self.V  = layers.Dense(1)\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_time = tf.expand_dims(hidden, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(features) + self.W2(hidden_time)))\n",
    "        weights = tf.nn.softmax(score, axis=1)\n",
    "        context = tf.reduce_sum(weights * features, axis=1)\n",
    "        return context, tf.squeeze(weights, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    \"\"\"Attention‑based LSTM decoder with AMP‑safe logits.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, units: int,\n",
    "                 vocab_size: int, dropout: float = 0.5):\n",
    "        super().__init__(name=\"decoder\")\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.f_beta    = layers.Dense(1, activation='sigmoid')\n",
    "        self.lstm      = layers.LSTM(units, return_sequences=True,\n",
    "                                     return_state=True)\n",
    "        self.dropout   = layers.Dropout(dropout)\n",
    "\n",
    "        self.deep_proj = layers.Dense(units * 2)\n",
    "        # Force float32 logits for numerical stability under AMP\n",
    "        self.fc        = layers.Dense(vocab_size, dtype='float32')\n",
    "\n",
    "    def call(self, x, features, hidden, cell):\n",
    "        context, alpha = self.attention(features, hidden)\n",
    "        context = self.f_beta(hidden) * context\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        lstm_input = tf.concat([tf.expand_dims(context, 1), x], -1)\n",
    "\n",
    "        hidden = tf.cast(hidden, lstm_input.dtype)\n",
    "        cell   = tf.cast(cell, lstm_input.dtype)\n",
    "\n",
    "        lstm_out, h_t, c_t = self.lstm(lstm_input, initial_state=[hidden, cell])\n",
    "        lstm_out = tf.squeeze(lstm_out, 1)\n",
    "\n",
    "        proj = self.deep_proj(tf.concat([lstm_out, context], -1))\n",
    "        proj = tf.reshape(proj, (-1, self.units, 2))\n",
    "        maxout = tf.reduce_max(proj, axis=-1)\n",
    "        maxout = self.dropout(maxout)\n",
    "\n",
    "        logits = self.fc(maxout)  #  ← float32 regardless of global policy\n",
    "        return tf.expand_dims(logits, 1), h_t, c_t, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TOP‑LEVEL TRAINING WRAPPER (AMP READY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioningModel:\n",
    "    def __init__(self, config, processor):\n",
    "        self.config    = config\n",
    "        self.processor = processor\n",
    "\n",
    "        self.encoder      = None\n",
    "        self.decoder      = None\n",
    "        self.optimizer    = None\n",
    "        self.loss_fn      = None\n",
    "        self.ckpt_manager = None\n",
    "\n",
    "        # Logs\n",
    "        self.best_bleu      = 0.0\n",
    "        self.train_loss_log = []\n",
    "        self.train_bleu_log = []\n",
    "        self.val_bleu_log   = []\n",
    "        self.bleu_subset_idx: Optional[List[int]] = None\n",
    "        self.grad_norm_log: List[float] = []\n",
    "\n",
    "        self.smoothie = SmoothingFunction().method4\n",
    "\n",
    "    def _cast_logits(self, logits):\n",
    "        \"\"\"Helper: safely cast logits for softmax / log‑softmax.\"\"\"\n",
    "        return tf.cast(logits, tf.float32) if logits.dtype != tf.float32 else logits\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # BUILD\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    def build_model(self):\n",
    "        print(\"Building model …\")\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(\n",
    "            embedding_dim=self.config['embedding_dim'],\n",
    "            units=self.config['units'],\n",
    "            vocab_size=self.processor.vocab_size,\n",
    "            dropout=self.config.get('decoder_dropout', 0.5)\n",
    "        )\n",
    "\n",
    "        lr_schedule = CosineDecay(\n",
    "            initial_learning_rate=self.config['learning_rate'],\n",
    "            decay_steps=50_000)\n",
    "\n",
    "        if self.config['mixed_precision']:\n",
    "            base_opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "            # automatic loss-scaling for AMP\n",
    "            self.optimizer = tf.keras.mixed_precision.LossScaleOptimizer(base_opt)\n",
    "        else:\n",
    "            self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "        self.loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        # Checkpointing\n",
    "        if not self.config.get('save_checkpoints', True):\n",
    "            print(\"Checkpointing disabled.\")\n",
    "            return\n",
    "\n",
    "        ckpt = tf.train.Checkpoint(encoder=self.encoder,\n",
    "                                   decoder=self.decoder,\n",
    "                                   optimizer=self.optimizer)\n",
    "        ckpt_dir = self.config['checkpoint_dir']\n",
    "        os.makedirs(ckpt_dir, exist_ok=True)\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            ckpt, ckpt_dir,\n",
    "            max_to_keep=1 if self.config.get('delete_old_checkpoints', True) else 5)\n",
    "\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(\"Restored:\", self.ckpt_manager.latest_checkpoint)\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"Print model summaries for Encoder, Attention, and Decoder.\"\"\"\n",
    "        print(\"Building model summaries...\")\n",
    "\n",
    "        # Dummy inputs\n",
    "        dummy_image = tf.random.uniform((1, 299, 299, 3))\n",
    "        dummy_features = tf.random.uniform((1, 64, 2048))\n",
    "        dummy_hidden = tf.zeros((1, self.config['units']))\n",
    "        dummy_cell = tf.zeros((1, self.config['units']))\n",
    "        dummy_token = tf.zeros((1, 1), dtype=tf.int32)\n",
    "\n",
    "        # --- Encoder Summary ---\n",
    "        print(\"\\nEncoder Summary:\")\n",
    "        self.encoder(dummy_image)\n",
    "        self.encoder.summary()\n",
    "\n",
    "        # --- Bahdanau Attention Summary ---\n",
    "        print(\"\\nBahdanau Attention Summary:\")\n",
    "        attention_layer = BahdanauAttention(self.config['units'])\n",
    "        features_input = tf.keras.Input(shape=(64, 2048), name=\"features\")\n",
    "        hidden_input = tf.keras.Input(shape=(self.config['units'],), name=\"hidden\")\n",
    "        context_vector, attn_weights = attention_layer(features_input, hidden_input)\n",
    "        attention_model = tf.keras.Model(inputs=[features_input, hidden_input], outputs=[context_vector, attn_weights])\n",
    "        attention_model.summary()\n",
    "\n",
    "        # --- Decoder Summary ---\n",
    "        print(\"\\nDecoder Summary:\")\n",
    "        self.decoder(dummy_token, dummy_features, dummy_hidden, dummy_cell)\n",
    "        self.decoder.summary()\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # TRAIN STEP (handles loss‑scaling)\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    @tf.function\n",
    "    def train_step(self, img_tensor, target, cap_len):\n",
    "        batch_size = tf.shape(img_tensor)[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward ──────────────────────────────────────────────────────────\n",
    "            features   = self.encoder(img_tensor)        # (B,L,2048)  fp16 when AMP\n",
    "            feat_dtype = features.dtype\n",
    "\n",
    "            hidden = tf.zeros((batch_size, self.config['units']), dtype=feat_dtype)\n",
    "            cell   = tf.zeros_like(hidden)\n",
    "\n",
    "            start_tok = self.processor.tokenizer.word_index['<start>']\n",
    "            dec_input = tf.expand_dims(tf.repeat(start_tok, batch_size), 1)\n",
    "\n",
    "            L             = tf.shape(features)[1]\n",
    "            attn_accum    = tf.zeros((batch_size, L), dtype=feat_dtype)\n",
    "            total_ce_loss = tf.constant(0.0, dtype=tf.float32)   # stay fp32\n",
    "\n",
    "            ce_t = tf.constant(0.0, dtype=tf.float32)            # dummy init\n",
    "\n",
    "            for t in tf.range(1, self.config['max_length']):\n",
    "                logits, hidden, cell, alpha = self.decoder(dec_input, features, hidden, cell)\n",
    "                attn_accum += alpha\n",
    "\n",
    "                ce_t   = self.loss_fn(target[:, t], tf.squeeze(logits, 1))   # fp32\n",
    "                mask   = tf.cast(target[:, t] > 0, tf.float32)\n",
    "                total_ce_loss += tf.reduce_sum(ce_t * mask)\n",
    "\n",
    "                pred_ids = tf.argmax(logits, -1, output_type=tf.int32)[:, 0]\n",
    "\n",
    "                ss_mask  = tf.random.uniform([batch_size], dtype=tf.float32)\n",
    "                ss_mask  = ss_mask < self.ss_prob\n",
    "                next_ids = tf.where(ss_mask, pred_ids, target[:, t])\n",
    "\n",
    "                dec_input = tf.expand_dims(next_ids, 1)\n",
    "\n",
    "            total_tokens = tf.reduce_sum(tf.cast(cap_len, tf.float32))\n",
    "            ce_loss      = total_ce_loss / total_tokens\n",
    "            reg_loss     = tf.reduce_mean(tf.square(1.0 - tf.cast(attn_accum, tf.float32)))\n",
    "            loss         = ce_loss + self.config.get('attention_reg_lambda', 1.0) * reg_loss\n",
    "\n",
    "            # ---- explicit loss-scaling when the optimiser is a LossScaleOptimizer\n",
    "            if isinstance(self.optimizer, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                loss_for_grad = self.optimizer.get_scaled_loss(loss)\n",
    "            else:\n",
    "                loss_for_grad = loss\n",
    "\n",
    "        # backward ──────────────────────────────────────────────────────────────\n",
    "        variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads     = tape.gradient(loss_for_grad, variables)\n",
    "\n",
    "        if isinstance(self.optimizer, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "            grads = self.optimizer.get_unscaled_gradients(grads)\n",
    "\n",
    "        grads, _ = tf.clip_by_global_norm(grads, self.config['grad_clip_value'])\n",
    "        self.optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "        grad_norm = tf.linalg.global_norm(grads)\n",
    "        return loss, grad_norm\n",
    "\n",
    "    def beam_search_decode(self, image_path: str, beam_size: int = 5,\n",
    "                           length_penalty: float = 0.7,\n",
    "                           return_attention: bool = False):\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0)\n",
    "        base_feat = self.encoder(img_tensor)\n",
    "        start_id = self.processor.tokenizer.word_index['<start>']\n",
    "        end_id   = self.processor.tokenizer.word_index['<end>']\n",
    "\n",
    "        beams = [{'seq':[start_id], 'score':0.0,\n",
    "                  'hidden':tf.zeros((1, self.config['units'])),\n",
    "                  'cell':tf.zeros((1, self.config['units'])),\n",
    "                  'alphas':[]}]\n",
    "        completed = []\n",
    "\n",
    "        for _ in range(self.config['max_length']):\n",
    "            cand = []\n",
    "            for b in beams:\n",
    "                last = b['seq'][-1]\n",
    "                if last == end_id:\n",
    "                    completed.append(b); continue\n",
    "                dec_in = tf.expand_dims([last], 0)\n",
    "                logits, h, c, alpha = self.decoder(dec_in, base_feat,\n",
    "                                                   b['hidden'], b['cell'])\n",
    "                log_p = tf.nn.log_softmax(self._cast_logits(logits[0, 0]))\n",
    "                top = tf.math.top_k(log_p, k=beam_size).indices.numpy()\n",
    "                for tok in top:\n",
    "                    tok = int(tok)\n",
    "                    cand.append({'seq': b['seq']+[tok],\n",
    "                                 'score': b['score']+float(log_p[tok]),\n",
    "                                 'hidden': h, 'cell': c,\n",
    "                                 'alphas': b['alphas']+[alpha[0].numpy()]})\n",
    "            if not cand:\n",
    "                break\n",
    "            cand.sort(key=lambda x: x['score']/(len(x['seq'])**length_penalty),\n",
    "                       reverse=True)\n",
    "            beams = cand[:beam_size]\n",
    "            if len(completed) >= beam_size:\n",
    "                break\n",
    "\n",
    "        best = max(completed+beams,\n",
    "                    key=lambda b: b['score']/(len(b['seq'])**length_penalty))\n",
    "        words = [self.processor.tokenizer.index_word.get(i, '')\n",
    "                 for i in best['seq']\n",
    "                 if self.processor.tokenizer.index_word.get(i, '') not in\n",
    "                 ('<start>', '<end>', '<unk>')]\n",
    "        return (words, best['alphas']) if return_attention else words\n",
    "\n",
    "    def greedy_decode(self, image_path: str, return_attention=False):\n",
    "        img_tensor = tf.expand_dims(\n",
    "            self.processor.load_image_eval(tf.convert_to_tensor(image_path)), 0)\n",
    "        feat = self.encoder(img_tensor)\n",
    "        hidden = tf.zeros((1, self.config['units']))\n",
    "        cell   = tf.zeros_like(hidden)\n",
    "        dec_in = tf.expand_dims([self.processor.tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "        words, alphas = [], []\n",
    "        for _ in range(self.config['max_length']):\n",
    "            logits, hidden, cell, alpha = self.decoder(dec_in, feat, hidden, cell)\n",
    "            pred = tf.argmax(self._cast_logits(logits[0, 0])).numpy()\n",
    "            word = self.processor.tokenizer.index_word.get(pred, '')\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            if word not in ('<start>', '<unk>'):\n",
    "                words.append(word)\n",
    "            alphas.append(alpha[0].numpy())\n",
    "            dec_in = tf.expand_dims([pred], 0)\n",
    "\n",
    "        return (words, alphas) if return_attention else words\n",
    "\n",
    "    def evaluate_bleu(self, data, max_samples: int = None):\n",
    "        \"\"\"\n",
    "        Compute BLEU-1 … BLEU-4 for the given (image, caption) dataset,\n",
    "        gracefully skipping empty / malformed GT captions.\n",
    "        \"\"\"\n",
    "        refs, hyps = [], []\n",
    "        subset = data[:max_samples] if max_samples else data\n",
    "\n",
    "        for img_name, _ in tqdm.tqdm(subset, desc=\"BLEU evaluation\"):\n",
    "            # ── hypothesis ─────────────────────────────────────────────────────\n",
    "            img_path = os.path.join(self.config['image_dir'], img_name)\n",
    "            hyp_tokens = self.greedy_decode(img_path)          # list[str]\n",
    "\n",
    "            # ── references ────────────────────────────────────────────────────\n",
    "            gt_tokens = []\n",
    "            for raw_cap in self.processor.captions_dict.get(img_name, [])[:5]:\n",
    "                pc = self.processor.preprocess_caption(raw_cap)\n",
    "                if not pc:                                     # bad / empty caption\n",
    "                    continue\n",
    "                tokens = [w for w in pc.split()\n",
    "                        if w not in (\"<start>\", \"<end>\")]\n",
    "                if tokens:                                     # keep non-empty refs\n",
    "                    gt_tokens.append(tokens)\n",
    "\n",
    "            if not gt_tokens:                                  # nothing usable\n",
    "                continue\n",
    "\n",
    "            refs.append(gt_tokens)\n",
    "            hyps.append(hyp_tokens)\n",
    "\n",
    "        if not refs:\n",
    "            raise RuntimeError(\"No valid reference captions were found\")\n",
    "\n",
    "        # ── BLEU-n for n = 1…4 ───────────────────────────────────────────────\n",
    "        bleu = {}\n",
    "        for n in range(1, 5):\n",
    "            weights = (1.0 / n,) * n + (0.0,) * (4 - n)\n",
    "            bleu[f\"bleu-{n}\"] = corpus_bleu(\n",
    "                refs, hyps, weights=weights, smoothing_function=self.smoothie\n",
    "            )\n",
    "            print(f\"BLEU-{n}: {bleu[f'bleu-{n}']:.4f}\")\n",
    "\n",
    "        return bleu\n",
    "\n",
    "    def train(self, train_ds, val_data, epochs=None,\n",
    "              subset_size: int = 200):\n",
    "\n",
    "        if epochs is None:\n",
    "            epochs = self.config['epochs']\n",
    "\n",
    "        if self.bleu_subset_idx is None:\n",
    "            total_train = len(self.processor.train_data)\n",
    "            subset_size = min(subset_size, total_train)\n",
    "            self.bleu_subset_idx = random.sample(range(total_train), subset_size)\n",
    "\n",
    "        def _subset(data, idx):\n",
    "            return [data[i] for i in idx]\n",
    "\n",
    "        patience         = self.config.get('patience', 8)\n",
    "        wait             = 0\n",
    "        apply_early_stop = self.config.get('early_stop', True)\n",
    "        self.ss_max_prob = self.config.get('scheduled_sampling_max_prob', 0.0)\n",
    "\n",
    "        self.grad_norm_log = []        # fresh log each train() call\n",
    "\n",
    "        # ── epoch loop ─────────────────────────────────────────────────────\n",
    "        for epoch in range(epochs):\n",
    "            self.ss_prob = self.ss_max_prob * (epoch / max(1, epochs - 1))\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs} (ε={self.ss_prob:.3f})\")\n",
    "\n",
    "            start_time       = time.time()\n",
    "            total_loss       = 0.0\n",
    "            latest_grad_norm = 0.0\n",
    "\n",
    "            prog = tf.keras.utils.Progbar(None, stateful_metrics=['loss'])\n",
    "\n",
    "            # ── batch loop ───────────────────────────────────────────────\n",
    "            for batch, (img, tgt, cap_len, _) in enumerate(train_ds):\n",
    "                if batch == 0 and prog.target is None:\n",
    "                    prog.target = (len(self.processor.train_data)\n",
    "                                   // self.config['batch_size'] + 1)\n",
    "\n",
    "                batch_loss, batch_norm = self.train_step(img, tgt, cap_len)\n",
    "\n",
    "                total_loss       += float(batch_loss)\n",
    "                latest_grad_norm  = float(batch_norm)\n",
    "                self.grad_norm_log.append(latest_grad_norm)\n",
    "\n",
    "                prog.update(batch + 1, values=[('loss', batch_loss)])\n",
    "\n",
    "            avg_loss = total_loss / (batch + 1)\n",
    "            self.train_loss_log.append(avg_loss)\n",
    "\n",
    "            # ── quick BLEU on fixed subset ───────────────────────────────\n",
    "            train_subset = _subset(self.processor.train_data, self.bleu_subset_idx)\n",
    "            train_bleu   = self.evaluate_bleu(train_subset)['bleu-4']\n",
    "            val_bleu     = self.evaluate_bleu(val_data)['bleu-4']\n",
    "            self.train_bleu_log.append(train_bleu)\n",
    "            self.val_bleu_log.append(val_bleu)\n",
    "\n",
    "            if self.ckpt_manager:\n",
    "                self.ckpt_manager.save()\n",
    "\n",
    "            # ── early-stop check ─────────────────────────────────────────\n",
    "            if val_bleu > self.best_bleu:\n",
    "                self.best_bleu = val_bleu\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "                if apply_early_stop and wait >= patience:\n",
    "                    print(f\"Early stopping (no BLEU gain for {wait} epochs)\")\n",
    "                    break\n",
    "\n",
    "            # ↓↓↓ **safe LR readout** (works for schedule *or* constant) ↓↓↓\n",
    "            lr_obj = self.optimizer.learning_rate\n",
    "            if callable(lr_obj):\n",
    "                lr_now = float(lr_obj(self.optimizer.iterations).numpy())\n",
    "            else:\n",
    "                lr_now = float(tf.keras.backend.get_value(lr_obj))\n",
    "            # ↑↑↑---------------------------------------------------------↑↑↑\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: \"\n",
    "                  f\"loss={avg_loss:.4f}  \"\n",
    "                  f\"trainBLEU={train_bleu:.4f}  valBLEU={val_bleu:.4f}  \"\n",
    "                  f\"lr={lr_now:.2e}  \"\n",
    "                  f\"time={time.time() - start_time:.1f}s\")\n",
    "\n",
    "        return self.train_loss_log, self.val_bleu_log\n",
    "\n",
    "\n",
    "    def plot_attention(self, image_path: str, caption: list, alphas: list):\n",
    "        \"\"\"Improved visualization of attention with better contrast and 299x299 alignment.\"\"\"\n",
    "        img = np.array(Image.open(image_path).resize((299, 299)))\n",
    "        fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "        for t in range(len(caption)):\n",
    "            ax = fig.add_subplot(3, int(np.ceil(len(caption) / 3)), t + 1)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "            alpha = np.array(alphas[t])\n",
    "            attention_shape = int(np.sqrt(alpha.size))\n",
    "            alpha = alpha.reshape((attention_shape, attention_shape))\n",
    "\n",
    "            # Normalize and boost contrast\n",
    "            alpha -= alpha.min()\n",
    "            if alpha.max() > 0:\n",
    "                alpha /= alpha.max()\n",
    "\n",
    "            # Resize attention map to 299×299\n",
    "            alpha_resized = Image.fromarray(np.uint8(255 * alpha)).resize((299, 299), resample=Image.BICUBIC)\n",
    "            alpha_resized = np.array(alpha_resized) / 255.0  # back to [0,1] float\n",
    "\n",
    "            ax.imshow(alpha_resized, cmap='jet', alpha=0.5, extent=(0, 299, 299, 0))\n",
    "            ax.set_title(f\"{t+1}: '{caption[t]}'\", fontsize=10)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_history(self):\n",
    "        \"\"\"Plot loss curve **and** both train/val BLEU-4 curves.\"\"\"\n",
    "        plt.figure(figsize=(14, 5))\n",
    "\n",
    "        # --- left: training loss ---\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_loss_log, label='Train Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Cross-Entropy Loss')\n",
    "        plt.title('Training Loss')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        # --- right: BLEU-4 ---\n",
    "        plt.subplot(1, 2, 2)\n",
    "        if self.train_bleu_log:\n",
    "            plt.plot(self.train_bleu_log, label='Train BLEU-4')\n",
    "        plt.plot(self.val_bleu_log,   label='Val BLEU-4')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU-4')\n",
    "        plt.title('BLEU-4 Scores')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def speak_caption(self, caption: str, filename=\"caption_audio.mp3\"):\n",
    "        \"\"\"Generate speech audio from caption text.\"\"\"\n",
    "        if not caption:\n",
    "            print(\"Empty caption, nothing to speak\")\n",
    "            return\n",
    "            \n",
    "        tts = gTTS(text=caption, lang='en')\n",
    "        tts.save(filename)\n",
    "        display(Audio(filename))\n",
    "        print(f\"Audio saved to {filename}\")\n",
    "    \n",
    "    def demo(\n",
    "            self,\n",
    "            image_path: str,\n",
    "            filename: str = \"caption_audio.mp3\",\n",
    "            beam_size: int = 5,\n",
    "            length_penalty: float = 0.7):\n",
    "        \"\"\"\n",
    "        End-to-end demo (beam-search inference):\n",
    "\n",
    "        1. Original image  – now titled with the **filename**\n",
    "        2. Ground-truth captions\n",
    "        3. Generated caption\n",
    "        4. Audio playback\n",
    "        5. Attention heat-maps\n",
    "        \"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "            return\n",
    "\n",
    "        # ---------------- 1. original image ----------------\n",
    "        img_name = os.path.basename(image_path)              # <- filename for the title\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.title(img_name, fontsize=14, pad=10)             # <- show filename here\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # ---------------- 2. ground-truth captions ----------\n",
    "        gt_caps = self.processor.captions_dict.get(img_name, [])\n",
    "        if gt_caps:\n",
    "            print(\"Ground-truth captions:\")\n",
    "            for cap in gt_caps:\n",
    "                print(\" -\", cap)\n",
    "        else:\n",
    "            print(\"No ground-truth captions found.\")\n",
    "\n",
    "        # ---------------- 3. caption generation -------------\n",
    "        words, attention = self.beam_search_decode(\n",
    "            image_path,\n",
    "            beam_size=beam_size,\n",
    "            length_penalty=length_penalty,\n",
    "            return_attention=True\n",
    "        )\n",
    "        caption = \" \".join(words)\n",
    "        print(\"\\nGenerated caption:\")\n",
    "        print(caption)\n",
    "\n",
    "        # ---------------- 4. audio --------------------------\n",
    "        self.speak_caption(caption, filename=filename)\n",
    "\n",
    "        # ---------------- 5. attention plot ----------------\n",
    "        self.plot_attention(image_path, words, attention)\n",
    "\n",
    "    def prime_dataset(self, ds, steps: int = None) -> None:\n",
    "        \"\"\"\n",
    "        Pre-fill a tf.data shuffle buffer so the first training epoch\n",
    "        starts without the usual “Filling up shuffle buffer …” pause.\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        ds    : the *un-iterated* tf.data.Dataset you’ll pass to train()\n",
    "        steps : number of iterator steps to advance; default uses\n",
    "                buffer_size // batch_size + 1 from config.\n",
    "        \"\"\"\n",
    "        if steps is None:\n",
    "            steps = self.config['buffer_size'] // self.config['batch_size'] + 1\n",
    "\n",
    "        it = iter(ds)\n",
    "        for _ in range(steps):\n",
    "            try:\n",
    "                next(it)\n",
    "            except StopIteration:  # dataset shorter than requested priming\n",
    "                break\n",
    "\n",
    "    def fine_tune_cnn(self,\n",
    "                      train_ds,\n",
    "                      val_data,\n",
    "                      layers_to_unfreeze: int = 2,\n",
    "                      lr: float = 1e-5,\n",
    "                      epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Phase-2 fine-tuning of the top Inception blocks.\n",
    "        Call after initial caption training for an extra accuracy bump.\n",
    "        \"\"\"\n",
    "        print(f\"\\nUnfreezing top {layers_to_unfreeze} Inception blocks …\")\n",
    "        self.encoder.unfreeze_top_layers(layers_to_unfreeze)\n",
    "\n",
    "        # New, low learning-rate optimiser\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "        print(f\"Fine-tuning CNN for {epochs} epoch(s) at lr={lr} …\")\n",
    "        self.train(train_ds, val_data, epochs=epochs)\n",
    "\n",
    "        print(\"CNN fine-tune finished.\")\n",
    "\n",
    "\n",
    "    def plot_grad_norms(self):\n",
    "        \"\"\"Simple line plot of gradient norms per batch.\"\"\"\n",
    "        if not self.grad_norm_log:\n",
    "            print(\"No grad-norm data logged yet.\")\n",
    "            return\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(self.grad_norm_log)\n",
    "        plt.xlabel('Update step')\n",
    "        plt.ylabel('Global grad norm')\n",
    "        plt.title('Gradient-norm trajectory')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = processor.load_captions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.display_samples(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.prepare_captions(subset_ratio=CONFIG['subset_ratio'])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "train_ds, val_ds, _ = processor.prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioningModel(CONFIG, processor)\n",
    "model.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prime_dataset(ds=train_ds, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train(train_ds, processor.val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_val = random.sample(processor.val_data, min(len(processor.val_data), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_ds=train_ds, val_data=reduced_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_grad_norms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fine_tune_cnn(train_ds, reduced_val, layers_to_unfreeze=8, lr=1e-5, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_bleu(processor.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_bleu(processor.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = processor.train_data[:5]  # known to be from training set\n",
    "for img_name, _ in samples:\n",
    "    img_path = os.path.join(CONFIG['image_dir'], img_name)\n",
    "    pred = model.greedy_decode(img_path)\n",
    "    print ('------------')\n",
    "    print(\"Predicted:\", ' '.join(pred))\n",
    "    print(\"Ground Truth:\", processor.captions_dict[img_name][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pair = random.choice(processor.test_data)\n",
    "sample_img = os.path.join(CONFIG['image_dir'], sample_pair[0])\n",
    "model.demo(sample_img, filename='caption_audio01.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_ds.take(1):\n",
    "    img_tensor, tok_ids, _, filenames = batch\n",
    "    fname = filenames[0].numpy().decode()\n",
    "\n",
    "    decoded = ' '.join(\n",
    "        processor.tokenizer.index_word.get(int(i), '')\n",
    "        for i in tok_ids[0].numpy() if i)\n",
    "\n",
    "    print(\"Filename :\", fname)\n",
    "    print(\"Caption  :\", decoded)\n",
    "    print(\"GT       :\", processor.captions_dict[fname][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
