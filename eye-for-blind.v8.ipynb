{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, math, random, json, pickle, itertools, warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from collections import Counter, deque\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "import tensorflow as tf                  # type: ignore\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, object] = {\n",
    "    # Data\n",
    "    'image_dir'            : '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file'         : '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    'subset_ratio'         : 1.0,\n",
    "    # Vocabulary & sequence\n",
    "    'vocab_min_count'      : 2,          # [spec 1] ↓ from 5\n",
    "    'top_k'                : 10_000,     # [spec 1] explicit cap\n",
    "    'max_length'           : 30,\n",
    "    # Model\n",
    "    'embedding_dim'        : 512,\n",
    "    'units'                : 1024,\n",
    "    'decoder_dropout'      : 0.3,\n",
    "    'attention_reg_lambda' : 0.1,        # [spec 5]  coverage regulariser\n",
    "    # Training\n",
    "    'epochs'               : 30,\n",
    "    'batch_size'           : 128,\n",
    "    'buffer_size'          : 10_000,\n",
    "    'scheduled_sampling_max_prob': 0.4,  # [spec 4]  ↑ from 0.15\n",
    "    'mixed_precision'      : True,\n",
    "    'grad_clip_value'      : 10.0,\n",
    "    'early_stop'           : True,\n",
    "    'patience'             : 20,\n",
    "    # Optimiser / LR\n",
    "    'initial_lr'           : 5e-4,\n",
    "    'lr_alpha'             : 1e-2,\n",
    "    # Checkpoints\n",
    "    'checkpoint_dir'       : './checkpoints/split_by_image',\n",
    "    'save_checkpoints'     : True,\n",
    "    'delete_old_checkpoints': True,\n",
    "    # Misc / reproducibility\n",
    "    'seed'                 : 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ENV‑SETUP & MIXED‑PRECISION POLICY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if CONFIG['mixed_precision']:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"[AMP] mixed_float16 policy active ✨\")\n",
    "\n",
    "# single-GPU safe-growth\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(\"Device list:\", tf.config.list_logical_devices('GPU'))\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg      = cfg\n",
    "        self.caption_map: Dict[str, List[str]] = {}\n",
    "        self.tokeniser: Optional[Tokenizer]    = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_pairs: List[Tuple[str,str]] = []\n",
    "        self.val_pairs  : List[Tuple[str,str]] = []\n",
    "        self.test_pairs : List[Tuple[str,str]] = []\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.1 load raw captions file\n",
    "    # ───────────────────────────────────────────\n",
    "    def load_captions(self):\n",
    "        fp = self.cfg['caption_file']\n",
    "        df = pd.read_csv(fp, sep='|', names=['img','num','cap'], engine='python')\n",
    "        df['img']  = df['img'].str.strip(); df['cap'] = df['cap'].str.strip()\n",
    "        for img,g in df.groupby('img'):\n",
    "            self.caption_map[img] = g['cap'].tolist()\n",
    "        print(f\"Loaded {len(self.caption_map):,} images with captions\")\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.2 text preproc\n",
    "    # ───────────────────────────────────────────\n",
    "    @staticmethod\n",
    "    def preprocess(txt:str)->str:\n",
    "        if not isinstance(txt,str) or not txt.strip(): return \"\"\n",
    "        txt = re.sub(r\"[^a-z0-9.,? ]\",\"\",txt.lower().strip())\n",
    "        return f\"<start> {txt} <end>\"\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.3 build tokeniser & split\n",
    "    # ───────────────────────────────────────────\n",
    "    def prepare(self):\n",
    "        if not self.caption_map: self.load_captions()\n",
    "        all_caps = [self.preprocess(c) for caps in self.caption_map.values() for c in caps]\n",
    "        # vocab pruning\n",
    "        cnt = Counter(w for sent in all_caps for w in sent.split())\n",
    "        keep = {w for w,f in cnt.items() if f>=self.cfg['vocab_min_count']} | {'<start>','<end>'}\n",
    "        filtered = [c for c in all_caps if all(w in keep for w in c.split())]\n",
    "\n",
    "        # 95-th percentile length\n",
    "        self.cfg['max_length'] = int(np.percentile([len(s.split()) for s in filtered],95))\n",
    "        print(\"max_length →\",self.cfg['max_length'])\n",
    "\n",
    "        # tokeniser with explicit pad-idx 0  [spec 1,2]\n",
    "        tok = Tokenizer(num_words=self.cfg['top_k'], oov_token='<unk>', filters='', lower=True)\n",
    "        tok.fit_on_texts(filtered)\n",
    "        tok.word_index['<pad>']  = 0\n",
    "        tok.index_word[0]        = '<pad>'\n",
    "        self.tokeniser, self.vocab_size = tok, self.cfg['top_k']+1\n",
    "        print(\"vocab_size capped at\",self.vocab_size)\n",
    "\n",
    "        # img-wise (80/10/10) split\n",
    "        pairs = [(img,self.preprocess(c)) for img,caps in self.caption_map.items() for c in caps\n",
    "                 if self.preprocess(c) and all(w in keep for w in self.preprocess(c).split())]\n",
    "        imgset = list({img for img,_ in pairs})\n",
    "        if self.cfg['subset_ratio']<1.0:\n",
    "            k=int(len(imgset)*self.cfg['subset_ratio']); imgset=random.sample(imgset,k)\n",
    "        random.shuffle(imgset)\n",
    "        n=len(imgset); n_tr=int(.8*n); n_v=int(.1*n)\n",
    "        tr,val,test = set(imgset[:n_tr]),set(imgset[n_tr:n_tr+n_v]),set(imgset[n_tr+n_v:])\n",
    "        f=lambda s:[p for p in pairs if p[0] in s]\n",
    "        self.train_pairs,self.val_pairs,self.test_pairs = map(f,(tr,val,test))\n",
    "        print(f\"train/val/test images: {len(tr)}/{len(val)}/{len(test)}\")\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.4 helpers\n",
    "    # ───────────────────────────────────────────\n",
    "    def encode(self, cap:str):\n",
    "        seq=self.tokeniser.texts_to_sequences([cap])[0]\n",
    "        seq=pad_sequences([seq],maxlen=self.cfg['max_length'],padding='post',value=0)[0] # [spec 2]\n",
    "        return seq, len([t for t in seq if t!=0])\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([],tf.string)])\n",
    "    def _load_img(self,path):\n",
    "        img=tf.io.read_file(path); img=tf.image.decode_jpeg(img,3); img=tf.image.convert_image_dtype(img,tf.float32)\n",
    "        return img\n",
    "\n",
    "    def _augment(self,img,train=True):\n",
    "        shape=tf.shape(img)[:2]\n",
    "        scale=342./tf.cast(tf.reduce_min(shape),tf.float32)\n",
    "        img=tf.image.resize(img,tf.cast(tf.cast(shape,tf.float32)*scale,tf.int32))\n",
    "        if train: img=tf.image.random_flip_left_right(img); img=tf.image.random_crop(img,[299,299,3])\n",
    "        else:     img=tf.image.resize_with_crop_or_pad(img,299,299)\n",
    "        return tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "    def _gen(self,data,train=True):\n",
    "        for img,cap in data:\n",
    "            path=os.path.join(self.cfg['image_dir'],img)\n",
    "            img_tensor=self._augment(self._load_img(tf.constant(path)),train)\n",
    "            tok,ln=self.encode(cap)\n",
    "            yield img_tensor, tok.astype(np.int32), ln, img\n",
    "\n",
    "    def make_ds(self,data,train=True):\n",
    "        sig=(tf.TensorSpec((299,299,3),tf.float32),\n",
    "             tf.TensorSpec((self.cfg['max_length'],),tf.int32),\n",
    "             tf.TensorSpec((),tf.int32),\n",
    "             tf.TensorSpec((),tf.string))\n",
    "        ds=tf.data.Dataset.from_generator(lambda:self._gen(data,train),output_signature=sig)\n",
    "        if train: ds=ds.shuffle(self.cfg['buffer_size'])\n",
    "        ds=ds.batch(self.cfg['batch_size']).prefetch(AUTOTUNE)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MODEL BUILDING BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self): super().__init__(); base=tf.keras.applications.InceptionV3(include_top=False,weights='imagenet',input_shape=(299,299,3))\n",
    "    def build(self,input_shape):\n",
    "        self.base=tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "        self.base.trainable=False; self.reshape=layers.Reshape((-1,2048))\n",
    "    def call(self,x): return self.reshape(self.base(x))\n",
    "    def unfreeze_top_layers(self,n=8):                       # [spec 7]\n",
    "        for l in self.base.layers[-n:]: l.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self,units): super().__init__(); self.W1=layers.Dense(units); self.W2=layers.Dense(units); self.V=layers.Dense(1)\n",
    "    def call(self,feat,hidden):\n",
    "        score=self.V(tf.nn.tanh(self.W1(feat)+self.W2(tf.expand_dims(hidden,1))))\n",
    "        attn=tf.nn.softmax(score,1); ctx=tf.reduce_sum(attn*feat,1)\n",
    "        return ctx,tf.squeeze(attn,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self,embed_dim,units,vocab,drop):\n",
    "        super().__init__(); self.units=units\n",
    "        self.embed=layers.Embedding(vocab,embed_dim,mask_zero=True)\n",
    "        self.attn=BahdanauAttention(units); self.beta=layers.Dense(1,activation='sigmoid')\n",
    "        self.lstm=layers.LSTM(units,return_sequences=True,return_state=True)\n",
    "        self.drop=layers.Dropout(drop); self.proj=layers.Dense(units*2); self.fc=layers.Dense(vocab,dtype='float32')\n",
    "        \n",
    "    def call(self,x,feat,h,c):\n",
    "        ctx,alpha=self.attn(feat,h); ctx=self.beta(h)*ctx\n",
    "        x=self.embed(x); x=tf.concat([tf.expand_dims(ctx,1),x],-1)\n",
    "        o,h,c=self.lstm(x,initial_state=[h,c]); o=tf.squeeze(o,1)\n",
    "        m=tf.reshape(self.proj(tf.concat([o,ctx],-1)),(-1,self.units,2)); m=tf.reduce_max(m,2)\n",
    "        return tf.expand_dims(self.drop(m),1),h,c,alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TOP‑LEVEL TRAINING WRAPPER (AMP READY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningSystem:\n",
    "    def __init__(self,cfg:Dict[str,object],proc:DataProcessor):\n",
    "        self.cfg,self.proc=cfg,proc\n",
    "        self.encoder,self.decoder=None,None\n",
    "        self.opt,self.loss_fn=None,None\n",
    "        self.ckpt_mgr=None\n",
    "        # logs\n",
    "        self.best_bleu=0; self.loss_log=[]; self.tr_bleu=[]; self.val_bleu=[]\n",
    "        self.grad_norms=[]\n",
    "        self.smooth=SmoothingFunction().method4\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.1 build & compile\n",
    "    # ─────────────────────────────\n",
    "    def build(self,steps_per_epoch):\n",
    "        print(\"Building model …\")\n",
    "        self.encoder=Encoder(); self.decoder=Decoder(self.cfg['embedding_dim'],self.cfg['units'],\n",
    "                                                     self.proc.vocab_size,self.cfg['decoder_dropout'])\n",
    "\n",
    "        decay_steps=int(steps_per_epoch*self.cfg['epochs'])     # [spec 6]\n",
    "        sch=CosineDecay(self.cfg['initial_lr'],decay_steps,alpha=self.cfg['lr_alpha'])\n",
    "        base_opt=tf.keras.optimizers.Adam(sch)\n",
    "        self.opt=(tf.keras.mixed_precision.LossScaleOptimizer(base_opt)\n",
    "                  if self.cfg['mixed_precision'] else base_opt)\n",
    "\n",
    "        self.loss_fn=SparseCategoricalCrossentropy(from_logits=True,label_smoothing=0.1,reduction='none') # [spec 3]\n",
    "\n",
    "        if self.cfg['save_checkpoints']:\n",
    "            ckpt=tf.train.Checkpoint(enc=self.encoder,dec=self.decoder,opt=self.opt)\n",
    "            self.ckpt_mgr=tf.train.CheckpointManager(ckpt,self.cfg['checkpoint_dir'],max_to_keep=3)\n",
    "            if self.ckpt_mgr.latest_checkpoint:\n",
    "                ckpt.restore(self.ckpt_mgr.latest_checkpoint); print(\"Restored\",self.ckpt_mgr.latest_checkpoint)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.2 helpers\n",
    "    # ─────────────────────────────\n",
    "    def _cast(self,x): return tf.cast(x,tf.float32) if x.dtype!=tf.float32 else x\n",
    "\n",
    "    # scheduled-sampling prob updated every epoch  [spec 4]\n",
    "    def _ss_prob(self,epoch): return self.cfg['scheduled_sampling_max_prob']*epoch/max(1,self.cfg['epochs']-1)\n",
    "\n",
    "    # trigram-blocker util                               [spec 11]\n",
    "    @staticmethod\n",
    "    def _repeat_trigram(seq,tok):\n",
    "        return len(seq)>=2 and (seq[-2],seq[-1],tok) in set(zip(seq,seq[1:],seq[2:]))\n",
    "\n",
    "    # deduplicate prediction util                        [spec 12]\n",
    "    @staticmethod\n",
    "    def _dedup(tokens):\n",
    "        out=[]\n",
    "        for i,t in enumerate(tokens):\n",
    "            if i<2 or not(t==tokens[i-1]==tokens[i-2]): out.append(t)\n",
    "        return out\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.3 train-step @tf.function\n",
    "    # ─────────────────────────────\n",
    "    @tf.function\n",
    "    def _train_step(self,img,tgt,ln,ss_p):\n",
    "        B=tf.shape(img)[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            feat=self.encoder(img)                                    # (B,64,2048)\n",
    "            h=tf.zeros((B,self.cfg['units']),feat.dtype); c=tf.zeros_like(h)\n",
    "            dec_in=tf.expand_dims(tf.fill([B],self.proc.tokeniser.word_index['<start>']),1)\n",
    "            attn_cov=tf.zeros((B,tf.shape(feat)[1]),feat.dtype)\n",
    "            total_ce=tf.constant(0.,tf.float32)\n",
    "\n",
    "            for t in tf.range(1,self.cfg['max_length']):\n",
    "                logits,h,c,alpha=self.decoder(dec_in,feat,h,c)\n",
    "                attn_cov+=alpha\n",
    "                ce=self.loss_fn(tgt[:,t],tf.squeeze(logits,1))\n",
    "                mask=tf.cast(tgt[:,t]!=0,tf.float32); total_ce+=tf.reduce_sum(ce*mask)\n",
    "\n",
    "                pred=tf.argmax(logits,-1,output_type=tf.int32)[:,0]\n",
    "                use_pred=tf.less(tf.random.uniform([B]),ss_p)\n",
    "                nxt=tf.where(use_pred,pred,tgt[:,t])\n",
    "                dec_in=tf.expand_dims(nxt,1)\n",
    "\n",
    "            ce_loss=total_ce/tf.reduce_sum(tf.cast(ln,tf.float32))\n",
    "            reg=tf.reduce_mean(tf.square(1.-self._cast(attn_cov)))\n",
    "            loss=ce_loss+self.cfg['attention_reg_lambda']*reg        # coverage [spec 5]\n",
    "\n",
    "            if isinstance(self.opt,tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                loss_scaled=self.opt.get_scaled_loss(loss)\n",
    "            else: loss_scaled=loss\n",
    "\n",
    "        vars=self.encoder.trainable_variables+self.decoder.trainable_variables\n",
    "        grads=tape.gradient(loss_scaled,vars)\n",
    "        if isinstance(self.opt,tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "            grads=self.opt.get_unscaled_gradients(grads)\n",
    "        grads,_=tf.clip_by_global_norm(grads,self.cfg['grad_clip_value'])\n",
    "        self.opt.apply_gradients(zip(grads,vars))\n",
    "        return loss,tf.linalg.global_norm(grads)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.4 greedy decode – **batched** & @tf.function  [spec 8]\n",
    "    # ─────────────────────────────\n",
    "    @tf.function\n",
    "    def _greedy_batch(self,feat_batch):\n",
    "        B=tf.shape(feat_batch)[0]\n",
    "        h=tf.zeros((B,self.cfg['units']),feat_batch.dtype); c=tf.zeros_like(h)\n",
    "        dec_in=tf.expand_dims(tf.fill([B],self.proc.tokeniser.word_index['<start>']),1)\n",
    "        seq=tf.TensorArray(tf.int32,size=self.cfg['max_length'])\n",
    "        for t in tf.range(self.cfg['max_length']):\n",
    "            logits,h,c,_=self.decoder(dec_in,feat_batch,h,c)\n",
    "            nxt=tf.argmax(logits[:,-1],-1,output_type=tf.int32)\n",
    "            seq=seq.write(t,nxt)\n",
    "            dec_in=tf.expand_dims(nxt,1)\n",
    "        return tf.transpose(seq.stack())  # (B,T)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.5 evaluate BLEU util (uses cached CNN feats)    [spec 8,9]\n",
    "    # ─────────────────────────────\n",
    "    def _load_val_feats(self,save_if_missing=True):\n",
    "        cache='val_feats.npz'\n",
    "        if os.path.exists(cache):\n",
    "            return np.load(cache)['arr_0']\n",
    "        feats=[]\n",
    "        for img,_ in self.proc.val_pairs:\n",
    "            path=os.path.join(self.cfg['image_dir'],img)\n",
    "            tensor=self.proc._augment(self.proc._load_img(tf.constant(path)),train=False)\n",
    "            feats.append(self.encoder(tf.expand_dims(tensor,0))[0].numpy())\n",
    "        arr=np.stack(feats)\n",
    "        if save_if_missing: np.savez_compressed(cache,arr_0=arr)\n",
    "        return arr\n",
    "\n",
    "    def _compute_bleu(self,data,batch_size=64,max_imgs=None):\n",
    "        refs,hyps=[],[]\n",
    "        subset=data if max_imgs is None else random.sample(data,min(max_imgs,len(data)))\n",
    "        # pre-decode CNN feats in batches for speed\n",
    "        feat_list=[]\n",
    "        for img,_ in subset:\n",
    "            path=os.path.join(self.cfg['image_dir'],img)\n",
    "            feat=self.encoder(tf.expand_dims(\n",
    "                self.proc._augment(self.proc._load_img(tf.constant(path)),train=False),0))\n",
    "            feat_list.append(feat)\n",
    "        batched=tf.data.Dataset.from_tensor_slices(tf.concat(feat_list,0)).batch(batch_size)\n",
    "        preds=[]\n",
    "        for feat_b in batched:\n",
    "            seqs=self._greedy_batch(feat_b).numpy()                   # [spec 8]\n",
    "            preds.extend(seqs)\n",
    "        for (img,_),seq in zip(subset,preds):\n",
    "            hyp=[self.proc.tokeniser.index_word.get(i,'') for i in seq\n",
    "                 if i not in (0,self.proc.tokeniser.word_index['<end>'],\n",
    "                              self.proc.tokeniser.word_index['<start>'])]\n",
    "            hyp=self._dedup(hyp)                                      # [spec 12]\n",
    "            gt=[[w for w in self.proc.preprocess(c).split()\n",
    "                 if w not in ('<start>','<end>')] for c in self.proc.caption_map[img][:5]]\n",
    "            refs.append(gt); hyps.append(hyp)\n",
    "        weight=(0.25,0.25,0.25,0.25)\n",
    "        return corpus_bleu(refs,hyps,weights=weight,smoothing_function=self.smooth)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.6 beam search w/ trigram block [spec 11] + dedup [12]\n",
    "    # ─────────────────────────────\n",
    "    def beam_search(self,img_path,beam=5,len_pen=0.7):\n",
    "        feat=self.encoder(tf.expand_dims(\n",
    "            self.proc._augment(self.proc._load_img(tf.constant(img_path)),train=False),0))\n",
    "        start,end=self.proc.tokeniser.word_index['<start>'],self.proc.tokeniser.word_index['<end>']\n",
    "        beams=[(0,[start],tf.zeros((1,self.cfg['units'])),tf.zeros((1,self.cfg['units'])),[])]\n",
    "        completed=[]\n",
    "        for _ in range(self.cfg['max_length']):\n",
    "            cand=[]\n",
    "            for score,seq,h,c,alphas in beams:\n",
    "                if seq[-1]==end: completed.append((score,seq,alphas)); continue\n",
    "                logits,h1,c1,alpha=self.decoder(tf.expand_dims([seq[-1]],0),feat,h,c)\n",
    "                log_p=tf.nn.log_softmax(self._cast(logits[0,0]))\n",
    "                top=tf.math.top_k(log_p,beam).indices.numpy()\n",
    "                for tok in top:\n",
    "                    tok=int(tok); \n",
    "                    if self._repeat_trigram(seq,tok): continue\n",
    "                    cand.append((score+float(log_p[tok]),seq+[tok],h1,c1,alphas+[alpha[0].numpy()]))\n",
    "            if not cand: break\n",
    "            cand.sort(key=lambda x:x[0]/((len(x[1])**len_pen)),reverse=True)\n",
    "            beams=cand[:beam]\n",
    "            if len(completed)>=beam: break\n",
    "        best=max(completed+beams,key=lambda x:x[0]/((len(x[1]))**len_pen))\n",
    "        words=[self.proc.tokeniser.index_word.get(i,'') for i in best[1]\n",
    "               if i not in (start,end,0)] \n",
    "        return \" \".join(self._dedup(words))\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.7 main TRAIN LOOP (spec 4,6,7,10,14)\n",
    "    # ─────────────────────────────\n",
    "    def train(self,train_ds,val_subset,val_full):\n",
    "        steps_per_epoch=math.ceil(len(self.proc.train_pairs)/self.cfg['batch_size'])\n",
    "        self.build(steps_per_epoch)\n",
    "        for epoch in range(self.cfg['epochs']):\n",
    "            ss_p=self._ss_prob(epoch)                                 # [spec 4]\n",
    "            prog=tf.keras.utils.Progbar(steps_per_epoch,unit_name='batch')\n",
    "            tot_loss=0\n",
    "            for img,tgt,ln,_ in train_ds:\n",
    "                loss,g=self._train_step(img,tgt,ln,ss_p)\n",
    "                tot_loss+=float(loss); self.grad_norms.append(float(g))\n",
    "                prog.add(1,values=[('loss',loss)])\n",
    "            self.loss_log.append(tot_loss/steps_per_epoch)\n",
    "\n",
    "            # micro-val BLEU (≤500 imgs) every epoch\n",
    "            micro_bleu=self._compute_bleu(val_subset)\n",
    "            # full val BLEU every 5 epochs      [spec 10]\n",
    "            full_bleu = self._compute_bleu(val_full) if epoch%5==0 else None\n",
    "            self.tr_bleu.append(micro_bleu); self.val_bleu.append(full_bleu or np.nan)\n",
    "\n",
    "            # checkpoint\n",
    "            if self.ckpt_mgr: self.ckpt_mgr.save()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.cfg['epochs']}  \"\n",
    "                  f\"loss={self.loss_log[-1]:.4f}  microBLEU={micro_bleu:.4f}\"\n",
    "                  + (f\"  fullBLEU={full_bleu:.4f}\" if full_bleu is not None else \"\")\n",
    "                  + f\"  ss_p={ss_p:.2f}\")\n",
    "\n",
    "            # unfreeze CNN after epoch 5     [spec 7]\n",
    "            if epoch==4:\n",
    "                self.encoder.unfreeze_top_layers(8); \n",
    "                self.opt=tf.keras.optimizers.Adam(1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor=DataProcessor(CONFIG); processor.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=processor.make_ds(processor.train_pairs,train=True)\n",
    "val_ds =processor.make_ds(processor.val_pairs ,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed 500-img micro-val subset\n",
    "micro_val=random.sample(processor.val_pairs,min(500,len(processor.val_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full val cached feats\n",
    "val_feats=processor.val_pairs                               # caching handled inside class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system=CaptioningSystem(CONFIG,processor)\n",
    "system.train(train_ds,micro_val,processor.val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_img=os.path.join(CONFIG['image_dir'],processor.test_pairs[0][0])\n",
    "print(\"Beam-search caption:\",system.beam_search(demo_img))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
