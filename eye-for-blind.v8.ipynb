{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, math, random, json, pickle, itertools, warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from collections import Counter, deque\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "import tensorflow as tf                  # type: ignore\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecay\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG: Dict[str, object] = {\n",
    "    # Data\n",
    "    'image_dir'            : '/home/flickr30k_images/flickr30k_images',\n",
    "    'caption_file'         : '/home/flickr30k_images/flickr30k_images/results.csv',\n",
    "    'subset_ratio'         : 1.0,\n",
    "    # Vocabulary & sequence\n",
    "    'vocab_min_count'      : 2,          # [spec 1] ↓ from 5\n",
    "    'top_k'                : 10_000,     # [spec 1] explicit cap\n",
    "    'max_length'           : 30,\n",
    "    # Model\n",
    "    'embedding_dim'        : 512,\n",
    "    'units'                : 1024,\n",
    "    'decoder_dropout'      : 0.3,\n",
    "    'attention_reg_lambda' : 0.1,        # [spec 5]  coverage regulariser\n",
    "    # Training\n",
    "    'epochs'               : 50,\n",
    "    'batch_size'           : 128,\n",
    "    'buffer_size'          : 15_000,\n",
    "    'scheduled_sampling_max_prob': 0.4,  # [spec 4]  ↑ from 0.15\n",
    "    'mixed_precision'      : True,\n",
    "    'grad_clip_value'      : 10.0,\n",
    "    'early_stop'           : True,\n",
    "    'patience'             : 20,\n",
    "    # Optimiser / LR\n",
    "    'initial_lr'           : 5e-4,\n",
    "    'lr_alpha'             : 1e-2,\n",
    "    # Checkpoints\n",
    "    'checkpoint_dir'       : './checkpoints/chk-v8',\n",
    "    'save_checkpoints'     : True,\n",
    "    'delete_old_checkpoints': True,\n",
    "    # Misc / reproducibility\n",
    "    'seed'                 : 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ENV‑SETUP & MIXED‑PRECISION POLICY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AMP] mixed_float16 policy active ✨\n",
      "Device list: [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(CONFIG['seed'])\n",
    "random.seed(CONFIG['seed'])\n",
    "tf.random.set_seed(CONFIG['seed'])\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if CONFIG['mixed_precision']:\n",
    "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "    print(\"[AMP] mixed_float16 policy active ✨\")\n",
    "\n",
    "# single-GPU safe-growth\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "print(\"Device list:\", tf.config.list_logical_devices('GPU'))\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg      = cfg\n",
    "        self.caption_map: Dict[str, List[str]] = {}\n",
    "        self.tokeniser: Optional[Tokenizer]    = None\n",
    "        self.vocab_size = 0\n",
    "        self.train_pairs: List[Tuple[str,str]] = []\n",
    "        self.val_pairs  : List[Tuple[str,str]] = []\n",
    "        self.test_pairs : List[Tuple[str,str]] = []\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.1 load raw captions file\n",
    "    # ───────────────────────────────────────────\n",
    "    def load_captions(self):\n",
    "        fp = self.cfg['caption_file']\n",
    "        df = pd.read_csv(fp, sep='|', names=['img','num','cap'], engine='python')\n",
    "        df['img']  = df['img'].str.strip(); df['cap'] = df['cap'].str.strip()\n",
    "        for img,g in df.groupby('img'):\n",
    "            self.caption_map[img] = g['cap'].tolist()\n",
    "        print(f\"Loaded {len(self.caption_map):,} images with captions\")\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.2 text preproc\n",
    "    # ───────────────────────────────────────────\n",
    "    @staticmethod\n",
    "    def preprocess(txt:str)->str:\n",
    "        if not isinstance(txt,str) or not txt.strip(): return \"\"\n",
    "        txt = re.sub(r\"[^a-z0-9.,? ]\",\"\",txt.lower().strip())\n",
    "        return f\"<start> {txt} <end>\"\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.3 build tokeniser & split\n",
    "    # ───────────────────────────────────────────\n",
    "    def prepare(self):\n",
    "        if not self.caption_map: self.load_captions()\n",
    "        all_caps = [self.preprocess(c) for caps in self.caption_map.values() for c in caps]\n",
    "        # vocab pruning\n",
    "        cnt = Counter(w for sent in all_caps for w in sent.split())\n",
    "        keep = {w for w,f in cnt.items() if f>=self.cfg['vocab_min_count']} | {'<start>','<end>'}\n",
    "        filtered = [c for c in all_caps if all(w in keep for w in c.split())]\n",
    "\n",
    "        # 95-th percentile length\n",
    "        self.cfg['max_length'] = int(np.percentile([len(s.split()) for s in filtered],95))\n",
    "        print(\"max_length ->\",self.cfg['max_length'])\n",
    "\n",
    "        # tokeniser with explicit pad-idx 0  [spec 1,2]\n",
    "        tok = Tokenizer(num_words=self.cfg['top_k'], oov_token='<unk>', filters='', lower=True)\n",
    "        tok.fit_on_texts(filtered)\n",
    "        tok.word_index['<pad>']  = 0\n",
    "        tok.index_word[0]        = '<pad>'\n",
    "        self.tokeniser, self.vocab_size = tok, self.cfg['top_k']+1\n",
    "        print(\"vocab_size capped at\",self.vocab_size)\n",
    "\n",
    "        # img-wise (80/10/10) split\n",
    "        pairs = [(img,self.preprocess(c)) for img,caps in self.caption_map.items() for c in caps\n",
    "                 if self.preprocess(c) and all(w in keep for w in self.preprocess(c).split())]\n",
    "        imgset = list({img for img,_ in pairs})\n",
    "        if self.cfg['subset_ratio']<1.0:\n",
    "            k=int(len(imgset)*self.cfg['subset_ratio']); imgset=random.sample(imgset,k)\n",
    "        random.shuffle(imgset)\n",
    "        n=len(imgset); n_tr=int(.8*n); n_v=int(.1*n)\n",
    "        tr,val,test = set(imgset[:n_tr]),set(imgset[n_tr:n_tr+n_v]),set(imgset[n_tr+n_v:])\n",
    "        f=lambda s:[p for p in pairs if p[0] in s]\n",
    "        self.train_pairs,self.val_pairs,self.test_pairs = map(f,(tr,val,test))\n",
    "        print(f\"train/val/test images: {len(tr)}/{len(val)}/{len(test)}\")\n",
    "\n",
    "    # ───────────────────────────────────────────\n",
    "    # 3.4 helpers\n",
    "    # ───────────────────────────────────────────\n",
    "    def encode(self, cap:str):\n",
    "        seq=self.tokeniser.texts_to_sequences([cap])[0]\n",
    "        seq=pad_sequences([seq],maxlen=self.cfg['max_length'],padding='post',value=0)[0] # [spec 2]\n",
    "        return seq, len([t for t in seq if t!=0])\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec([],tf.string)])\n",
    "    def _load_img(self,path):\n",
    "        img=tf.io.read_file(path); img=tf.image.decode_jpeg(img,3); img=tf.image.convert_image_dtype(img,tf.float32)\n",
    "        return img\n",
    "\n",
    "    def _augment(self,img,train=True):\n",
    "        shape=tf.shape(img)[:2]\n",
    "        scale=342./tf.cast(tf.reduce_min(shape),tf.float32)\n",
    "        img=tf.image.resize(img,tf.cast(tf.cast(shape,tf.float32)*scale,tf.int32))\n",
    "        if train: img=tf.image.random_flip_left_right(img); img=tf.image.random_crop(img,[299,299,3])\n",
    "        else:     img=tf.image.resize_with_crop_or_pad(img,299,299)\n",
    "        return tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "    def _gen(self,data,train=True):\n",
    "        for img,cap in data:\n",
    "            path=os.path.join(self.cfg['image_dir'],img)\n",
    "            img_tensor=self._augment(self._load_img(tf.constant(path)),train)\n",
    "            tok,ln=self.encode(cap)\n",
    "            yield img_tensor, tok.astype(np.int32), ln, img\n",
    "\n",
    "    def make_ds(self,data,train=True):\n",
    "        sig=(tf.TensorSpec((299,299,3),tf.float32),\n",
    "             tf.TensorSpec((self.cfg['max_length'],),tf.int32),\n",
    "             tf.TensorSpec((),tf.int32),\n",
    "             tf.TensorSpec((),tf.string))\n",
    "        ds=tf.data.Dataset.from_generator(lambda:self._gen(data,train),output_signature=sig)\n",
    "        if train: ds=ds.shuffle(self.cfg['buffer_size'])\n",
    "        ds=ds.batch(self.cfg['batch_size']).prefetch(AUTOTUNE)\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MODEL BUILDING BLOCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self): super().__init__(); base=tf.keras.applications.InceptionV3(include_top=False,weights='imagenet',input_shape=(299,299,3))\n",
    "    def build(self,input_shape):\n",
    "        self.base=tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "        self.base.trainable=False; self.reshape=layers.Reshape((-1,2048))\n",
    "    def call(self,x): return self.reshape(self.base(x))\n",
    "    def unfreeze_top_layers(self,n=8):                       # [spec 7]\n",
    "        for l in self.base.layers[-n:]: l.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self,units): super().__init__(); self.W1=layers.Dense(units); self.W2=layers.Dense(units); self.V=layers.Dense(1)\n",
    "    def call(self,feat,hidden):\n",
    "        score=self.V(tf.nn.tanh(self.W1(feat)+self.W2(tf.expand_dims(hidden,1))))\n",
    "        attn=tf.nn.softmax(score,1); ctx=tf.reduce_sum(attn*feat,1)\n",
    "        return ctx,tf.squeeze(attn,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self,embed_dim,units,vocab,drop):\n",
    "        super().__init__(); self.units=units\n",
    "        self.embed=layers.Embedding(vocab,embed_dim,mask_zero=True)\n",
    "        self.attn=BahdanauAttention(units); self.beta=layers.Dense(1,activation='sigmoid')\n",
    "        self.lstm=layers.LSTM(units,return_sequences=True,return_state=True)\n",
    "        self.drop=layers.Dropout(drop); self.proj=layers.Dense(units*2); self.fc=layers.Dense(vocab,dtype='float32')\n",
    "        \n",
    "    def call(self,x,feat,h,c):\n",
    "        ctx,alpha=self.attn(feat,h); ctx=self.beta(h)*ctx\n",
    "        x=self.embed(x); x=tf.concat([tf.expand_dims(ctx,1),x],-1)\n",
    "        o,h,c=self.lstm(x,initial_state=[h,c]); o=tf.squeeze(o,1)\n",
    "        m=tf.reshape(self.proj(tf.concat([o,ctx],-1)),(-1,self.units,2)); m=tf.reduce_max(m,2)\n",
    "        return tf.expand_dims(self.drop(m),1),h,c,alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. TOP‑LEVEL TRAINING WRAPPER (AMP READY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningSystem:\n",
    "    def __init__(self,cfg:Dict[str,object],proc:DataProcessor):\n",
    "        self.cfg,self.proc=cfg,proc\n",
    "        self.encoder,self.decoder=None,None\n",
    "        self.opt,self.loss_fn=None,None\n",
    "        self.ckpt_mgr=None\n",
    "        # logs\n",
    "        self.best_bleu=0; self.loss_log=[]; self.tr_bleu=[]; self.val_bleu=[]\n",
    "        self.grad_norms=[]\n",
    "        self.smooth=SmoothingFunction().method4\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.1 build & compile\n",
    "    # ─────────────────────────────\n",
    "    def build(self, steps_per_epoch):\n",
    "        print(\"Building model …\")\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder(self.cfg['embedding_dim'],\n",
    "                               self.cfg['units'],\n",
    "                               self.proc.vocab_size,\n",
    "                               self.cfg['decoder_dropout'])\n",
    "\n",
    "        decay_steps = int(steps_per_epoch * self.cfg['epochs'])\n",
    "        schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "            self.cfg['initial_lr'], decay_steps, alpha=self.cfg['lr_alpha'])\n",
    "        base_opt = tf.keras.optimizers.Adam(schedule)\n",
    "        self.opt = (tf.keras.mixed_precision.LossScaleOptimizer(base_opt)\n",
    "                    if self.cfg['mixed_precision'] else base_opt)\n",
    "\n",
    "        # --- loss with fallback for TF versions lacking built-in smoothing ----\n",
    "        try:\n",
    "            self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True, label_smoothing=0.1, reduction='none')\n",
    "        except TypeError:                                            # manual LS\n",
    "            ls = 0.1\n",
    "            def _smooth_loss(y_true, logits, ls=ls):\n",
    "                vocab = tf.shape(logits)[-1]\n",
    "                y_true = tf.cast(y_true, tf.int32)\n",
    "                one = tf.one_hot(y_true, vocab, dtype=logits.dtype)  # ensure dtypes match\n",
    "                smooth = one * (1 - ls) + ls / tf.cast(vocab, logits.dtype)\n",
    "                return tf.keras.backend.categorical_crossentropy(\n",
    "                    smooth, logits, from_logits=True)\n",
    "            self.loss_fn = _smooth_loss\n",
    "        # ---------------------------------------------------------------------\n",
    "\n",
    "        if self.cfg['save_checkpoints']:\n",
    "            ckpt = tf.train.Checkpoint(enc=self.encoder, dec=self.decoder, opt=self.opt)\n",
    "            self.ckpt_mgr = tf.train.CheckpointManager(ckpt, self.cfg['checkpoint_dir'], max_to_keep=3)\n",
    "            if self.ckpt_mgr.latest_checkpoint:\n",
    "                ckpt.restore(self.ckpt_mgr.latest_checkpoint)\n",
    "                print(\"Restored\", self.ckpt_mgr.latest_checkpoint)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.2 helpers\n",
    "    # ─────────────────────────────\n",
    "    def _cast(self,x): return tf.cast(x,tf.float32) if x.dtype!=tf.float32 else x\n",
    "\n",
    "    # scheduled-sampling prob updated every epoch  [spec 4]\n",
    "    def _ss_prob(self,epoch): return self.cfg['scheduled_sampling_max_prob']*epoch/max(1,self.cfg['epochs']-1)\n",
    "\n",
    "    # trigram-blocker util                               [spec 11]\n",
    "    @staticmethod\n",
    "    def _repeat_trigram(seq,tok):\n",
    "        return len(seq)>=2 and (seq[-2],seq[-1],tok) in set(zip(seq,seq[1:],seq[2:]))\n",
    "\n",
    "    # deduplicate prediction util                        [spec 12]\n",
    "    @staticmethod\n",
    "    def _dedup(tokens):\n",
    "        out=[]\n",
    "        for i,t in enumerate(tokens):\n",
    "            if i<2 or not(t==tokens[i-1]==tokens[i-2]): out.append(t)\n",
    "        return out\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.3 train-step @tf.function\n",
    "    # ─────────────────────────────\n",
    "    @tf.function\n",
    "    def _train_step(self, img, tgt, ln, ss_p):\n",
    "        B = tf.shape(img)[0]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            feat = self.encoder(img)                                      # (B,64,2048)\n",
    "            h = tf.zeros((B, self.cfg['units']), feat.dtype)\n",
    "            c = tf.zeros_like(h)\n",
    "\n",
    "            start_id = self.proc.tokeniser.word_index['<start>']\n",
    "            dec_in = tf.expand_dims(tf.fill([B], start_id), 1)\n",
    "\n",
    "            L = tf.shape(feat)[1]\n",
    "            attn_cov = tf.zeros((B, L), feat.dtype)\n",
    "\n",
    "            total_ce = tf.constant(0.0, tf.float32)                       # keep running sum in fp32\n",
    "\n",
    "            for t in tf.range(1, self.cfg['max_length']):\n",
    "                logits, h, c, alpha = self.decoder(dec_in, feat, h, c)\n",
    "                attn_cov += alpha                                         # accumulate coverage\n",
    "\n",
    "                ce = self.loss_fn(tgt[:, t], tf.squeeze(logits, 1))       # ce may be fp16\n",
    "                ce = tf.cast(ce, tf.float32)                               # promote to fp32\n",
    "                mask = tf.cast(tgt[:, t] != 0, tf.float32)\n",
    "                total_ce += tf.reduce_sum(ce * mask)\n",
    "\n",
    "                pred = tf.argmax(logits, -1, output_type=tf.int32)[:, 0]\n",
    "                use_pred = tf.less(\n",
    "                    tf.random.uniform([B], dtype=tf.float32), ss_p)\n",
    "                nxt = tf.where(use_pred, pred, tgt[:, t])\n",
    "                dec_in = tf.expand_dims(nxt, 1)\n",
    "\n",
    "            ce_loss = total_ce / tf.reduce_sum(tf.cast(ln, tf.float32))\n",
    "            reg = tf.reduce_mean(\n",
    "                tf.square(tf.constant(1.0, tf.float32) -\n",
    "                          self._cast(attn_cov)))                          # coverage penalty\n",
    "\n",
    "            loss = ce_loss + self.cfg['attention_reg_lambda'] * reg\n",
    "\n",
    "            if isinstance(self.opt, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "                loss_scaled = self.opt.get_scaled_loss(loss)\n",
    "            else:\n",
    "                loss_scaled = loss\n",
    "\n",
    "        vars_ = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "        grads = tape.gradient(loss_scaled, vars_)\n",
    "        if isinstance(self.opt, tf.keras.mixed_precision.LossScaleOptimizer):\n",
    "            grads = self.opt.get_unscaled_gradients(grads)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, self.cfg['grad_clip_value'])\n",
    "        self.opt.apply_gradients(zip(grads, vars_))\n",
    "\n",
    "        return loss, tf.linalg.global_norm(grads)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.4 greedy decode – **batched** & @tf.function  [spec 8]\n",
    "    # ─────────────────────────────\n",
    "    @tf.function\n",
    "    def _greedy_batch(self,feat_batch):\n",
    "        B=tf.shape(feat_batch)[0]\n",
    "        h=tf.zeros((B,self.cfg['units']),feat_batch.dtype); c=tf.zeros_like(h)\n",
    "        dec_in=tf.expand_dims(tf.fill([B],self.proc.tokeniser.word_index['<start>']),1)\n",
    "        seq=tf.TensorArray(tf.int32,size=self.cfg['max_length'])\n",
    "        for t in tf.range(self.cfg['max_length']):\n",
    "            logits,h,c,_=self.decoder(dec_in,feat_batch,h,c)\n",
    "            nxt=tf.argmax(logits[:,-1],-1,output_type=tf.int32)\n",
    "            seq=seq.write(t,nxt)\n",
    "            dec_in=tf.expand_dims(nxt,1)\n",
    "        return tf.transpose(seq.stack())  # (B,T)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.5 evaluate BLEU util (uses cached CNN feats)    [spec 8,9]\n",
    "    # ─────────────────────────────\n",
    "    def _load_val_feats(self,save_if_missing=True):\n",
    "        cache='val_feats.npz'\n",
    "        if os.path.exists(cache):\n",
    "            return np.load(cache)['arr_0']\n",
    "        feats=[]\n",
    "        for img,_ in self.proc.val_pairs:\n",
    "            path=os.path.join(self.cfg['image_dir'],img)\n",
    "            tensor=self.proc._augment(self.proc._load_img(tf.constant(path)),train=False)\n",
    "            feats.append(self.encoder(tf.expand_dims(tensor,0))[0].numpy())\n",
    "        arr=np.stack(feats)\n",
    "        if save_if_missing: np.savez_compressed(cache,arr_0=arr)\n",
    "        return arr\n",
    "\n",
    "    def _compute_bleu(self,data,batch_size=64,max_imgs=None):\n",
    "        refs,hyps=[],[]\n",
    "        subset=data if max_imgs is None else random.sample(data,min(max_imgs,len(data)))\n",
    "        # pre-decode CNN feats in batches for speed\n",
    "        feat_list=[]\n",
    "        for img,_ in subset:\n",
    "            path=os.path.join(self.cfg['image_dir'],img)\n",
    "            feat=self.encoder(tf.expand_dims(\n",
    "                self.proc._augment(self.proc._load_img(tf.constant(path)),train=False),0))\n",
    "            feat_list.append(feat)\n",
    "        batched=tf.data.Dataset.from_tensor_slices(tf.concat(feat_list,0)).batch(batch_size)\n",
    "        preds=[]\n",
    "        for feat_b in batched:\n",
    "            seqs=self._greedy_batch(feat_b).numpy()                   # [spec 8]\n",
    "            preds.extend(seqs)\n",
    "        for (img,_),seq in zip(subset,preds):\n",
    "            hyp=[self.proc.tokeniser.index_word.get(i,'') for i in seq\n",
    "                 if i not in (0,self.proc.tokeniser.word_index['<end>'],\n",
    "                              self.proc.tokeniser.word_index['<start>'])]\n",
    "            hyp=self._dedup(hyp)                                      # [spec 12]\n",
    "            gt=[[w for w in self.proc.preprocess(c).split()\n",
    "                 if w not in ('<start>','<end>')] for c in self.proc.caption_map[img][:5]]\n",
    "            refs.append(gt); hyps.append(hyp)\n",
    "        weight=(0.25,0.25,0.25,0.25)\n",
    "        return corpus_bleu(refs,hyps,weights=weight,smoothing_function=self.smooth)\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.6 beam search w/ trigram block [spec 11] + dedup [12]\n",
    "    # ─────────────────────────────\n",
    "    def beam_search(self, image_path, beam=5, len_pen=0.7, return_attention=False):\n",
    "        img = self.proc._augment(self.proc._load_img(tf.constant(image_path)), train=False)\n",
    "        feat = self.encoder(tf.expand_dims(img, 0))                      # (1,64,2048)\n",
    "        dty  = feat.dtype\n",
    "        start = self.proc.tokeniser.word_index['<start>']\n",
    "        end   = self.proc.tokeniser.word_index['<end>']\n",
    "\n",
    "        beams = [(0.0, [start],\n",
    "                  tf.zeros((1, self.cfg['units']), dty),\n",
    "                  tf.zeros((1, self.cfg['units']), dty),\n",
    "                  [])]                                                   # score, seq, h, c, α-list\n",
    "        completed = []\n",
    "\n",
    "        for _ in range(self.cfg['max_length']):\n",
    "            cand = []\n",
    "            for score, seq, h, c, alphas in beams:\n",
    "                if seq[-1] == end:\n",
    "                    completed.append((score, seq, alphas)); continue\n",
    "                logits, h1, c1, alpha = self.decoder(tf.expand_dims([seq[-1]], 0),\n",
    "                                                     feat, h, c)\n",
    "                logp = tf.nn.log_softmax(self._cast(logits[0, 0]))\n",
    "                topk = tf.math.top_k(logp, beam).indices.numpy()\n",
    "                for tok in topk:\n",
    "                    if self._repeat_trigram(seq, tok): continue\n",
    "                    cand.append((score + float(logp[tok]),\n",
    "                                 seq + [tok], h1, c1, alphas + [alpha[0].numpy()]))\n",
    "\n",
    "            if not cand: break\n",
    "            cand.sort(key=lambda x: x[0] / (len(x[1]) ** len_pen), reverse=True)\n",
    "            beams = cand[:beam]\n",
    "            if len(completed) >= beam: break\n",
    "\n",
    "        best = max(completed + beams,\n",
    "                   key=lambda x: x[0] / (len(x[1]) ** len_pen))\n",
    "        words = [self.proc.tokeniser.index_word.get(i, '')\n",
    "                 for i in best[1]\n",
    "                 if i not in (start, end, 0)]\n",
    "        caption = \" \".join(self._dedup(words))\n",
    "        return (caption, best[2]) if return_attention else caption\n",
    "\n",
    "    # ─────────────────────────────\n",
    "    # 5.7 main TRAIN LOOP (spec 4,6,7,10,14)\n",
    "    # ─────────────────────────────\n",
    "    def train(self,train_ds,val_subset,val_full):\n",
    "        steps_per_epoch=math.ceil(len(self.proc.train_pairs)/self.cfg['batch_size'])\n",
    "        self.build(steps_per_epoch)\n",
    "        for epoch in range(self.cfg['epochs']):\n",
    "            ss_p=self._ss_prob(epoch)                                 # [spec 4]\n",
    "            prog=tf.keras.utils.Progbar(steps_per_epoch,unit_name='batch')\n",
    "            tot_loss=0\n",
    "            for img,tgt,ln,_ in train_ds:\n",
    "                loss,g=self._train_step(img,tgt,ln,ss_p)\n",
    "                tot_loss+=float(loss); self.grad_norms.append(float(g))\n",
    "                prog.add(1,values=[('loss',loss)])\n",
    "            self.loss_log.append(tot_loss/steps_per_epoch)\n",
    "\n",
    "            # micro-val BLEU (≤500 imgs) every epoch\n",
    "            micro_bleu=self._compute_bleu(val_subset)\n",
    "            # full val BLEU every 5 epochs      [spec 10]\n",
    "            full_bleu = self._compute_bleu(val_full) if epoch%5==0 else None\n",
    "            self.tr_bleu.append(micro_bleu); self.val_bleu.append(full_bleu or np.nan)\n",
    "\n",
    "            # checkpoint\n",
    "            if self.ckpt_mgr: self.ckpt_mgr.save()\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{self.cfg['epochs']}  \"\n",
    "                  f\"loss={self.loss_log[-1]:.4f}  microBLEU={micro_bleu:.4f}\"\n",
    "                  + (f\"  fullBLEU={full_bleu:.4f}\" if full_bleu is not None else \"\")\n",
    "                  + f\"  ss_p={ss_p:.2f}\")\n",
    "\n",
    "            # unfreeze CNN after epoch 5     [spec 7]\n",
    "            if epoch==4:\n",
    "                self.encoder.unfreeze_top_layers(8); \n",
    "                self.opt=tf.keras.optimizers.Adam(1e-5)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #  DEMO  show image, GT captions, model caption, attention overlays\n",
    "    # -------------------------------------------------------------------------\n",
    "    def demo(self, image_path, beam=5, len_pen=0.7):\n",
    "        import matplotlib.pyplot as plt\n",
    "        from PIL import Image\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            print(\"Image not found:\", image_path); return\n",
    "\n",
    "        # 1) image preview\n",
    "        img_name = os.path.basename(image_path)\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        plt.imshow(Image.open(image_path)); plt.title(img_name); plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # 2) ground-truth captions\n",
    "        gts = self.proc.caption_map.get(img_name, [])\n",
    "        print(\"Ground-truth captions:\")\n",
    "        for c in gts[:5]: print(\" •\", c)\n",
    "\n",
    "        # 3) prediction + attention\n",
    "        caption, alphas = self.beam_search(image_path, beam, len_pen, return_attention=True)\n",
    "        print(\"\\nPredicted:\", caption)\n",
    "\n",
    "        # attention heat-maps\n",
    "        words = caption.split()\n",
    "        if not alphas: return\n",
    "        L = int(np.sqrt(alphas[0].shape[0]))           # 8×8 grid\n",
    "        img = np.array(Image.open(image_path).resize((299, 299)))\n",
    "\n",
    "        cols = 3\n",
    "        rows = int(np.ceil(len(words) / cols))\n",
    "        plt.figure(figsize=(4 * cols, 3 * rows))\n",
    "        for i, (w, a) in enumerate(zip(words, alphas)):\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            plt.imshow(img); plt.axis('off'); plt.title(w)\n",
    "            a = a.reshape(L, L); a = (a - a.min()) / (a.ptp() + 1e-6)\n",
    "            a_up = Image.fromarray((a * 255).astype(np.uint8)).resize((299, 299))\n",
    "            plt.imshow(np.array(a_up) / 255., cmap='jet', alpha=0.5, extent=(0, 299, 299, 0))\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    #  TRAINING CURVES  loss & BLEU-4\n",
    "    # -------------------------------------------------------------------------\n",
    "    def plot_history(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "        epochs = range(1, len(self.loss_log) + 1)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(epochs, self.loss_log, label='Train CE-loss')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.grid(True); plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(epochs, self.tr_bleu, label='Micro-val BLEU-4')\n",
    "        if any(np.isfinite(self.val_bleu)):\n",
    "            plt.plot(epochs, np.nan_to_num(self.val_bleu, nan=np.nan),\n",
    "                     label='Full-val BLEU-4', linestyle='--')\n",
    "        plt.xlabel('Epoch'); plt.ylabel('BLEU-4'); plt.grid(True); plt.legend()\n",
    "\n",
    "        plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31,784 images with captions\n",
      "max_length -> 25\n",
      "vocab_size capped at 10001\n",
      "train/val/test images: 25426/3178/3179\n"
     ]
    }
   ],
   "source": [
    "processor=DataProcessor(CONFIG); processor.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds=processor.make_ds(processor.train_pairs,train=True)\n",
    "val_ds =processor.make_ds(processor.val_pairs ,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed 500-img micro-val subset\n",
    "micro_val=random.sample(processor.val_pairs,min(500,len(processor.val_pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full val cached feats\n",
    "val_feats=processor.val_pairs                               # caching handled inside class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "system=CaptioningSystem(CONFIG, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model …\n",
      "950/950 [==============================] - 1781s 2s/batch - loss: 3.5762\n",
      "Epoch 1/50  loss=3.5762  microBLEU=0.0996  fullBLEU=0.0933  ss_p=0.00\n",
      "152/950 [===>..........................] - ETA: 24:29 - loss: 3.3637"
     ]
    }
   ],
   "source": [
    "system.train(train_ds,micro_val,processor.val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system.plot_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = os.path.join(CONFIG['image_dir'], processor.test_pairs[0][0])\n",
    "system.demo(test_img, beam=5, len_pen=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
